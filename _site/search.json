[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About\nBook\nWorkshops\nTalks\nR-Ladies\nPapers\n\n\n\nSTATISTICIAN | ACTUARY | DATA SCIENTIST\n‚ÄúI‚Äôve always loved maths and science, I meant to be a Veterinary, I wanted to do ballet classes, teach gymnastic, be an economist, a psychologist‚Ä¶ Many years passed since I started the journey of my career as an investigator, it turned out to be an everyday challenge with tasks to solve, and most importantly a continuous learning path. Each day brings with it a fresh set of challenges to solve and opportunities for growth. It‚Äôs a path marked by curiosity, resilience, and a relentless pursuit of excellence‚Äîa journey that I‚Äôm grateful to be on.‚Äù\nFederica Gazzelloni is an Actuary, and a Data Scientist with a focus on health metrics, machine learning, and data visualization. With experience spanning corporate, academic, and research roles, Federica has developed a robust skill set that bridges actuarial science, statistical modeling, and public health.\nFederica began her career as an actuary, working in corporate and academic settings where she focused on quantitative analysis, risk modeling, and compliance in the insurance and pensions industry. As a research-oriented actuary, she not only applied advanced actuarial principles but also developed a deep understanding of the statistical methods. Federica taught mathematics to high school students and instructed university students in computer science, helping to cultivate the next generation of data-driven professionals.\nIn recent years, Federica has expanded her focus to health data modeling, particularly in the context of infectious disease research. As the world faced the Covid-19 pandemic, her expertise in statistical methods and health metrics became even more relevant. Collaborating with the Institute for Health Metrics and Evaluation (IHME), Federica contributed to global health studies that aimed to understand the spread and impact of Covid-19, as well as other pressing health issues. This experience inspired her to create a practical manual for health data analysis, which serves as a resource for professionals and students alike who wish to explore health metrics and epidemiological modeling in-depth. Her book, to be published by CRC Press, combines foundational knowledge with practical applications, including R code for real-world case studies in health metrics.\nFederica‚Äôs role in the pre-publication stages of GBD research enables her to directly contribute to some of the most impactful global health research being conducted today. This work not only requires technical acumen in statistical modeling but also a keen eye for detail and a commitment to accuracy, given the far-reaching implications of public health data.\nBeyond her work in health metrics, Federica is an active member of the open-source community, where she contributes to several organizations dedicated to education and software development. She is a Certified Carpentries Instructor, delivering workshops that empower learners with foundational data science skills in R, Python, and other tools. Federica has taught workshops for prestigious organizations, including the Helmholtz Information & Data Science Academy, the University of Washington, and the Centers for Disease Control and Prevention (CDC), where she introduced participants to key concepts in programming, data wrangling, and statistical analysis. Her teaching style is characterized by clarity, engagement, and inclusivity, making technical content accessible to learners of all backgrounds and skill levels.\nFederica also serves as the Lead Organizer for R-Ladies Rome, a chapter of the global R-Ladies organization that promotes gender diversity in the R programming and data science communities. Since its founding in 2023, R-Ladies Rome has grown significantly under her leadership, reaching a large and engaged audience through online and in-person events. Through R-Ladies, Federica has organized a range of activities, from tutorials on data visualization to workshops on advanced R packages, all aimed at fostering a supportive and inclusive community for women and underrepresented groups in data science. Notable events include a session with Hadley Wickham, Chief Scientist at Posit PBC, and an upcoming workshop on building reproducible data pipelines, which reflects the group‚Äôs commitment to high-quality, practical learning opportunities.\nIn addition to her work with R-Ladies Rome, Federica has collaborated with other R user groups globally, including R-Ladies New York, R-Ladies Paris, and the TunisR User Group. These partnerships have expanded the reach of her initiatives, creating a more interconnected and supportive global R community. Federica is also involved with Bioconductor and the R Consortium, furthering her commitment to open-source development and collaborative learning.\nOne of Federica‚Äôs key strengths is her ability to use data visualization as a tool for communication and insight. Her background in data visualization allows her to create compelling visual representations of complex health data, making it easier for diverse audiences to grasp intricate statistical relationships. Whether through static graphs or interactive dashboards, Federica‚Äôs visualizations aim to tell a story, uncover trends, and empower decision-makers with actionable insights.\nThroughout her career, Federica has consistently demonstrated a commitment to knowledge sharing and community building. Her work is informed by a belief in the power of data to drive positive change, especially in fields like public health where informed decision-making can save lives. Her contributions to education, open-source software, and health metrics research underscore her dedication to making data science an inclusive and impactful field.\n\n\n\nHealth Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R\nDiscover the fascinating world of health metrics and infectious disease analysis with our manual!\nWhether you‚Äôre new to the field or a seasoned practitioner, this resource offers valuable insights into key concepts and practical applications.\n\n\n\n\n\n\n\n\nRead it online for free:\n\nüëâ üîó hmsidR\n\n\nReserve your paperback copy:\n\nüëâ üîóRoutledge Website\n\nüëâ üîóAmazon\n\n\n\n\n\nEditor CRC Press - Chapman & Hall\n\n\n\n\n\n\n\nLearn about essential public health metrics like DALYs, YLL, and YLD, which provide crucial indicators of a population‚Äôs health status. Dive into recent infectious disease outbreaks, including the impact of Covid19, and explore the most affected locations.\nWith a mix of explanatory text and hands-on R code examples, you‚Äôll become proficient in analyzing real-world case studies. We‚Äôll compare health metrics across different locations and test predictive models using tidymodels and INLA. Plus, you‚Äôll discover the power of spatial visualization techniques using ggplot2, leaflet, sf, rgdal, and other popular R packages. Whether you‚Äôre a beginner or an advanced practitioner, this book is designed to enhance your understanding and skills in health data analysis.\nIdeal for early-stage practitioners and graduate students in public health, this resource is your guide to unlocking the full potential of R for spatial and health metrics analysis.\n\n\n\nCDC Centers for Disease Control\nData Analysis and Visualization with R\nDate: 2024-08-19 to 2024-08-20 | Centers for Disease Control\nWebsite: https://rrlove-cdc.github.io/2024-08-19-cdc-online/\nEtherPad: https://pad.carpentries.org/2024-08-19-cdc-online\n\nData Carpentry Genomics\nDate: 2024-06-10 to 2024-06-13 | Centers for Disease Control\nWebsite: https://fgazzelloni.github.io/2024-06-10-cdc-online/\nEtherPad: https://pad.carpentries.org/2024-06-10-cdc-online\nUniversity of Washington\nNetwork of the National Library of Medicine (NNLM) Region 5 Library Carpentry\nDate: March 19-21 2024 | Network of the National Library of Medicine (NNLM)\nWebsite: https://nnlm-ncds.github.io/2024-03-19-nnlm-uw-online/\nEtherPad: https://pad.carpentries.org/2024-03-19-nnlm-uw-online\nHelmholtz Information & Data Science Academy\nSoftware Carpentry (Shell, Git, and programming with R)\nDate: February 12-13 2024 | Helmholtz Online\nWebsite: https://macrobiotus.github.io/2024-02-12-helmholtz-online/\nEtherPad: https://pad.carpentries.org/2024-02-12-helmholtz-online\n\n\n\n\nList of my talks\n\nShinyConf2024 - Shiny4Good Talk\nDate: April 2024\nAbstract: This talk will offer insights gained from active participation in the Data Science Learning Community through participation in Shiny book clubs. By exploring the unique dynamics of these forums, I will highlight the collaborative and knowledge-sharing atmosphere that the Data Science Learning Community cultivates. The talk will draw examples from the book clubs to illustrate how practical discussions and exercises amplify the learning experience.\n#beginner #Shiny4Good #shinyapp #showcase #rstats #datascience\n\n\n\n\n\n\nSharing the mission with the open source community\n\nR-Ladies DC\nDate: September 2022\nAbstract: This tutorial is meant for people new to spatial analysis and modeling with RStudio but comfortable in making simple data visualization with ggplot2. In this video, you will learn how to make a map with RStudio, and how to use data modeling for making spatial model analysis.\nMaterial: It is helpful to have the following R packages installed beforehand: {tidyverse}, {ggthemes}, {maptools}, {ggmap}, {sf}, {spocc}, {dismo}, {SpatialEpi}, and {oregonfrogs} dataset from\nremotes::install_github(\"fgazzelloni/oregonfrogs\")\n\nGitHub Repo: https://github.com/Fgazzelloni/How-to-Spatial-Modeling-with-R\n\nBook: https://fgazzelloni.github.io/How-to-Spatial-Modeling-with-R\n\n\n\nR-Ladies NYC & R-Ladies Rome\nDate: April 2023\nAbstract: In this video, you will learn about Modeling infectious diseases with R using both deterministic and Bayesian SIR model methods. We will explore both Deterministic and Bayesian SIR model methods, and learn how to use the well-known SIR model to understand how epidemics unfold and how to prevent their spread.\nThis video is perfect for students of science, healthcare professionals, or anyone who wants to gain a deeper understanding of how to use R for Modeling infectious diseases.\nAgenda:\n\nIntroduction presentation of the Chapters and R-Ladies Global action\nSIR model with R - quick intro assessment (Speaker Federica Gazzelloni)\nBayesian workflow for disease transmission (Speaker Jacqueline Buros)\nQ&A session\n\nMaterial:\n\nGitHub Repo: https://github.com/Fgazzelloni/sir-model-with-R/\n\n\n\nR-Ladies Cambridge\nDate: March 2024\nAbstract: In this video, you‚Äôll learn how to use ggplot2 to replicate one of the ongoing #DuboisChallenge2024 plates. The original plates are part of W.E.B. Du Bois‚Äôs legacy from back in 1900, showcased at the Paris Exposition. I‚Äôll be using modern tools such as R. We‚Äôll explore the colors used in the plates and delve into the intricacies of the challenge by understanding the perspective of hand-made graphs in the ‚ÄòDuboisian‚Äô style.\nMaterial:\n\nGitHub Repo: https://github.com/Fgazzelloni/R-Ladies-Cambridge-Dataviz-lunch-Replicating-Du-Bois-with-R\n\nWebSite Collection: https://fgazzelloni.quarto.pub/unlocking-the-power-of-data-visualization-with-r/duboischallenge/\n\n\n\nR-Ladies NYC & R-Ladies Rome\nDate: June 2024\nAbstract: In this video, you‚Äôll learn how to make a website in R with Quarto. We‚Äôll explore the basics of Quarto and how to publish a website. The tutorial is designed to be accessible to beginners, with no prior experience required. By the end of the session, you will have a fully functional website that you can use to showcase your projects and achievements.\nMaterial:\n\nGithub repository: https://github.com/Fgazzelloni/building_a_website_in_r\n\nFinal version of the Website: https://fgazzelloni.quarto.pub/my-website-in-r‚Äìquarto/\n\n\n\nResources:\n\nQuarto Docs: https://quarto.org/docs/websites/\n\nCSS customization for your website: https://www.w3schools.com/css/\n\nBackground image (change - ‚Äúpink‚Äù with your favorite pick): https://www.pexels.com/search/pink/\n\n\n\n\nAuthored & Co-authored Publications\n\n\n\nList of Publications 2020 to Present\n\nN\nYear\nJournal\nTitle\nAuthor\nPubid\n\n\n\n18\n2025\nThe Lancet Public Health\nChanging life expectancy in European countries 1990‚Äì2021: a subanalysis of causes and risk factors from the Global Burden of Disease Study 2021\nN Steel, CMM Bauer-Staeb, JA Ford, C Abbafati, MA Abdalla, ...\naqlVkmm33-oC\n\n\n17\n2025\nThe Lancet\nBurden of 375 diseases and injuries, risk-attributable burden of 88 risk factors, and healthy life expectancy in 204 countries and territories, including 660 subnational¬†‚Ä¶\nSI Hay, KL Ong, DF Santomauro, MA Aalipour, H Aalruz, HS Ababneh, ...\nQIV2ME_5wuYC\n\n\n16\n2025\nThe Lancet\nGlobal age-sex-specific all-cause mortality and life expectancy estimates for 204 countries and territories and 660 subnational locations, 1950‚Äì2023: a demographic analysis for¬†‚Ä¶\nAE Schumacher, P Zheng, RM Barber, MA Aalipour, H Aalruz, ...\ndhFuZR0502QC\n\n\n15\n2025\nHealth Policy\nThe Role of Burden of Disease Estimates in European Union Health Policymaking: A Systematic Analysis of EU Legislation\nJ Chen-Xu, D Grad, N Mahrouseh, JV Santos, S Riva, F Gazzelloni, ...\nmVmsd5A6BfQC\n\n\n14\n2025\nChapman and Hall/CRC\nHealth Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R\nF Gazzelloni\n4DMP91E08xMC\n\n\n13\n2025\nElsevier\nChanging life expectancy in European countries 1990‚Äì2021: a subanalysis of causes and risk factors from the Global Burden of Disease Study 2021\nS Nicholas, CMM Bauer-Staeb, JA Ford, C Abbafati, MA Abdalla, ...\nWp0gIr-vW9MC\n\n\n12\n2024\nThe Lancet\nGlobal age-sex-specific mortality, life expectancy, and population estimates in 204 countries and territories and 811 subnational locations, 1950‚Äì2021, and the impact of the¬†‚Ä¶\nAE Schumacher, HH Kyu, A Aali, C Abbafati, J Abbas, ...\n8k81kl-MbHgC\n\n\n11\n2024\nThe lancet\nGlobal fertility in 204 countries and territories, 1950‚Äì2021, with forecasts to 2100: a comprehensive demographic analysis for the Global Burden of Disease Study 2021\nNV Bhattacharjee, AE Schumacher, A Aali, YH Abate, R Abbasgholizadeh, ...\n5nxA0vEk-isC\n\n\n10\n2024\nThe Lancet\nGlobal, regional, and national stillbirths at 20 weeks' gestation or longer in 204 countries and territories, 1990‚Äì2021: findings from the Global Burden of Disease Study 2021\nH Comfort, TA McHugh, AE Schumacher, A Harris, EA May, KR Paulson, ...\nM3ejUd6NZC8C\n\n\n9\n2024\nBMC public health\nThe state of health in the European Union (EU-27) in 2019: a systematic analysis for the Global Burden of Disease study 2019\nJV Santos, A Padron-Monedero, B Bikbov, DA Grad, D Plass, EA Mechili, ...\nkNdYIx-mwKoC\n\n\n8\n2024\nEuropean Journal of Public Health\nExploring Burden of Disease Metrics in EU legislation: A Systematic Analysis\nJ Chen-Xu, DA Grad, N Mahrouseh, S Cuschieri, F Gazzelloni, EA Mechili, ...\n4TOpqqG69KYC\n\n\n7\n2024\nBritish Actuarial Journal\nLoss modelling from first principles\nP Parodi, D Thrumble, P Watson, Z Ji, A Wang, I Bhatia, J Lees, S Mealy, ...\nUebtZRa9Y70C\n\n\n6\n2023\nEpidemiology & Infection\nBurden of infectious disease studies in Europe and the United Kingdom: a review of methodological design choices\nP Charalampous, JA Haagsma, LS Jakobsen, V Gorasso, I Noguer, ...\nufrVoPGSRksC\n\n\n5\n2023\nArchives of Public Health\nBurden of disease attributable to risk factors in European countries: a scoping literature review\nV Gorasso, JN Morgado, P Charalampous, SM Pires, JA Haagsma, ...\n0EnyYjriUFMC\n\n\n4\n2022\nEuropean journal of public health\nBurden of non-communicable disease studies in Europe: a systematic review of data sources and methodological choices\nP Charalampous, V Gorasso, D Plass, SM Pires, E Von Der Lippe, ...\nW7OEmFMy1HYC\n\n\n3\n2022\nBMC public health\nMethodological considerations in injury burden of disease studies across Europe: a systematic literature review\nP Charalampous, E Pallari, V Gorasso, E Von der Lippe, ...\nWF5omc3nYNoC\n\n\n2\n2021\nEuropean Journal of Public Health\nThe state of health in the European Union in 2019\nJ Vasco Santos, A Padron Monedero, B Bikbov, DA Grad, D Plass, ...\neQOLeE2rZwMC\n\n\n1\n2021\nEuropean Journal of Public Health\nThe state of health in the European Union in 2019\nJV Santos, AP Monedero, B Bikbov, DA Grad, D Plass, EA Mechili, ...\nYsMSGLbcyi4C\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/book/index.html",
    "href": "about/book/index.html",
    "title": "Federica Gazzelloni",
    "section": "",
    "text": "Health Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R\nDiscover the fascinating world of health metrics and infectious disease analysis with our manual!\nWhether you‚Äôre new to the field or a seasoned practitioner, this resource offers valuable insights into key concepts and practical applications.\n\n\n\n\n\n\n\n\nRead it online for free:\n\nüëâ üîó hmsidR\n\nReserve your paperback copy:\n\nüëâ üîóRoutledge Website\nüëâ üîóAmazon\n\n\n\n\n\nEditor CRC Press - Chapman & Hall\n\n\n\n\n\n\n\n\n\nLearn about essential public health metrics like DALYs, YLL, and YLD, which provide crucial indicators of a population‚Äôs health status. Dive into recent infectious disease outbreaks, including the impact of Covid19, and explore the most affected locations.\nWith a mix of explanatory text and hands-on R code examples, you‚Äôll become proficient in analyzing real-world case studies. We‚Äôll compare health metrics across different locations and test predictive models using tidymodels and INLA. Plus, you‚Äôll discover the power of spatial visualization techniques using ggplot2, leaflet, sf, rgdal, and other popular R packages. Whether you‚Äôre a beginner or an advanced practitioner, this book is designed to enhance your understanding and skills in health data analysis.\nIdeal for early-stage practitioners and graduate students in public health, this resource is your guide to unlocking the full potential of R for spatial and health metrics analysis.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/talks/index.html",
    "href": "about/talks/index.html",
    "title": "Federica Gazzelloni",
    "section": "",
    "text": "ShinyConf2024 - Shiny4Good Talk\nDate: April 2024\nAbstract: This talk will offer insights gained from active participation in the Data Science Learning Community through participation in Shiny book clubs. By exploring the unique dynamics of these forums, I will highlight the collaborative and knowledge-sharing atmosphere that the Data Science Learning Community cultivates. The talk will draw examples from the book clubs to illustrate how practical discussions and exercises amplify the learning experience.\n#beginner #Shiny4Good #shinyapp #showcase #rstats #datascience\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/papers/index.html",
    "href": "about/papers/index.html",
    "title": "Federica Gazzelloni",
    "section": "",
    "text": "Authored & Co-authored Publications\n\n\n\nList of Publications 2020 to Present\n\nN\nYear\nJournal\nTitle\nAuthor\nPubid\n\n\n\n18\n2025\nThe Lancet\nBurden of 375 diseases and injuries, risk-attributable burden of 88 risk factors, and healthy life expectancy in 204 countries and territories, including 660 subnational¬†‚Ä¶\nSI Hay, KL Ong, DF Santomauro, MA Aalipour, H Aalruz, HS Ababneh, ...\nQIV2ME_5wuYC\n\n\n17\n2025\nThe Lancet Public Health\nChanging life expectancy in European countries 1990‚Äì2021: a subanalysis of causes and risk factors from the Global Burden of Disease Study 2021\nN Steel, CMM Bauer-Staeb, JA Ford, C Abbafati, MA Abdalla, ...\naqlVkmm33-oC\n\n\n16\n2025\nThe Lancet\nGlobal age-sex-specific all-cause mortality and life expectancy estimates for 204 countries and territories and 660 subnational locations, 1950‚Äì2023: a demographic analysis for¬†‚Ä¶\nAE Schumacher, P Zheng, RM Barber, MA Aalipour, H Aalruz, ...\ndhFuZR0502QC\n\n\n15\n2025\nHealth policy\nThe role of burden of disease estimates in European Union Health policymaking: A systematic analysis of EU legislation\nJ Chen-Xu, D Grad, N Mahrouseh, JV Santos, S Riva, F Gazzelloni, ...\nmVmsd5A6BfQC\n\n\n14\n2025\nChapman and Hall/CRC\nHealth Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R\nF Gazzelloni\n4DMP91E08xMC\n\n\n13\n2025\nElsevier\nChanging life expectancy in European countries 1990‚Äì2021: a subanalysis of causes and risk factors from the Global Burden of Disease Study 2021\nS Nicholas, CMM Bauer-Staeb, JA Ford, C Abbafati, MA Abdalla, ...\nWp0gIr-vW9MC\n\n\n12\n2024\nThe Lancet\nGlobal age-sex-specific mortality, life expectancy, and population estimates in 204 countries and territories and 811 subnational locations, 1950‚Äì2021, and the impact of the¬†‚Ä¶\nAE Schumacher, HH Kyu, A Aali, C Abbafati, J Abbas, ...\n8k81kl-MbHgC\n\n\n11\n2024\nThe lancet\nGlobal fertility in 204 countries and territories, 1950‚Äì2021, with forecasts to 2100: a comprehensive demographic analysis for the Global Burden of Disease Study 2021\nNV Bhattacharjee, AE Schumacher, A Aali, YH Abate, R Abbasgholizadeh, ...\n5nxA0vEk-isC\n\n\n10\n2024\nThe Lancet\nGlobal, regional, and national stillbirths at 20 weeks' gestation or longer in 204 countries and territories, 1990‚Äì2021: findings from the Global Burden of Disease Study 2021\nH Comfort, TA McHugh, AE Schumacher, A Harris, EA May, KR Paulson, ...\nM3ejUd6NZC8C\n\n\n9\n2024\nBMC Public Health\nThe state of health in the European Union (EU-27) in 2019: a systematic analysis for the Global Burden of Disease study 2019\nJV Santos, A Padron-Monedero, B Bikbov, DA Grad, D Plass, EA Mechili, ...\nkNdYIx-mwKoC\n\n\n8\n2024\nEuropean Journal of Public Health\nExploring Burden of Disease Metrics in EU legislation: A Systematic Analysis\nJ Chen-Xu, DA Grad, N Mahrouseh, S Cuschieri, F Gazzelloni, EA Mechili, ...\n4TOpqqG69KYC\n\n\n7\n2024\nBritish Actuarial Journal\nLoss modelling from first principles\nP Parodi, D Thrumble, P Watson, Z Ji, A Wang, I Bhatia, J Lees, S Mealy, ...\nUebtZRa9Y70C\n\n\n6\n2023\nEpidemiology & Infection\nBurden of infectious disease studies in Europe and the United Kingdom: a review of methodological design choices\nP Charalampous, JA Haagsma, LS Jakobsen, V Gorasso, I Noguer, ...\nufrVoPGSRksC\n\n\n5\n2023\nArchives of Public Health\nBurden of disease attributable to risk factors in European countries: a scoping literature review\nV Gorasso, JN Morgado, P Charalampous, SM Pires, JA Haagsma, ...\n0EnyYjriUFMC\n\n\n4\n2022\nEuropean journal of public health\nBurden of non-communicable disease studies in Europe: a systematic review of data sources and methodological choices\nP Charalampous, V Gorasso, D Plass, SM Pires, E Von Der Lippe, ...\nW7OEmFMy1HYC\n\n\n3\n2022\nBMC public health\nMethodological considerations in injury burden of disease studies across Europe: a systematic literature review\nP Charalampous, E Pallari, V Gorasso, E Von der Lippe, ...\nWF5omc3nYNoC\n\n\n2\n2021\nEuropean Journal of Public Health\nThe state of health in the European Union in 2019\nJ Vasco Santos, A Padron Monedero, B Bikbov, DA Grad, D Plass, ...\neQOLeE2rZwMC\n\n\n1\n2021\nEuropean Journal of Public Health\nThe state of health in the European Union in 2019\nJV Santos, AP Monedero, B Bikbov, DA Grad, D Plass, EA Mechili, ...\nYsMSGLbcyi4C\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/rpackages/index.html",
    "href": "content/rpackages/index.html",
    "title": "Overview",
    "section": "",
    "text": "Over time, a number of small R tools I developed for my own work have turned into packages that I now share publicly. To make them easier to find and explore, I have added a dedicated R Packages section to my website.\nThis section brings together the public R packages I have developed with links to their documentation, source code, and examples of use. The aim is not only to list packages, but to provide context: why they exist, what problems they address, and how they fit into real analytical workflows."
  },
  {
    "objectID": "content/rpackages/index.html#what-you-will-find-here",
    "href": "content/rpackages/index.html#what-you-will-find-here",
    "title": "Overview",
    "section": "What you will find here",
    "text": "What you will find here\nThe R Packages section includes:\n\nPackage names and short descriptions, to quickly understand the purpose of each project\nLinks to CRAN and/or GitHub repositories, for installation and source code access\nKey features, highlighting the main ideas and design choices behind each package\nDocumentation, vignettes, and tutorials, where available, to support practical use\nOngoing updates, as packages evolve, new features are added, or new projects are released\n\nAll packages are presented in one place to make exploration straightforward and transparent."
  },
  {
    "objectID": "content/rpackages/index.html#why-a-dedicated-section",
    "href": "content/rpackages/index.html#why-a-dedicated-section",
    "title": "Overview",
    "section": "Why a dedicated section",
    "text": "Why a dedicated section\nOpen-source work is often scattered across repositories, issues, and releases. While GitHub and CRAN are essential platforms, they are not always the easiest entry point for understanding why a package exists or how it is meant to be used.\nThis dedicated section provides a more coherent view of my work as an R developer, linking technical tools back to the teaching, research, and communication contexts in which they were created. It also reflects a broader commitment to reproducibility, clarity, and sharing reusable solutions with the community."
  },
  {
    "objectID": "content/rpackages/index.html#selected-packages",
    "href": "content/rpackages/index.html#selected-packages",
    "title": "Overview",
    "section": "Selected packages",
    "text": "Selected packages\nThe tab currently features packages covering different aspects of data analysis and development, for example:\n\n{oregonfrogs}: A package for analyzing and visualizing Oregon frog population data.\n{hmsidwR}: Tools for health metrics data collection as long as extra tools for unzipping data and for spatial interpolation using inverse distance weighting.\n{typeR}: A package providing utilities for working with type-safe data structures in R providing live coding simulations.\n\nAdditional packages and updates will appear as projects mature or are made public.\nYou can explore the full list directly on the website by visiting the R Packages section.\n‚∏ª\nThis space will continue to grow over time. If you use any of these packages, have suggestions, or would like to discuss potential improvements or collaborations, feel free to get in touch via the contact page or GitHub.\nThank you for following and supporting my work in the R ecosystem."
  },
  {
    "objectID": "content/rpackages/posts/hmsidwR/index.html",
    "href": "content/rpackages/posts/hmsidwR/index.html",
    "title": "hmsidwR R-package",
    "section": "",
    "text": "This is the package that covers all datasets used in the Health Metrics and the Spread of Infectious Diseases: Machine Learning Application and Spatial Modelling Analysis with R book.\nBOOK: https://bookdown.org/fede_gazzelloni/hmsidR/\nGITHUB: https://github.com/Fgazzelloni/hmsidwR\nVIGNETTE: https://fgazzelloni.github.io/hmsidwR/\n\n\n\n\n\n‚ÄúIt‚Äôs been an incredible journey, and while the book is nearing its completion, I feel a mix of emotions‚Äîboth the excitement of sharing it with you and a touch of sadness that this creative process is coming to an end. üòå\nWriting this book has been an immensely rewarding experience. I‚Äôve spent countless hours into selecting the right material, striving to present it in a way that‚Äôs accessible and helpful for early-career researchers and students. One of the biggest challenges was deciding what to include and what to leave out, constantly refining the content to ensure it‚Äôs both comprehensive and easy to follow.\nEven though the book is almost finished, my mind is still buzzing with ideas. There‚Äôs always that lingering thought of what else could be added to make it even better. But at some point, you have to step back and trust that what you‚Äôve created will serve its purpose. And now, it‚Äôs time to see how it will resonate with you, the readers.\nAs I reflect on this journey, I‚Äôm filled with gratitude for all the support and feedback I‚Äôve received along the way. Your encouragement has been invaluable in shaping this book into what it is today.\nSo, what‚Äôs next? While this project may be wrapping up, my passion for writing and sharing knowledge is far from over. Stay tuned for more updates and future projects! In the meantime, I invite you to dive into the book, explore its content, and share your thoughts. Your insights will be crucial as I make final touches and consider potential additions.\nThank you for being a part of this journey. I hope the book serves as a valuable resource for your research and studies. Let‚Äôs see where this new chapter takes us!\nHappy learning! ü§ìüìö‚ú®‚Äù"
  },
  {
    "objectID": "content/rpackages/posts/hmsidwR/index.html#overview",
    "href": "content/rpackages/posts/hmsidwR/index.html#overview",
    "title": "hmsidwR R-package",
    "section": "",
    "text": "This is the package that covers all datasets used in the Health Metrics and the Spread of Infectious Diseases: Machine Learning Application and Spatial Modelling Analysis with R book.\nBOOK: https://bookdown.org/fede_gazzelloni/hmsidR/\nGITHUB: https://github.com/Fgazzelloni/hmsidwR\nVIGNETTE: https://fgazzelloni.github.io/hmsidwR/\n\n\n\n\n\n‚ÄúIt‚Äôs been an incredible journey, and while the book is nearing its completion, I feel a mix of emotions‚Äîboth the excitement of sharing it with you and a touch of sadness that this creative process is coming to an end. üòå\nWriting this book has been an immensely rewarding experience. I‚Äôve spent countless hours into selecting the right material, striving to present it in a way that‚Äôs accessible and helpful for early-career researchers and students. One of the biggest challenges was deciding what to include and what to leave out, constantly refining the content to ensure it‚Äôs both comprehensive and easy to follow.\nEven though the book is almost finished, my mind is still buzzing with ideas. There‚Äôs always that lingering thought of what else could be added to make it even better. But at some point, you have to step back and trust that what you‚Äôve created will serve its purpose. And now, it‚Äôs time to see how it will resonate with you, the readers.\nAs I reflect on this journey, I‚Äôm filled with gratitude for all the support and feedback I‚Äôve received along the way. Your encouragement has been invaluable in shaping this book into what it is today.\nSo, what‚Äôs next? While this project may be wrapping up, my passion for writing and sharing knowledge is far from over. Stay tuned for more updates and future projects! In the meantime, I invite you to dive into the book, explore its content, and share your thoughts. Your insights will be crucial as I make final touches and consider potential additions.\nThank you for being a part of this journey. I hope the book serves as a valuable resource for your research and studies. Let‚Äôs see where this new chapter takes us!\nHappy learning! ü§ìüìö‚ú®‚Äù"
  },
  {
    "objectID": "content/podcasts/index.html",
    "href": "content/podcasts/index.html",
    "title": "Podcasts",
    "section": "",
    "text": "The Accounting of Human Suffering\n\n\n\nhealth metrics\n\ndata science\n\npublic health\n\n\n\n\n\n\n\n\n\nFeb 26, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nValuing Human Life in a Spreadsheet\n\n\n\nhealth metrics\n\ndata science\n\n\n\n\n\n\n\n\n\nFeb 9, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Plague Bills To Predictive Algorithms\n\n\n\nhealth metrics\n\ndata science\n\n\n\n\n\n\n\n\n\nFeb 8, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nBills of Mortality to AI\n\n\n\nhealth metrics\n\ndata science\n\n\n\n\n\n\n\n\n\nFeb 8, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nThe New Science of Fighting Infectious Disease\n\n\n\nhealth metrics\n\ndata science\n\nresearch\n\ninfectious disease\n\n\n\n\n\n\n\n\n\nJan 8, 2026\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/podcasts/posts/the-accounting-of-human-suffering/index.html#episode-overview",
    "href": "content/podcasts/posts/the-accounting-of-human-suffering/index.html#episode-overview",
    "title": "The Accounting of Human Suffering",
    "section": "Episode Overview",
    "text": "Episode Overview\n\n\"The Accounting of Human Suffering\" refers to the core mission of health metrics to quantify the total human toll of diseases, injuries, and risk factors. This process is centered on the Disability-Adjusted Life Year (DALY), a composite metric that measures one lost year of healthy life.\n\n\nThe discussion involves two primary pillars: morbidity and mortality. Years of Life Lost (YLLs) are calculated by comparing the age at death to a standard life expectancy to measure the impact of premature death. Years Lived with Disability (YLDs) are determined by multiplying the prevalence of a condition by a specific disability weight, which represents the severity of health loss on a scale from 0 to 1.\n\n\nKey technical \"ingredients\" for this accounting include standardized life tables, mortality rates, and incidence vs.¬†prevalence data. The sources emphasize that these metrics provide a standardized language to compare the burden of diverse conditions‚Äîsuch as the immediate mortality of a stroke versus the long-term impact of chronic respiratory disease‚Äîacross different global populations.\n\n\nUltimately, this accounting is used to identify health priorities, evaluate the effectiveness of interventions, and guide public health policy to achieve greater global health equity. The discussion also extends to emerging metrics like Well-being-Adjusted Health Expectancy (WAHE), which incorporate happiness, life satisfaction, and social connectedness into the traditional clinical framework."
  },
  {
    "objectID": "content/podcasts/posts/the-accounting-of-human-suffering/index.html#key-topics-discussed",
    "href": "content/podcasts/posts/the-accounting-of-human-suffering/index.html#key-topics-discussed",
    "title": "The Accounting of Human Suffering",
    "section": "Key Topics Discussed",
    "text": "Key Topics Discussed\n\nYears of Life Lost (YLLs) are calculated by comparing the age at death to a standard life expectancy to measure the impact of premature death\nprevalence data"
  },
  {
    "objectID": "content/podcasts/posts/from-plague-bills-to-predictive-algorithms/index.html#episode-overview",
    "href": "content/podcasts/posts/from-plague-bills-to-predictive-algorithms/index.html#episode-overview",
    "title": "From Plague Bills To Predictive Algorithms",
    "section": "Episode Overview",
    "text": "Episode Overview\n\nFrom death records to predictive algorithms.\n\n\nThis episode traces 400 years of health intelligence‚Äîfrom counting who died to forecasting what comes next.\n\n\nWe move from John Graunt‚Äôs 17th-century Bills of Mortality, through modern burden-of-disease metrics like DALYs, to today‚Äôs machine-learning models that learn from data to anticipate outbreaks and inform prevention."
  },
  {
    "objectID": "content/podcasts/posts/from-plague-bills-to-predictive-algorithms/index.html#key-topics-discussed",
    "href": "content/podcasts/posts/from-plague-bills-to-predictive-algorithms/index.html#key-topics-discussed",
    "title": "From Plague Bills To Predictive Algorithms",
    "section": "Key Topics Discussed",
    "text": "Key Topics Discussed\n\nData-driven insights\nHealth metrics analysis\nInnovative approaches to public health"
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "Overview",
    "section": "",
    "text": "Welcome to my Blog section! My goal is to provide valuable information and perspectives that can help readers navigate the ever-evolving landscape of data science and programming.\nHere, I share my thoughts on a variety of topics, including:\n\nData Science: Discussions on methodologies, best practices, and emerging trends in data analysis\nR Programming: Tutorials, code snippets, and reflections on using R for data manipulation, analysis, and visualization.\nData Visualization: Showcasing effective visualizations and discussing techniques for conveying complex data\n\nThrough this blog, I aim to foster a community of learners and practitioners who are passionate about data science and programming. Whether you‚Äôre a beginner or an experienced professional, I hope you‚Äôll find something here that sparks your interest and helps you on your journey.\nFeel free to explore the latest posts and join the conversation!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualizations\n\n\nIn this section, I showcase a curated collection of my favorite data visualizations created using R, highlighting various themes and datasets. For a full experience, please visit: Unlocking The Power of Data Visualization with R Website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analytics and Statistical Techniques\n\n\nIn this section, I explore various data analytics and statistical techniques used to derive insights from data.\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/blog/posts/stats/posts/simpson_paradox/index.html",
    "href": "content/blog/posts/stats/posts/simpson_paradox/index.html",
    "title": "Understanding Simpson‚Äôs Paradox: A Simple Explanation",
    "section": "",
    "text": "Simpson's Paradox is a statistical phenomenon where a trend appears in different groups of data but disappears or reverses when these groups are combined. This paradox highlights the importance of considering confounding variables and understanding the causal relationship between variables.\nExample Scenario: Work Environment\nLet‚Äôs consider a hypothetical work environment where the number of women (W) is greater than the number of men (M). However, when looking at the distribution of managerial positions (P), it seems that more men occupy higher-level positions compared to women.\nNow, suppose there‚Äôs a characteristic Z, representing gender, and you suspect it might influence the choice of assigning a managerial position (P) because a specific time dedicated to a critical task (T) is primarily marketed toward men (M).\nTo illustrate this paradox, we‚Äôll create synthetic data in R.\nInstall and load necessary library\n\nlibrary(dplyr)\n\nSet seed for reproducibility\n\nset.seed(123)\n\nGenerate synthetic data\n\nn &lt;- 1000  # Number of employees \nW &lt;- round(runif(n, 200, 800))  # Number of women\nM &lt;- n - W  # Number of men\n\nAssign managerial positions based on gender and a confounding variable\n\nP_W &lt;- round(runif(W, 0, 1))  # 0 for no managerial position, 1 for managerial position for women\nP_M &lt;- round(runif(M, 0.2, 1))  # Higher chance of managerial position for men due to confounding variable\n\nCreate a data frame\n\nw &lt;- tibble(gender=\"Women\",count=W,manager=P_W)\n\nm &lt;- tibble(gender=\"Men\",count=M,manager=P_M)\n\ndata &lt;- rbind(w,m)\ndata%&gt;%head\n\n# A tibble: 6 √ó 3\n  gender count manager\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Women    373       0\n2 Women    673       1\n3 Women    445       0\n4 Women    730       1\n5 Women    764       1\n6 Women    227       0\n\n\nDisplay the initial summary\n\nsummary(data)\n\n    gender              count        manager      \n Length:2000        Min.   :200   Min.   :0.0000  \n Class :character   1st Qu.:352   1st Qu.:0.0000  \n Mode  :character   Median :500   Median :1.0000  \n                    Mean   :500   Mean   :0.5615  \n                    3rd Qu.:648   3rd Qu.:1.0000  \n                    Max.   :800   Max.   :1.0000  \n\n\nIn this example, we have created a dataset with a larger number of women, but the chance of obtaining a managerial position for men is influenced by a confounding variable. Now, let‚Äôs examine the paradox.\nCalculate the proportion of managerial positions for each gender\n\nproportion_table &lt;- data %&gt;%\n  group_by(gender) %&gt;%\n  summarize(proportion = mean(manager))\n\nDisplay the proportions\n\nproportion_table\n\n# A tibble: 2 √ó 2\n  gender proportion\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Men         0.626\n2 Women       0.497\n\n\n\nlibrary(ggplot2)\nproportion_table%&gt;%\n  ggplot(aes(gender,proportion,fill=gender))+\n  geom_col(color=\"white\",show.legend = F)+\n  scale_fill_viridis_d()+\n  labs(title = \"Proportion of Managers by Gender\",\n       subtitle = \"Example of the Simpson's Paradox\",\n       x=\"\",\n       caption = \"Data: Syntetic | Graphics: Federica Gazzelloni\") +\n  coord_equal()+\n  ggthemes::theme_pander()+\n  theme(plot.caption = element_text(hjust = 0.5))\n\n\n\n\n\n\n\nIn this scenario, when examining the proportion of managerial positions within each gender group, it might appear that men have a higher chance. However, when we consider the entire dataset, we may find the opposite due to the confounding variable.\nThe key takeaway is that understanding causation is crucial, and Simpson‚Äôs Paradox emphasizes the need to consider confounding factors when interpreting data.\n\n\n Back to top"
  },
  {
    "objectID": "content/blog/posts/stats/posts/bayesian_statistics/index.html",
    "href": "content/blog/posts/stats/posts/bayesian_statistics/index.html",
    "title": "Bayesian Statistics model comparison",
    "section": "",
    "text": "In this post I‚Äôll go through some differences between Bayesian statistical packages in R. Bayesian statistics involves probabilities. This means that the probability of an event to occur is considered in the modeling procedure, and is mainly used in for making inferences, and can be used for an analysis of the speculation of the root cause of a phenomenon under the term of causal inference.\n\n\n\nBayesian Statistics\n\n\n\nIn more details, when Bayesian statistics is performed, the response variable is tested against (causal) predictors with the application of suited prior distributions, and the use of the likelihood function, to finally produce a posterior distribution which should be as much as possible close to the real future outcome of the response variable distribution.\nThe prior distribution is the starting point; it is the probability distribution on which the future outcome is linked to, such as the probability to have a Girl given the probability to have had a Boy.\n\\[P( \\text{ Girl } | \\text{ Boy })\\]\nThe probability to have had a Boy is the prior, while the conditional probability to have a Girl is the posterior.\nBriefly, here is a comparison between different R packages that use Bayesian inference for the calculation of the model probability distribution of the posterior.\nThe Stan model engine, for model replication and prediction is used in conjunction with the Montecarlo simulation technique for the best model solution. The Stan model engine is applied in the following packages:\n\nbrms\nrstanarm\nrethinking\nMCMCglmm\n\nAll of these packages adapt and adjust different model options for a modeling procedure which is the result of the best combination of efficiency to increase productivity and effectiveness, to identify and remove unnecessary steps, automate repetitive tasks, and utilize the most suitable software tools.\nThis is the original source code that I have updated: https://www.jstatsoft.org/article/view/v080i01\n\nA wide range of distributions and link functions are supported, allowing users to fit - among others - linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. (The Brms package)\n\nLoading required packages\n\nlibrary(tidyverse)\nlibrary(\"brms\") \nlibrary(\"rstanarm\")\nlibrary(\"rethinking\") \nlibrary(\"MCMCglmm\")  \n\nHelper function to better compute the effective sample size\n\neff_size &lt;- function(x) {\n  if (is(x, \"brmsfit\")) {\n    samples &lt;- as.data.frame(x$fit)\n  } else if (is(x, \"stanreg\")) {\n    samples &lt;- as.data.frame(x$stanfit)\n  } else if (is(x, \"ulam\")) {\n    samples &lt;- as.data.frame(x@stanfit)\n  } else if (is(x, \"stanfit\")) {\n    samples &lt;- as.data.frame(x)\n  } else if (is(x, \"MCMCglmm\")) {\n    samples &lt;- cbind(x$Sol, x$VCV)\n  } else {\n    stop(\"invalid input\")\n  }\n  # call an internal function of rstan\n  floor(apply(samples, MARGIN = 2, FUN = rstan:::ess_rfun))\n}\n\n\n\n# only used for Stan packages\niter &lt;- 6000  \nwarmup &lt;- 1000\nchains &lt;- 1\nadapt_delta &lt;- 0.8\n\n# only used for MCMCglmm\nnitt &lt;- 35000  \nburnin &lt;- 10000\nthin &lt;- 5\n# leads to 5000 posterior samples\n\n\n\n\nprior_dye_brms &lt;- c(set_prior(\"normal(0, 2000)\", class = \"Intercept\"),\n                    set_prior(\"cauchy(0, 50)\", class = \"sd\"),\n                    set_prior(\"cauchy(0, 50)\", class = \"sigma\"))\n\ndye_brms &lt;- brm(Yield ~ 1 + (1 | Batch), \n                data = lme4::Dyestuff, \n                prior = prior_dye_brms, \n                chains = 0)\n\ntime_dye_brms &lt;- system.time(capture.output(\n  dye_brms &lt;- update(dye_brms, \n                     iter = iter, \n                     warmup = warmup, \n                     chains = chains,\n                     control = list(adapt_delta = adapt_delta))\n))\n# summary(dye_brms)\neff_dye_brms &lt;- min(eff_size(dye_brms)) / time_dye_brms[[1]]\n\n\n\ntime_dye_rstanarm &lt;- system.time(capture.output(\n  dye_rstanarm &lt;- stan_glmer(Yield ~ 1 + (1 | Batch), data = lme4::Dyestuff,\n                             prior_intercept = normal(0, 2000),\n                             iter = iter, warmup = warmup, chains = chains,\n                             adapt_delta = adapt_delta)\n))\n# summary(dye_rstanarm)\neff_dye_rstanarm &lt;- min(eff_size(dye_rstanarm)) / time_dye_rstanarm[[1]]\n\n\n\nd &lt;-  lme4::Dyestuff\n\ndat &lt;- list(\n  Yield = d$Yield,\n  Batch = d$Batch\n)\n\ndye_flist &lt;- alist(\n  Yield ~ dnorm(eta, sigma),\n  eta &lt;- a + a_Batch[Batch],\n  a ~ dnorm(0,2000),\n  a_Batch[Batch] ~ dnorm(0, sd_Batch),\n  sigma ~ dcauchy(0, 50),\n  sd_Batch ~ dcauchy(0, 50))\n\ndye_rethinking &lt;- ulam(dye_flist, \n                       data = dat, \n                       chains=1,\n                       cores = 4,\n                       sample = TRUE)\n\ntime_dye_rethinking &lt;- system.time(capture.output(\n  dye_rethinking &lt;- update(dye_rethinking,\n       iter = iter, \n       warmup = warmup, \n       chains = chains,\n       control = list(adapt_delta = adapt_delta))\n))\n\n\n# summary(dye_rethinking)\neff_dye_rethinking &lt;- min(eff_size(dye_rethinking)) / time_dye_rethinking[[1]]\n\n\n\ntime_dye_MCMCglmm &lt;- system.time(capture.output(\n  dye_MCMCglmm &lt;- MCMCglmm(Yield ~ 1, \n                           random = ~ Batch, data = lme4::Dyestuff, \n                           thin = thin, nitt = nitt, burnin = burnin)\n))\n# summary(dye_MCMCglmm)\neff_dye_MCMCglmm &lt;- min(eff_size(dye_MCMCglmm)) / time_dye_MCMCglmm[[1]]\n\n\n\nprint(c(brms = eff_dye_brms, \n        rstanarm = eff_dye_rstanarm, \n        rethinking = eff_dye_rethinking, \n        MCMCglmm = eff_dye_MCMCglmm))\n\n\n\nbrms\nrstanarm\nrethinking\nMCMCglmm\n\n\n559.55398\n202.97177\n3660.71429\n34.11514"
  },
  {
    "objectID": "content/blog/posts/stats/posts/bayesian_statistics/index.html#dyestuff",
    "href": "content/blog/posts/stats/posts/bayesian_statistics/index.html#dyestuff",
    "title": "Bayesian Statistics model comparison",
    "section": "",
    "text": "prior_dye_brms &lt;- c(set_prior(\"normal(0, 2000)\", class = \"Intercept\"),\n                    set_prior(\"cauchy(0, 50)\", class = \"sd\"),\n                    set_prior(\"cauchy(0, 50)\", class = \"sigma\"))\n\ndye_brms &lt;- brm(Yield ~ 1 + (1 | Batch), \n                data = lme4::Dyestuff, \n                prior = prior_dye_brms, \n                chains = 0)\n\ntime_dye_brms &lt;- system.time(capture.output(\n  dye_brms &lt;- update(dye_brms, \n                     iter = iter, \n                     warmup = warmup, \n                     chains = chains,\n                     control = list(adapt_delta = adapt_delta))\n))\n# summary(dye_brms)\neff_dye_brms &lt;- min(eff_size(dye_brms)) / time_dye_brms[[1]]\n\n\n\ntime_dye_rstanarm &lt;- system.time(capture.output(\n  dye_rstanarm &lt;- stan_glmer(Yield ~ 1 + (1 | Batch), data = lme4::Dyestuff,\n                             prior_intercept = normal(0, 2000),\n                             iter = iter, warmup = warmup, chains = chains,\n                             adapt_delta = adapt_delta)\n))\n# summary(dye_rstanarm)\neff_dye_rstanarm &lt;- min(eff_size(dye_rstanarm)) / time_dye_rstanarm[[1]]\n\n\n\nd &lt;-  lme4::Dyestuff\n\ndat &lt;- list(\n  Yield = d$Yield,\n  Batch = d$Batch\n)\n\ndye_flist &lt;- alist(\n  Yield ~ dnorm(eta, sigma),\n  eta &lt;- a + a_Batch[Batch],\n  a ~ dnorm(0,2000),\n  a_Batch[Batch] ~ dnorm(0, sd_Batch),\n  sigma ~ dcauchy(0, 50),\n  sd_Batch ~ dcauchy(0, 50))\n\ndye_rethinking &lt;- ulam(dye_flist, \n                       data = dat, \n                       chains=1,\n                       cores = 4,\n                       sample = TRUE)\n\ntime_dye_rethinking &lt;- system.time(capture.output(\n  dye_rethinking &lt;- update(dye_rethinking,\n       iter = iter, \n       warmup = warmup, \n       chains = chains,\n       control = list(adapt_delta = adapt_delta))\n))\n\n\n# summary(dye_rethinking)\neff_dye_rethinking &lt;- min(eff_size(dye_rethinking)) / time_dye_rethinking[[1]]\n\n\n\ntime_dye_MCMCglmm &lt;- system.time(capture.output(\n  dye_MCMCglmm &lt;- MCMCglmm(Yield ~ 1, \n                           random = ~ Batch, data = lme4::Dyestuff, \n                           thin = thin, nitt = nitt, burnin = burnin)\n))\n# summary(dye_MCMCglmm)\neff_dye_MCMCglmm &lt;- min(eff_size(dye_MCMCglmm)) / time_dye_MCMCglmm[[1]]\n\n\n\nprint(c(brms = eff_dye_brms, \n        rstanarm = eff_dye_rstanarm, \n        rethinking = eff_dye_rethinking, \n        MCMCglmm = eff_dye_MCMCglmm))\n\n\n\nbrms\nrstanarm\nrethinking\nMCMCglmm\n\n\n559.55398\n202.97177\n3660.71429\n34.11514"
  },
  {
    "objectID": "content/blog/posts/stats/posts/rladies/index.html",
    "href": "content/blog/posts/stats/posts/rladies/index.html",
    "title": "R-Ladies Events Stats",
    "section": "",
    "text": "Let‚Äôs scrap the R-Ladies chapters events from Meetup.com We can use the meetupr package.\nurlname &lt;- c(\"rladies-paris\",\"rladies-rome\")\nevents &lt;- purrr::map(urlname,get_events)\ndat &lt;- dplyr::bind_rows(events)\n\n\n\nRLadies Wordcloud\n\n\n\nLoad necessary libraries\n\nsuppressPackageStartupMessages({\n  library(meetupr)\n  library(jsonlite)\n  library(tidyverse)\n  library(stringr)\n  library(tidytext)\n  library(wordcloud)\n  library(topicmodels)\n  library(broom)\n  library(scales)\n})\n\n\ntheme_set(theme_bw())\n\n\n\nurlname &lt;- \"rladies-rome\"\nevents &lt;- get_events(urlname)\ndplyr::arrange(events, desc(time))%&gt;%\n  head()\n\n\nurlname &lt;- c(\"rladies-paris\",\"rladies-rome\")\nevents &lt;- purrr::map(urlname,get_events)\ndat &lt;- dplyr::bind_rows(events)\ndat%&gt;%\n  mutate(link=gsub(\"https://www.meetup.com/\",\"\",link),\n         chapter=stringr::str_extract(link, \"^rladies(.+?)/\"),\n         chapter=gsub(\"/\",\"\",chapter))%&gt;%\n  count(chapter)\n\n\nTo do it for all chapters on meetup, we need the list of the chapters from the rladies github archive.\n\ndata &lt;- jsonlite::fromJSON('https://raw.githubusercontent.com/rladies/meetup_archive/main/data/events.json')\n\n\nchapter &lt;- data %&gt;%\n  count(group_urlname)%&gt;%\n  filter(!str_detect(group_urlname,\"@\"))\nchapters &lt;- chapter$group_urlname\n\nevents &lt;- purrr::map(chapters,get_events)\n# saveRDS(events,\"events.rds\")\n# another way\n# x &lt;- lapply(paths, func)\n# res &lt;- dplyr::bind_rows(x)\n\n\nbind_rows(events[1])%&gt;%head()\n\n\ndat &lt;- dplyr::bind_rows(events)\n# saveRDS(dat,\"dat.rds\")\n\n\ndat1 &lt;- dat%&gt;% \n  mutate(link=gsub(\"https://www.meetup.com/\",\"\",link),\n         chapter=stringr::str_extract(link, \"^rladies(.+?)/\"),\n         chapter=gsub(\"/\",\"\",chapter))%&gt;%\n  relocate(chapter)\n\n# saveRDS(dat1,\"dat1.rds\")\n\n\ndat2 &lt;- dat1%&gt;%\n  select(time,chapter,title,going,venue_city,\n         venue_lon,venue_lat,venue_state,venue_country)%&gt;%\n  mutate(time=as.Date(time))%&gt;%\n  arrange(desc(going))\n\n\ndat2%&gt;%\n  mutate(year=year(time),.after = time)%&gt;%\n  pull(year)%&gt;%\n  summary(year)\n\n\ndat3 &lt;- dat2%&gt;%\n  tidytext::unnest_tokens(word, title,drop = F)%&gt;%\n  select(chapter,title,going,word)%&gt;% \n  anti_join(get_stopwords())%&gt;%\n  filter(!str_length(word)&lt;=3)\n\n\ndat3%&gt;%\n  count(word, sort = TRUE) %&gt;%\n  with(wordcloud::wordcloud(word, n, max.words = 100))\n\n\n\nchapters_dtm &lt;- dat3 %&gt;%\n  count(title, word, sort = TRUE)%&gt;%\n  cast_dtm(title, word, n)\n\nchapters_dtm\n\n\nchapters_lda &lt;- topicmodels::LDA(chapters_dtm, \n                    k = 4, \n                    control = list(seed = 1234))\nchapters_lda_td &lt;- tidy(chapters_lda)\nchapters_lda_td\ntop_terms &lt;- chapters_lda_td %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 5) %&gt;%\n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntop_terms\n\n\ntop_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(term, beta)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(vars(topic), scales = \"free_x\")\n\n\nassignments &lt;- augment(chapters_lda, data = chapters_dtm)\n\nassignments%&gt;%\n  filter(!term==\"ladies\")\n\n# how words in titles changed overtime\ninaug_freq &lt;- dat3 %&gt;%\n  inner_join(dat2,by=c(\"chapter\",\"title\",\"going\"))%&gt;%#View\n  count(time, word) %&gt;%\n  complete(time, word, fill = list(n = 0)) %&gt;%\n  group_by(time) %&gt;%\n  mutate(time_total = sum(n), \n         percent = n / time_total) %&gt;%\n  ungroup()\n\ninaug_freq\n\n\n# library(broom)\nmodels &lt;- inaug_freq %&gt;%\n  group_by(word) %&gt;%\n  filter(sum(n) &gt; 50) %&gt;%\n  group_modify(\n    ~ tidy(glm(cbind(n, time_total - n) ~ time, ., \n               family = \"binomial\"))\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(term == \"time\")\n\nmodels\nmodels %&gt;%\n  filter(term == \"time\") %&gt;%\n  arrange(desc(abs(estimate)))\n\n\nmodels %&gt;%\n  mutate(adjusted.p.value = p.adjust(p.value)) %&gt;%\n  ggplot(aes(estimate, adjusted.p.value)) +\n  geom_point(shape=\".\") +\n  #scale_y_log10() +\n  geom_text(aes(label = word), \n            #vjust = 1, hjust = 1, \n            check_overlap = TRUE) +\n  labs(x = \"Estimated change over time\", y = \"Adjusted p-value\")\n\n\nmodels %&gt;%\n  slice_max(abs(estimate), n = 6) %&gt;%\n  inner_join(inaug_freq) %&gt;%\n  ggplot(aes(time, percent)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(word)) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(y = \"Frequency of word in speech\")"
  },
  {
    "objectID": "content/blog/posts/stats/posts/rladies/index.html#overview",
    "href": "content/blog/posts/stats/posts/rladies/index.html#overview",
    "title": "R-Ladies Events Stats",
    "section": "",
    "text": "Let‚Äôs scrap the R-Ladies chapters events from Meetup.com We can use the meetupr package.\nurlname &lt;- c(\"rladies-paris\",\"rladies-rome\")\nevents &lt;- purrr::map(urlname,get_events)\ndat &lt;- dplyr::bind_rows(events)\n\n\n\nRLadies Wordcloud\n\n\n\nLoad necessary libraries\n\nsuppressPackageStartupMessages({\n  library(meetupr)\n  library(jsonlite)\n  library(tidyverse)\n  library(stringr)\n  library(tidytext)\n  library(wordcloud)\n  library(topicmodels)\n  library(broom)\n  library(scales)\n})\n\n\ntheme_set(theme_bw())\n\n\n\nurlname &lt;- \"rladies-rome\"\nevents &lt;- get_events(urlname)\ndplyr::arrange(events, desc(time))%&gt;%\n  head()\n\n\nurlname &lt;- c(\"rladies-paris\",\"rladies-rome\")\nevents &lt;- purrr::map(urlname,get_events)\ndat &lt;- dplyr::bind_rows(events)\ndat%&gt;%\n  mutate(link=gsub(\"https://www.meetup.com/\",\"\",link),\n         chapter=stringr::str_extract(link, \"^rladies(.+?)/\"),\n         chapter=gsub(\"/\",\"\",chapter))%&gt;%\n  count(chapter)\n\n\nTo do it for all chapters on meetup, we need the list of the chapters from the rladies github archive.\n\ndata &lt;- jsonlite::fromJSON('https://raw.githubusercontent.com/rladies/meetup_archive/main/data/events.json')\n\n\nchapter &lt;- data %&gt;%\n  count(group_urlname)%&gt;%\n  filter(!str_detect(group_urlname,\"@\"))\nchapters &lt;- chapter$group_urlname\n\nevents &lt;- purrr::map(chapters,get_events)\n# saveRDS(events,\"events.rds\")\n# another way\n# x &lt;- lapply(paths, func)\n# res &lt;- dplyr::bind_rows(x)\n\n\nbind_rows(events[1])%&gt;%head()\n\n\ndat &lt;- dplyr::bind_rows(events)\n# saveRDS(dat,\"dat.rds\")\n\n\ndat1 &lt;- dat%&gt;% \n  mutate(link=gsub(\"https://www.meetup.com/\",\"\",link),\n         chapter=stringr::str_extract(link, \"^rladies(.+?)/\"),\n         chapter=gsub(\"/\",\"\",chapter))%&gt;%\n  relocate(chapter)\n\n# saveRDS(dat1,\"dat1.rds\")\n\n\ndat2 &lt;- dat1%&gt;%\n  select(time,chapter,title,going,venue_city,\n         venue_lon,venue_lat,venue_state,venue_country)%&gt;%\n  mutate(time=as.Date(time))%&gt;%\n  arrange(desc(going))\n\n\ndat2%&gt;%\n  mutate(year=year(time),.after = time)%&gt;%\n  pull(year)%&gt;%\n  summary(year)\n\n\ndat3 &lt;- dat2%&gt;%\n  tidytext::unnest_tokens(word, title,drop = F)%&gt;%\n  select(chapter,title,going,word)%&gt;% \n  anti_join(get_stopwords())%&gt;%\n  filter(!str_length(word)&lt;=3)\n\n\ndat3%&gt;%\n  count(word, sort = TRUE) %&gt;%\n  with(wordcloud::wordcloud(word, n, max.words = 100))\n\n\n\nchapters_dtm &lt;- dat3 %&gt;%\n  count(title, word, sort = TRUE)%&gt;%\n  cast_dtm(title, word, n)\n\nchapters_dtm\n\n\nchapters_lda &lt;- topicmodels::LDA(chapters_dtm, \n                    k = 4, \n                    control = list(seed = 1234))\nchapters_lda_td &lt;- tidy(chapters_lda)\nchapters_lda_td\ntop_terms &lt;- chapters_lda_td %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 5) %&gt;%\n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntop_terms\n\n\ntop_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(term, beta)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(vars(topic), scales = \"free_x\")\n\n\nassignments &lt;- augment(chapters_lda, data = chapters_dtm)\n\nassignments%&gt;%\n  filter(!term==\"ladies\")\n\n# how words in titles changed overtime\ninaug_freq &lt;- dat3 %&gt;%\n  inner_join(dat2,by=c(\"chapter\",\"title\",\"going\"))%&gt;%#View\n  count(time, word) %&gt;%\n  complete(time, word, fill = list(n = 0)) %&gt;%\n  group_by(time) %&gt;%\n  mutate(time_total = sum(n), \n         percent = n / time_total) %&gt;%\n  ungroup()\n\ninaug_freq\n\n\n# library(broom)\nmodels &lt;- inaug_freq %&gt;%\n  group_by(word) %&gt;%\n  filter(sum(n) &gt; 50) %&gt;%\n  group_modify(\n    ~ tidy(glm(cbind(n, time_total - n) ~ time, ., \n               family = \"binomial\"))\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(term == \"time\")\n\nmodels\nmodels %&gt;%\n  filter(term == \"time\") %&gt;%\n  arrange(desc(abs(estimate)))\n\n\nmodels %&gt;%\n  mutate(adjusted.p.value = p.adjust(p.value)) %&gt;%\n  ggplot(aes(estimate, adjusted.p.value)) +\n  geom_point(shape=\".\") +\n  #scale_y_log10() +\n  geom_text(aes(label = word), \n            #vjust = 1, hjust = 1, \n            check_overlap = TRUE) +\n  labs(x = \"Estimated change over time\", y = \"Adjusted p-value\")\n\n\nmodels %&gt;%\n  slice_max(abs(estimate), n = 6) %&gt;%\n  inner_join(inaug_freq) %&gt;%\n  ggplot(aes(time, percent)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(word)) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(y = \"Frequency of word in speech\")"
  },
  {
    "objectID": "content/blog/posts/stats/posts/linear_prediction/index.html",
    "href": "content/blog/posts/stats/posts/linear_prediction/index.html",
    "title": "How does geom_smooth() make predictions",
    "section": "",
    "text": "Linear regression is a statistical technique used to represent the linear relationship between a response \\((y)\\) and a predictor \\((x)\\).\n\\[\ny= \\beta_0 + \\beta_1x\n\\]\n\n\n\n\n\n\nBelow we examine some mismatch in output when plotting the prediction results of a linear model made with the lm() and the predict() functions versus the output produced by the geom_smooth() layer in a ggplot() visualization.\nThe first dataset used is from the HistData package HistData::CholeraDeaths1849. We select just the deaths due to Cholera within 12 months in 1849, and visualize the trend in time of the number of deaths with the addition of a further layer made with the geom_smooth().\n?geom_smooth()\n\n?HistData::CholeraDeaths1849\n\nlibrary(tidyverse)\nlibrary(HistData)\ncholera &lt;- HistData::CholeraDeaths1849 %&gt;%\n  filter(cause_of_death==\"Cholera\")%&gt;%\n  select(date,deaths)\n  \ncholera %&gt;% head\n\n# A tibble: 6 √ó 2\n  date       deaths\n  &lt;date&gt;      &lt;dbl&gt;\n1 1849-01-01     13\n2 1849-01-02     19\n3 1849-01-03     28\n4 1849-01-04     24\n5 1849-01-05     23\n6 1849-01-06     39\n\n\n\nsummary(cholera)\n\n      date                deaths    \n Min.   :1849-01-01   Min.   :   0  \n 1st Qu.:1849-04-02   1st Qu.:   8  \n Median :1849-07-02   Median :  23  \n Mean   :1849-07-02   Mean   : 146  \n 3rd Qu.:1849-10-01   3rd Qu.: 192  \n Max.   :1849-12-31   Max.   :1121  \n\n\n\nggplot(cholera,aes(x=date,y=deaths))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  labs(title=\"Deaths due to Cholera in London (1849)\",\n       x=\"Date\",y=\"Cholera death\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nDeaths due to Cholera in London (1849)\n\n\n\nThe purpose of making a model is that to identify the inside pattern of a series of observations. This means that the model would need to be able to interpolate given observations in order to represent the overall pattern. As in the visualization above the geom_smooth() with the specification of the method=\"lm\" helps us visualize what the direction of a linear pattern would be on this data. If it is a growing pattern or not.\nClearly the points are shaping a bell distribution of deaths in time, and this is not the case of a linear relationship between date and cholera deaths, but we would like to dig into the output of the prediction of the application of a linear model on this data and then compare it with the output of the geom_smooth(method=\"lm\") line.\nLet‚Äôs apply a linear model to this data and make some rough predictions.\n\nmod &lt;- lm(deaths ~ date, data = cholera)\nsummary(mod)\n\n\nCall:\nlm(formula = deaths ~ date, data = cholera)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-265.88 -104.28  -59.87   11.19  930.92 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.954e+04  4.878e+03   6.056 3.49e-09 ***\ndate        6.678e-01  1.108e-01   6.026 4.13e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 223.1 on 363 degrees of freedom\nMultiple R-squared:  0.09094,   Adjusted R-squared:  0.08843 \nF-statistic: 36.31 on 1 and 363 DF,  p-value: 4.132e-09\n\n\nThe application of a linear model on this data produced an estimation of the intercept \\((\\beta_0)\\) and the slope \\((\\beta_1)\\).\n\\[\n\\beta_0=29540 \\text{ and } \\beta_1=0.6678\n\\] The intercept is the starting point of a linear model line on the y axes, while the slope is the inclination of the line, that can be positive or negative, indicating the growing or decreasing tendency of the relationship between the response and the predictor.\nLet‚Äôs draw this line.\n\nggplot(cholera, aes(x=date, y=deaths)) + \n    geom_point() + \n    geom_abline(slope=0.6678, intercept=29540,\n                col=\"pink\")+\n  theme_minimal()\n\n\n\n\n\n\n\nNow calculate the prediction and the Root Mean Squared Error (RMSE) to evaluate how the model worked.\n\npredictions &lt;- predict(mod, newdata = NULL)\n\nrmse &lt;- sqrt(mean((predictions - cholera$deaths)^2))\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 222.4774 \n\n\n\n\n\n\n\n\nWhat if we want to visualize the output of our prediction?\n\n\n\n\nggplot(cholera, aes(x=date)) + \n  geom_point(aes(y=deaths)) + \n  geom_smooth(aes(y=deaths),method = \"lm\",linewidth=2)+\n  geom_line(y=predictions, col=\"red\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow let‚Äôs use a different dataset. This data set comes from a paper by Brigham et al.¬†(2003) that analyses some tables from Farr‚Äôs report of the Registrar-General on mortality due to cholera in England in the years 1848-1849, during which there was a large epidemic throughout the country. In this case we do not have the time variable but the numbers of deaths are considered by 38 districts in London.\n?HistData::Cholera\n\ncholera2 &lt;- HistData::Cholera %&gt;%\n  rownames_to_column(var=\"id\")%&gt;%\n  select(id,district,cholera_deaths,popn)\n  \ncholera2 %&gt;% head\n\n  id            district cholera_deaths  popn\n1  1           Newington            907 63074\n2  2         Rotherhithe            352 17208\n3  3          Bermondsey            836 50900\n4  4 St George Southwark            734 45500\n5  5            St Olave            349 19278\n6  6          St Saviour            539 35227\n\n\nThe predictor in this case is a character, we are considering the relationship between the deaths rate and the districts, so we are looking at to see whether the deaths rate is different among 38 districts. The order in this case is arbitrary and this influences the pattern. The geom_smooth() is not drowning a line, it doesn‚Äôt know how the x-axis has to be ordered, because there is not a specified order to follow.\n\nggplot(cholera2, aes(x=id,y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nwhile if we set an order with as.integer(id), the line can be drawn but it hasn‚Äôt got much meaning. The trend is not going up or down because we are considering districts in the x-axis.\n\nggplot(cholera2, aes(x=as.integer(id),y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIf we consider the population, in the middle of 1849, a numeric vector, these values are by districts, each popn value corresponds to the level of population in one of the 38 districts. Let‚Äôs see what happens if we plot popn versus cholera_deaths.\n\nggplot(cholera2, aes(x=popn,y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow the values on the x-axis are numeric and have a meaning to be ordered from the lower to the highest but it is not a trend. Each point is one district population value with some deaths due to cholera. The geom_smooth line it is telling us that if the level of the population is higher, the level of deaths due to cholera is higher, than in other location with a lower level of population.\nBut we can evaluate the growing relationship between population level and numbers of deaths due to cholera.\nLet‚Äôs make a linear model and predict the future, roughly.\n\nmod2 &lt;- lm(cholera_deaths ~ popn , data = cholera2)\nmod2\n\n\nCall:\nlm(formula = cholera_deaths ~ popn, data = cholera2)\n\nCoefficients:\n(Intercept)         popn  \n  1.073e+02    4.357e-03  \n\n\nLet‚Äôs draw this line.\n\nggplot(cholera2, aes(x=popn, y=cholera_deaths)) + \n    geom_point() + \n    geom_abline(slope=0.004357, intercept=107.3,\n                col=\"pink\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\npredictions2 &lt;- predict(mod2, newdata = NULL)\n\nrmse &lt;- sqrt(mean((predictions2 - cholera2$cholera_deaths)^2))\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 281.7808 \n\n\n\n\n\n\n\n\nWhat if we want to visualize the output of our prediction?\n\n\n\n\nplot &lt;- ggplot(cholera2, aes(x=popn)) + \n  geom_point(aes(y=cholera_deaths)) + \n  geom_smooth(aes(y=cholera_deaths),method = \"lm\",linewidth=2)+\n  geom_line(y=predictions2, col=\"red\")+\n  labs(title=\"Cholera Deaths explanined by\\nLondon Districts Population (1849)\",\n       x=\"1849 London Population by 38 Districts\",\n       y=\"Cholera Deaths\",\n       caption=\"William Farr's Data on Cholera in London, 1849\")+\n  theme_minimal()\nplot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nCholera Deaths explanined by London Districts Population (1849)\n\n\n\n\nThink about that!\n\n\n\n\n\nImagine that we absolutely want to replicate the geom_smooth(method=\"lm\") line, we would need to consider some steps that the function takes in order to plot the prediction line that doesn‚Äôt much with ours this time. First think to consider is that we haven‚Äôt used new data but just produced the prediction from our dataset. But this is exactly as the same as before.\nThere is a function ggplot_build() that let‚Äôs us dig into the ggplot data manipulation used to make the geom_smooth line.\n?ggplot_build()\n\n# source: https://stackoverflow.com/questions/42673665/geom-smooth-gives-different-fit-than-nls-alone\ndat = ggplot_build(plot)$data[[2]]\n\n`geom_smooth()` using formula = 'y ~ x'\n\ndat &lt;- dat%&gt;%rename(cholera_drate=y,popn=x)\n\nThis time we use the newdata = dat insted of NULL.\n\npredictions3 &lt;- predict(mod2, newdata = dat)\n\nrmse &lt;- sqrt(mean((predictions3 - cholera2$cholera_deaths)^2))\n\nWarning in predictions3 - cholera2$cholera_deaths: longer object length is not\na multiple of shorter object length\n\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 411.1835 \n\n\n\nggplot(cholera2, aes(x=popn)) + \n  geom_point(aes(y=cholera_deaths)) + \n  geom_smooth(aes(y=cholera_deaths),method = \"lm\",linewidth=2)+\n  geom_line(data=dat,y=predictions3, col=\"red\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWe eventually matched the geom_smooth line, but why did our predictions on the original data result in that squiggly line?"
  },
  {
    "objectID": "content/blog/posts/stats/posts/linear_prediction/index.html#overview",
    "href": "content/blog/posts/stats/posts/linear_prediction/index.html#overview",
    "title": "How does geom_smooth() make predictions",
    "section": "",
    "text": "Linear regression is a statistical technique used to represent the linear relationship between a response \\((y)\\) and a predictor \\((x)\\).\n\\[\ny= \\beta_0 + \\beta_1x\n\\]\n\n\n\n\n\n\nBelow we examine some mismatch in output when plotting the prediction results of a linear model made with the lm() and the predict() functions versus the output produced by the geom_smooth() layer in a ggplot() visualization.\nThe first dataset used is from the HistData package HistData::CholeraDeaths1849. We select just the deaths due to Cholera within 12 months in 1849, and visualize the trend in time of the number of deaths with the addition of a further layer made with the geom_smooth().\n?geom_smooth()\n\n?HistData::CholeraDeaths1849\n\nlibrary(tidyverse)\nlibrary(HistData)\ncholera &lt;- HistData::CholeraDeaths1849 %&gt;%\n  filter(cause_of_death==\"Cholera\")%&gt;%\n  select(date,deaths)\n  \ncholera %&gt;% head\n\n# A tibble: 6 √ó 2\n  date       deaths\n  &lt;date&gt;      &lt;dbl&gt;\n1 1849-01-01     13\n2 1849-01-02     19\n3 1849-01-03     28\n4 1849-01-04     24\n5 1849-01-05     23\n6 1849-01-06     39\n\n\n\nsummary(cholera)\n\n      date                deaths    \n Min.   :1849-01-01   Min.   :   0  \n 1st Qu.:1849-04-02   1st Qu.:   8  \n Median :1849-07-02   Median :  23  \n Mean   :1849-07-02   Mean   : 146  \n 3rd Qu.:1849-10-01   3rd Qu.: 192  \n Max.   :1849-12-31   Max.   :1121  \n\n\n\nggplot(cholera,aes(x=date,y=deaths))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  labs(title=\"Deaths due to Cholera in London (1849)\",\n       x=\"Date\",y=\"Cholera death\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nDeaths due to Cholera in London (1849)\n\n\n\nThe purpose of making a model is that to identify the inside pattern of a series of observations. This means that the model would need to be able to interpolate given observations in order to represent the overall pattern. As in the visualization above the geom_smooth() with the specification of the method=\"lm\" helps us visualize what the direction of a linear pattern would be on this data. If it is a growing pattern or not.\nClearly the points are shaping a bell distribution of deaths in time, and this is not the case of a linear relationship between date and cholera deaths, but we would like to dig into the output of the prediction of the application of a linear model on this data and then compare it with the output of the geom_smooth(method=\"lm\") line.\nLet‚Äôs apply a linear model to this data and make some rough predictions.\n\nmod &lt;- lm(deaths ~ date, data = cholera)\nsummary(mod)\n\n\nCall:\nlm(formula = deaths ~ date, data = cholera)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-265.88 -104.28  -59.87   11.19  930.92 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.954e+04  4.878e+03   6.056 3.49e-09 ***\ndate        6.678e-01  1.108e-01   6.026 4.13e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 223.1 on 363 degrees of freedom\nMultiple R-squared:  0.09094,   Adjusted R-squared:  0.08843 \nF-statistic: 36.31 on 1 and 363 DF,  p-value: 4.132e-09\n\n\nThe application of a linear model on this data produced an estimation of the intercept \\((\\beta_0)\\) and the slope \\((\\beta_1)\\).\n\\[\n\\beta_0=29540 \\text{ and } \\beta_1=0.6678\n\\] The intercept is the starting point of a linear model line on the y axes, while the slope is the inclination of the line, that can be positive or negative, indicating the growing or decreasing tendency of the relationship between the response and the predictor.\nLet‚Äôs draw this line.\n\nggplot(cholera, aes(x=date, y=deaths)) + \n    geom_point() + \n    geom_abline(slope=0.6678, intercept=29540,\n                col=\"pink\")+\n  theme_minimal()\n\n\n\n\n\n\n\nNow calculate the prediction and the Root Mean Squared Error (RMSE) to evaluate how the model worked.\n\npredictions &lt;- predict(mod, newdata = NULL)\n\nrmse &lt;- sqrt(mean((predictions - cholera$deaths)^2))\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 222.4774 \n\n\n\n\n\n\n\n\nWhat if we want to visualize the output of our prediction?\n\n\n\n\nggplot(cholera, aes(x=date)) + \n  geom_point(aes(y=deaths)) + \n  geom_smooth(aes(y=deaths),method = \"lm\",linewidth=2)+\n  geom_line(y=predictions, col=\"red\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow let‚Äôs use a different dataset. This data set comes from a paper by Brigham et al.¬†(2003) that analyses some tables from Farr‚Äôs report of the Registrar-General on mortality due to cholera in England in the years 1848-1849, during which there was a large epidemic throughout the country. In this case we do not have the time variable but the numbers of deaths are considered by 38 districts in London.\n?HistData::Cholera\n\ncholera2 &lt;- HistData::Cholera %&gt;%\n  rownames_to_column(var=\"id\")%&gt;%\n  select(id,district,cholera_deaths,popn)\n  \ncholera2 %&gt;% head\n\n  id            district cholera_deaths  popn\n1  1           Newington            907 63074\n2  2         Rotherhithe            352 17208\n3  3          Bermondsey            836 50900\n4  4 St George Southwark            734 45500\n5  5            St Olave            349 19278\n6  6          St Saviour            539 35227\n\n\nThe predictor in this case is a character, we are considering the relationship between the deaths rate and the districts, so we are looking at to see whether the deaths rate is different among 38 districts. The order in this case is arbitrary and this influences the pattern. The geom_smooth() is not drowning a line, it doesn‚Äôt know how the x-axis has to be ordered, because there is not a specified order to follow.\n\nggplot(cholera2, aes(x=id,y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nwhile if we set an order with as.integer(id), the line can be drawn but it hasn‚Äôt got much meaning. The trend is not going up or down because we are considering districts in the x-axis.\n\nggplot(cholera2, aes(x=as.integer(id),y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIf we consider the population, in the middle of 1849, a numeric vector, these values are by districts, each popn value corresponds to the level of population in one of the 38 districts. Let‚Äôs see what happens if we plot popn versus cholera_deaths.\n\nggplot(cholera2, aes(x=popn,y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow the values on the x-axis are numeric and have a meaning to be ordered from the lower to the highest but it is not a trend. Each point is one district population value with some deaths due to cholera. The geom_smooth line it is telling us that if the level of the population is higher, the level of deaths due to cholera is higher, than in other location with a lower level of population.\nBut we can evaluate the growing relationship between population level and numbers of deaths due to cholera.\nLet‚Äôs make a linear model and predict the future, roughly.\n\nmod2 &lt;- lm(cholera_deaths ~ popn , data = cholera2)\nmod2\n\n\nCall:\nlm(formula = cholera_deaths ~ popn, data = cholera2)\n\nCoefficients:\n(Intercept)         popn  \n  1.073e+02    4.357e-03  \n\n\nLet‚Äôs draw this line.\n\nggplot(cholera2, aes(x=popn, y=cholera_deaths)) + \n    geom_point() + \n    geom_abline(slope=0.004357, intercept=107.3,\n                col=\"pink\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\npredictions2 &lt;- predict(mod2, newdata = NULL)\n\nrmse &lt;- sqrt(mean((predictions2 - cholera2$cholera_deaths)^2))\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 281.7808 \n\n\n\n\n\n\n\n\nWhat if we want to visualize the output of our prediction?\n\n\n\n\nplot &lt;- ggplot(cholera2, aes(x=popn)) + \n  geom_point(aes(y=cholera_deaths)) + \n  geom_smooth(aes(y=cholera_deaths),method = \"lm\",linewidth=2)+\n  geom_line(y=predictions2, col=\"red\")+\n  labs(title=\"Cholera Deaths explanined by\\nLondon Districts Population (1849)\",\n       x=\"1849 London Population by 38 Districts\",\n       y=\"Cholera Deaths\",\n       caption=\"William Farr's Data on Cholera in London, 1849\")+\n  theme_minimal()\nplot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nCholera Deaths explanined by London Districts Population (1849)\n\n\n\n\nThink about that!\n\n\n\n\n\nImagine that we absolutely want to replicate the geom_smooth(method=\"lm\") line, we would need to consider some steps that the function takes in order to plot the prediction line that doesn‚Äôt much with ours this time. First think to consider is that we haven‚Äôt used new data but just produced the prediction from our dataset. But this is exactly as the same as before.\nThere is a function ggplot_build() that let‚Äôs us dig into the ggplot data manipulation used to make the geom_smooth line.\n?ggplot_build()\n\n# source: https://stackoverflow.com/questions/42673665/geom-smooth-gives-different-fit-than-nls-alone\ndat = ggplot_build(plot)$data[[2]]\n\n`geom_smooth()` using formula = 'y ~ x'\n\ndat &lt;- dat%&gt;%rename(cholera_drate=y,popn=x)\n\nThis time we use the newdata = dat insted of NULL.\n\npredictions3 &lt;- predict(mod2, newdata = dat)\n\nrmse &lt;- sqrt(mean((predictions3 - cholera2$cholera_deaths)^2))\n\nWarning in predictions3 - cholera2$cholera_deaths: longer object length is not\na multiple of shorter object length\n\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 411.1835 \n\n\n\nggplot(cholera2, aes(x=popn)) + \n  geom_point(aes(y=cholera_deaths)) + \n  geom_smooth(aes(y=cholera_deaths),method = \"lm\",linewidth=2)+\n  geom_line(data=dat,y=predictions3, col=\"red\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWe eventually matched the geom_smooth line, but why did our predictions on the original data result in that squiggly line?"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "HIV infections data is from the aidsinfo.unaids.org. The data contains Global values from 2010 to 2022 for HIV estimate, HIV incidence of mortality, HIV prevalence, and HIV deaths.\n\n\nHIV infections\n\nNational data for the 2010 and 2020 are included in this dataset.\nThe following is estimating magnitude of change along 10 years time-frame from 2010 to 2020 of HIV infections for all countries with available data.\nLoad necessary libraries.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\ntidymodels_prefer()\nlibrary(spatialsample)\nlibrary(sf)\nlibrary(tmap)\ndata(\"World\")\n\n\nThis first dataset contains the average values, obtained by averaging the lower and upper bounds of 2010 and 2020 HIV infections for 173 countries.\n\naids_avg &lt;- read.csv(\"data/aids_avg.csv\")\naids_avg &lt;- aids_avg%&gt;%\n  filter(!country==\"Global\")%&gt;%\n  filter(!country==\"India\")%&gt;%\n  rename(aids_cc = aids_change2)\n\naids_avg%&gt;%dim\n\nThe percent change relative to the sum of changes for all countries is given by:\n\\[\\text{Percent Change (Relative to Sum)}=(\\frac{\\text{(Final Value‚àíInitial Value)}}{‚àë(Final Value‚àíInitialValue)})√ó100\\] This will give you the percent change for each country relative to the sum of all changes. As well as the percentage contribution of each country's change to the total change can be obtained.\n\n\nprevalence_rt &lt;- read.csv(\"data/Epidemic transition metrics_Incidence_prevalence ratio.csv\")\n\naids_prevalence_rt&lt;-prevalence_rt%&gt;%\n  filter(!Country==\"Global\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  mutate(avg_2010=(as.numeric(X2010_upper)-as.numeric(X2010_lower))/2,\n         avg_2020=(as.numeric(X2020_upper)-as.numeric(X2020_lower))/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_prev_cc=round(aids_change/sum(aids_change,na.rm = T),5))\n\n\n\ninc_mort_ratio &lt;- read_csv(\"data/Epidemic transition metrics_Incidence_mortality ratio.csv\")\n\naids_inc_mort_ratio &lt;- inc_mort_ratio%&gt;%\n  janitor::clean_names()%&gt;%\n    filter(!country==\"Global\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  mutate(avg_2010=(as.numeric(x2010_upper)-as.numeric(x2010_lower))/2,\n         avg_2020=(as.numeric(x2020_upper)-as.numeric(x2020_lower))/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_imr_cc=round(aids_change/sum(aids_change,na.rm = T),5))\n\n\n\ndeaths &lt;- read_csv(\"data/Epidemic transition metrics_Trend of AIDS-related deaths.csv\")\n\naids_deaths&lt;- deaths%&gt;%\n  filter(!Country==\"Global\")%&gt;%\n  filter(!Country==\"India\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  janitor::clean_names()%&gt;%\n  mutate(x2010_upper=as.numeric(str_extract(x2010_upper,\"([0-9]+)\")),\n         x2010_lower=as.numeric(str_extract(x2010_lower,\"([0-9]+)\")),\n         x2020_upper=as.numeric(str_extract(x2020_upper,\"([0-9]+)\")),\n         x2020_lower=as.numeric(str_extract(x2010_lower,\"([0-9]+)\")),\n         avg_2010=(x2010_upper-x2010_lower)/2,\n         avg_2020=(x2020_upper-x2020_lower)/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_d_cc=round(aids_change/sum(aids_change,na.rm = T),5))\n\n\n\ndat&lt;- aids_avg%&gt;%\n  select(country,aids_cc)%&gt;%\n  left_join(aids_prevalence_rt%&gt;%select(Country,aids_prev_cc),\n            by=c(\"country\"=\"Country\"))%&gt;%\n  left_join(aids_inc_mort_ratio%&gt;%select(country,aids_imr_cc),\n            by=c(\"country\"))%&gt;%\n  left_join(aids_deaths%&gt;%select(country,aids_d_cc),\n            by=c(\"country\"))\ndat%&gt;%head\n\n\ndat%&gt;%\n  dim()\n\n\n\ndat%&gt;%\n  pivot_longer(cols = contains(\"cc\"))%&gt;%\n  mutate(value=scale(value),\n         country=as.factor(country))%&gt;%\n  drop_na()%&gt;%\n  ggplot(aes(x=fct_reorder(country,value),y=value,\n             group=name,fill=name),color=\"grey24\")+\n  geom_col(position = \"stack\")+\n  scale_y_log10(expand=c(0,0),label=scales::comma_format())+\n  labs(title=\"HIV Distributions (2010-2020)\",\n       x=\"Country\",\n       fill=\"\",\n       caption = \"Graphic: @fgazzelloni\")+\n  theme(text=element_text(size=14),\n        axis.text.x = element_text(angle = 90,size=4,hjust=1),\n        panel.grid = element_blank(),\n        panel.background = element_rect(color = \"grey24\",fill=\"grey24\"))\n\n\n\nWorld &lt;- World %&gt;%\n  select(country=name,geometry)%&gt;%\n  filter(!country==\"Antarctica\")\n\naids_map &lt;- dat%&gt;%\n    pivot_longer(cols = contains(\"cc\"))%&gt;%\n  mutate(value=scale(value),\n         country=as.factor(country))%&gt;%\n  drop_na()%&gt;%\n  left_join(World,by=\"country\")%&gt;%\n  st_as_sf()%&gt;%\n  st_transform(crs=\"ESRI:54030\")\n\n\nlabels=c(\"aids_cc\"=\"HIV Country Contribution\",\n         \"aids_d_cc\"=\"HIV Deaths Country Contribution\",\n         \"aids_imr_cc\"=\"HIV incidence mortality rate Country Contribution\",\n         \"aids_prev_cc\"=\"HIV Prevalence Country Contribution\")\n\n\n\nggplot()+\n  geom_sf(data=World,color=\"grey25\",fill=\"grey75\")+\n  geom_sf(data=aids_map,\n          mapping=aes(geometry=geometry,fill=value),\n          color=\"red\")+\n  coord_sf(crs=\"ESRI:54030\",clip = \"off\")+\n  facet_wrap(~name,labeller = labeller(name=labels))+\n  scale_fill_viridis_c()+\n  labs(caption=\"Map: @fgazzelloni\")\n\n\n\ndata &lt;- dat%&gt;%\n  drop_na()%&gt;%\n  inner_join(World,by=c(\"country\"))%&gt;%\n  sf::st_as_sf(crs = 4326)\n\n\nset.seed(11132023)\nsplit &lt;- initial_split(data,prop = 0.8)\ntrain&lt;- training(split)\ntest &lt;- testing(split)\n\n\n\nfolds &lt;- spatial_clustering_cv(train, v = 5)\n\n\n\nautoplot(folds)+\n  labs(title=\"HIV Spatial Clustering Cross Validation\",\n       caption=\"DataSource: aidsinfo.unaids.org | Map: @fgazzelloni\")+\n  ggthemes::theme_map(base_size = 14)+\n  theme(plot.title = element_text(hjust=0.5),\n        plot.caption = element_text(hjust = 0.5))\n\n\nsource: https://spatialsample.tidymodels.org/articles/spatialsample.html\n\n# `splits` will be the `rsplit` object\ncompute_preds &lt;- function(splits) {\n  # fit the model to the analysis set\n  mod &lt;- lm(aids_cc ~ aids_prev_cc+aids_imr_cc+aids_d_cc,\n    data = analysis(split)\n  )\n  # identify the assessment set\n  holdout &lt;- assessment(split)\n  # return the assessment set, with true and predicted price\n  tibble::tibble(\n    geometry = holdout$geometry,\n    aids_cc = log10(holdout$aids_cc),\n    .pred = predict(mod, holdout)\n  )\n}\n\n\n\ncluster_folds &lt;- spatial_clustering_cv(data, v = 15)\nblock_folds &lt;- spatial_block_cv(data, v = 15)\n\n\ncluster_folds$type &lt;- \"cluster\"\nblock_folds$type &lt;- \"block\"\n\n\nresamples &lt;-\n  dplyr::bind_rows(\n    cluster_folds,\n    block_folds\n  )\n\n\ncv_res &lt;- resamples %&gt;%\n  mutate(.preds = map(splits, compute_preds))\n\n\ncv_rmse &lt;- cv_res %&gt;%\n  unnest(.preds) %&gt;%\n  drop_na()%&gt;%\n  filter(!aids_cc==-Inf)%&gt;%\n  group_by(id, type) %&gt;%\n  rmse(aids_cc, .pred)\n\n\ncv_res %&gt;%\n  unnest(.preds) %&gt;%\n  ggplot(aes(fill = .pred)) +\n  geom_sf(data=World,mapping=aes(geometry = geometry),inherit.aes = F)+\n  geom_sf(aes(geometry = geometry)) +\n  labs() +\n  scale_fill_viridis_c() +\n  facet_wrap(~type)\n\n\ncv_res %&gt;%\n  unnest(.preds) %&gt;%\n  ggplot(aes(fill = aids_cc)) +\n  geom_sf(data=World,mapping=aes(geometry = geometry),inherit.aes = F)+\n  geom_sf(aes(geometry = geometry)) +\n  labs() +\n  scale_fill_viridis_c() \n\n\n\nSpatialSample\nr-spatial.org\nhttps://www.tmwr.org/"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#hiv-average-values-2010-2020",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#hiv-average-values-2010-2020",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "This first dataset contains the average values, obtained by averaging the lower and upper bounds of 2010 and 2020 HIV infections for 173 countries.\n\naids_avg &lt;- read.csv(\"data/aids_avg.csv\")\naids_avg &lt;- aids_avg%&gt;%\n  filter(!country==\"Global\")%&gt;%\n  filter(!country==\"India\")%&gt;%\n  rename(aids_cc = aids_change2)\n\naids_avg%&gt;%dim\n\nThe percent change relative to the sum of changes for all countries is given by:\n\\[\\text{Percent Change (Relative to Sum)}=(\\frac{\\text{(Final Value‚àíInitial Value)}}{‚àë(Final Value‚àíInitialValue)})√ó100\\] This will give you the percent change for each country relative to the sum of all changes. As well as the percentage contribution of each country's change to the total change can be obtained."
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#hiv-prevalence-ratio-2010-2020",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#hiv-prevalence-ratio-2010-2020",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "prevalence_rt &lt;- read.csv(\"data/Epidemic transition metrics_Incidence_prevalence ratio.csv\")\n\naids_prevalence_rt&lt;-prevalence_rt%&gt;%\n  filter(!Country==\"Global\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  mutate(avg_2010=(as.numeric(X2010_upper)-as.numeric(X2010_lower))/2,\n         avg_2020=(as.numeric(X2020_upper)-as.numeric(X2020_lower))/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_prev_cc=round(aids_change/sum(aids_change,na.rm = T),5))"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#hiv-incidence-mortality-ratio-2010-2020",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#hiv-incidence-mortality-ratio-2010-2020",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "inc_mort_ratio &lt;- read_csv(\"data/Epidemic transition metrics_Incidence_mortality ratio.csv\")\n\naids_inc_mort_ratio &lt;- inc_mort_ratio%&gt;%\n  janitor::clean_names()%&gt;%\n    filter(!country==\"Global\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  mutate(avg_2010=(as.numeric(x2010_upper)-as.numeric(x2010_lower))/2,\n         avg_2020=(as.numeric(x2020_upper)-as.numeric(x2020_lower))/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_imr_cc=round(aids_change/sum(aids_change,na.rm = T),5))"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#hiv-deaths-2010-2020",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#hiv-deaths-2010-2020",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "deaths &lt;- read_csv(\"data/Epidemic transition metrics_Trend of AIDS-related deaths.csv\")\n\naids_deaths&lt;- deaths%&gt;%\n  filter(!Country==\"Global\")%&gt;%\n  filter(!Country==\"India\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  janitor::clean_names()%&gt;%\n  mutate(x2010_upper=as.numeric(str_extract(x2010_upper,\"([0-9]+)\")),\n         x2010_lower=as.numeric(str_extract(x2010_lower,\"([0-9]+)\")),\n         x2020_upper=as.numeric(str_extract(x2020_upper,\"([0-9]+)\")),\n         x2020_lower=as.numeric(str_extract(x2010_lower,\"([0-9]+)\")),\n         avg_2010=(x2010_upper-x2010_lower)/2,\n         avg_2020=(x2020_upper-x2020_lower)/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_d_cc=round(aids_change/sum(aids_change,na.rm = T),5))"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#all-data",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#all-data",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "dat&lt;- aids_avg%&gt;%\n  select(country,aids_cc)%&gt;%\n  left_join(aids_prevalence_rt%&gt;%select(Country,aids_prev_cc),\n            by=c(\"country\"=\"Country\"))%&gt;%\n  left_join(aids_inc_mort_ratio%&gt;%select(country,aids_imr_cc),\n            by=c(\"country\"))%&gt;%\n  left_join(aids_deaths%&gt;%select(country,aids_d_cc),\n            by=c(\"country\"))\ndat%&gt;%head\n\n\ndat%&gt;%\n  dim()\n\n\n\ndat%&gt;%\n  pivot_longer(cols = contains(\"cc\"))%&gt;%\n  mutate(value=scale(value),\n         country=as.factor(country))%&gt;%\n  drop_na()%&gt;%\n  ggplot(aes(x=fct_reorder(country,value),y=value,\n             group=name,fill=name),color=\"grey24\")+\n  geom_col(position = \"stack\")+\n  scale_y_log10(expand=c(0,0),label=scales::comma_format())+\n  labs(title=\"HIV Distributions (2010-2020)\",\n       x=\"Country\",\n       fill=\"\",\n       caption = \"Graphic: @fgazzelloni\")+\n  theme(text=element_text(size=14),\n        axis.text.x = element_text(angle = 90,size=4,hjust=1),\n        panel.grid = element_blank(),\n        panel.background = element_rect(color = \"grey24\",fill=\"grey24\"))"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#world-polygons",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#world-polygons",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "World &lt;- World %&gt;%\n  select(country=name,geometry)%&gt;%\n  filter(!country==\"Antarctica\")\n\naids_map &lt;- dat%&gt;%\n    pivot_longer(cols = contains(\"cc\"))%&gt;%\n  mutate(value=scale(value),\n         country=as.factor(country))%&gt;%\n  drop_na()%&gt;%\n  left_join(World,by=\"country\")%&gt;%\n  st_as_sf()%&gt;%\n  st_transform(crs=\"ESRI:54030\")\n\n\nlabels=c(\"aids_cc\"=\"HIV Country Contribution\",\n         \"aids_d_cc\"=\"HIV Deaths Country Contribution\",\n         \"aids_imr_cc\"=\"HIV incidence mortality rate Country Contribution\",\n         \"aids_prev_cc\"=\"HIV Prevalence Country Contribution\")\n\n\n\nggplot()+\n  geom_sf(data=World,color=\"grey25\",fill=\"grey75\")+\n  geom_sf(data=aids_map,\n          mapping=aes(geometry=geometry,fill=value),\n          color=\"red\")+\n  coord_sf(crs=\"ESRI:54030\",clip = \"off\")+\n  facet_wrap(~name,labeller = labeller(name=labels))+\n  scale_fill_viridis_c()+\n  labs(caption=\"Map: @fgazzelloni\")"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#spending-data",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#spending-data",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "data &lt;- dat%&gt;%\n  drop_na()%&gt;%\n  inner_join(World,by=c(\"country\"))%&gt;%\n  sf::st_as_sf(crs = 4326)\n\n\nset.seed(11132023)\nsplit &lt;- initial_split(data,prop = 0.8)\ntrain&lt;- training(split)\ntest &lt;- testing(split)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#spatial-cross-validation",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#spatial-cross-validation",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "folds &lt;- spatial_clustering_cv(train, v = 5)\n\n\n\nautoplot(folds)+\n  labs(title=\"HIV Spatial Clustering Cross Validation\",\n       caption=\"DataSource: aidsinfo.unaids.org | Map: @fgazzelloni\")+\n  ggthemes::theme_map(base_size = 14)+\n  theme(plot.title = element_text(hjust=0.5),\n        plot.caption = element_text(hjust = 0.5))"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#function-for-calculating-predictions",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#function-for-calculating-predictions",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "source: https://spatialsample.tidymodels.org/articles/spatialsample.html\n\n# `splits` will be the `rsplit` object\ncompute_preds &lt;- function(splits) {\n  # fit the model to the analysis set\n  mod &lt;- lm(aids_cc ~ aids_prev_cc+aids_imr_cc+aids_d_cc,\n    data = analysis(split)\n  )\n  # identify the assessment set\n  holdout &lt;- assessment(split)\n  # return the assessment set, with true and predicted price\n  tibble::tibble(\n    geometry = holdout$geometry,\n    aids_cc = log10(holdout$aids_cc),\n    .pred = predict(mod, holdout)\n  )\n}"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#spatial-clustering-and-spatial-block-cross-validations",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#spatial-clustering-and-spatial-block-cross-validations",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "cluster_folds &lt;- spatial_clustering_cv(data, v = 15)\nblock_folds &lt;- spatial_block_cv(data, v = 15)\n\n\ncluster_folds$type &lt;- \"cluster\"\nblock_folds$type &lt;- \"block\"\n\n\nresamples &lt;-\n  dplyr::bind_rows(\n    cluster_folds,\n    block_folds\n  )\n\n\ncv_res &lt;- resamples %&gt;%\n  mutate(.preds = map(splits, compute_preds))\n\n\ncv_rmse &lt;- cv_res %&gt;%\n  unnest(.preds) %&gt;%\n  drop_na()%&gt;%\n  filter(!aids_cc==-Inf)%&gt;%\n  group_by(id, type) %&gt;%\n  rmse(aids_cc, .pred)\n\n\ncv_res %&gt;%\n  unnest(.preds) %&gt;%\n  ggplot(aes(fill = .pred)) +\n  geom_sf(data=World,mapping=aes(geometry = geometry),inherit.aes = F)+\n  geom_sf(aes(geometry = geometry)) +\n  labs() +\n  scale_fill_viridis_c() +\n  facet_wrap(~type)\n\n\ncv_res %&gt;%\n  unnest(.preds) %&gt;%\n  ggplot(aes(fill = aids_cc)) +\n  geom_sf(data=World,mapping=aes(geometry = geometry),inherit.aes = F)+\n  geom_sf(aes(geometry = geometry)) +\n  labs() +\n  scale_fill_viridis_c()"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#references",
    "href": "content/blog/posts/dataviz/posts/usecases/hiv/index.html#references",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "SpatialSample\nr-spatial.org\nhttps://www.tmwr.org/"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/dogbreeds/index.html",
    "href": "content/blog/posts/dataviz/posts/dogbreeds/index.html",
    "title": "Dog breeds",
    "section": "",
    "text": "This #TidyTuesday post is all about drawing with R. I have replicated the dogs‚Äô prints (or paws) using R. For solving this kind of tasks you need to be very ‚Äúpractice‚Äù with functions outcomes. And this means to have in mind the shape the function will take. For example we know that \\(y = x^2\\) would shape a parabolic function.\nSo that, I decided to get some extra inspiration and used the ggforce::geom_ellipse() function to produce ellipses of different sizes.\n\nlibrary(tidyverse)\n\n\n\nDog breeds\n\nThe data set for this week #TidyTuesday contains Dog breeds information, in three datasets.To load the data:\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_traits.csv')\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/trait_description.csv')\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_rank.csv')\n\n\nA bit of data wrangling to make a rank dataset and see which are the 2020 favourite breeds.\n\nrank&lt;-breed_rank_all%&gt;%\n  count(Breed,`2020 Rank`,Image)%&gt;%\n  arrange(`2020 Rank`)%&gt;%\n  slice(1:10)%&gt;%\n  relocate(`2020 Rank`)%&gt;%\n  select(-n)\n\nrank\n\nSelect favourite breeds, and set the images\n\ndogs&lt;-c(\"Bulldogs\",\"Poodles\",\"Beagles\",\"Rottweilers\",\"Dachshunds\")\n\nimages &lt;-rank%&gt;%filter(Breed%in%dogs)%&gt;%select(Breed,Image)\n\nmy_df &lt;-breed_traits%&gt;%\n  select(-\"Coat Type\",-\"Coat Length\")%&gt;%\n  filter(Breed%in%dogs)%&gt;%\n  mutate(id=row_number())%&gt;%\n  relocate(id)%&gt;%\n  pivot_longer(cols=3:16,names_to=\"factor\",values_to=\"value\")%&gt;%\n  inner_join(images,by=\"Breed\")\n\n\n\nlibrary(extrafont)\nlibrary(showtext)\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nlibrary(sysfonts)\n#font_families_google()\nfont_add_google(name=\"Barlow Condensed\",family=\"dogs\")\n\nfamily = \"dogs\"\n\n\n\nggforce::geom_ellipse to make the paws\nggimage::geom_image to add the images\n\n\ndog_prints_plot &lt;-\n  \n  ggplot(my_df,aes(id,value,group=Breed))+\n  \n  # this is the largest part of the paw\n  ggforce::geom_ellipse(aes(x0=id,y0=value-0.2,a=0.2,b=0.25,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  # the following four ellipses are small extrems of the paw\n  ggforce::geom_ellipse(aes(x0=id+0.22,y0=value+0.2,a=0.1,b=0.12, \n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id-0.2,y0=value+0.15,a=0.1,b=0.12,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id+0.01,y0=value+0.25,a=0.1,b=0.1,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id-0.32,y0=value-0.15,a=0.1,b=0.1,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  # this is the grey part of the paw  \n  ggforce::geom_ellipse(aes(x0=id,y0=value-0.3,a=0.08,b=0.08,\n                            angle=50),color=\"grey65\",size=0.02,fill=\"grey65\",alpha=0.1) +\n  # The segment is to make the arrow up\n  geom_segment(x=0.43,xend=0.43,y=0,yend=6, size=3,color=\"grey65\",\n               arrow = arrow(length = unit(c(0, 0, 0.4, 0.4), 'cm')))+\n  \n  # description of the factors\n  geom_text(aes(label=factor),check_overlap = T, vjust=5.5, hjust=0.5, size=2.5, color=\"midnightblue\",family=family)+\n  \n  # The breed's names\n  geom_text(aes(x=id,y=6.5,label=Breed),color=\"black\",size=5.5,family=family,face=\"bold\")+\n  \n  # Images of the breeds\n  ggimage::geom_image(aes(x=id,y=-1.4,image=Image),size=0.2)+\n  \n  # expand the canvas to make the paws standing out\n  scale_y_continuous(expand = c(0,1))+\n  ylim(-1.5,7)+\n  coord_cartesian()+\n  labs(title=\"Top 5 selected Breeds: Was Goofy a dog?\", \n       caption=\"Datasource: American Kennel Club | #TidyTuesday 2022-w5 | Viz: Federica Gazzelloni\")+\n  ggthemes::theme_fivethirtyeight()+\n  theme(text = element_text(family = family,size=14),\n        plot.title = element_text(size=18,hjust=0.1),\n        plot.caption = element_text(face = \"bold\",vjust=0.5),\n        panel.grid.major.x = element_line(size=38),\n        panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_line(size=4),\n        plot.background = element_rect(color=\"white\",fill=\"white\"),\n        panel.background = element_rect(color=\"white\",fill=\"white\"),\n        legend.position = \"none\")\n\n\n\nlibrary(cowplot)\n\nfinal &lt;- ggdraw()+\n  draw_plot(dog_prints_plot)+\n  draw_image(\"print.png\",x=0.3,y=0.43,scale=0.16)+\n  draw_image(\"print.png\",x=0.4,y=0.43,scale=0.16)\n\nggsave(\"dog_prints_plot.png\",final)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/dogbreeds/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/dogbreeds/index.html#overview",
    "title": "Dog breeds",
    "section": "",
    "text": "This #TidyTuesday post is all about drawing with R. I have replicated the dogs‚Äô prints (or paws) using R. For solving this kind of tasks you need to be very ‚Äúpractice‚Äù with functions outcomes. And this means to have in mind the shape the function will take. For example we know that \\(y = x^2\\) would shape a parabolic function.\nSo that, I decided to get some extra inspiration and used the ggforce::geom_ellipse() function to produce ellipses of different sizes.\n\nlibrary(tidyverse)\n\n\n\nDog breeds\n\nThe data set for this week #TidyTuesday contains Dog breeds information, in three datasets.To load the data:\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_traits.csv')\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/trait_description.csv')\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_rank.csv')\n\n\nA bit of data wrangling to make a rank dataset and see which are the 2020 favourite breeds.\n\nrank&lt;-breed_rank_all%&gt;%\n  count(Breed,`2020 Rank`,Image)%&gt;%\n  arrange(`2020 Rank`)%&gt;%\n  slice(1:10)%&gt;%\n  relocate(`2020 Rank`)%&gt;%\n  select(-n)\n\nrank\n\nSelect favourite breeds, and set the images\n\ndogs&lt;-c(\"Bulldogs\",\"Poodles\",\"Beagles\",\"Rottweilers\",\"Dachshunds\")\n\nimages &lt;-rank%&gt;%filter(Breed%in%dogs)%&gt;%select(Breed,Image)\n\nmy_df &lt;-breed_traits%&gt;%\n  select(-\"Coat Type\",-\"Coat Length\")%&gt;%\n  filter(Breed%in%dogs)%&gt;%\n  mutate(id=row_number())%&gt;%\n  relocate(id)%&gt;%\n  pivot_longer(cols=3:16,names_to=\"factor\",values_to=\"value\")%&gt;%\n  inner_join(images,by=\"Breed\")\n\n\n\nlibrary(extrafont)\nlibrary(showtext)\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nlibrary(sysfonts)\n#font_families_google()\nfont_add_google(name=\"Barlow Condensed\",family=\"dogs\")\n\nfamily = \"dogs\"\n\n\n\nggforce::geom_ellipse to make the paws\nggimage::geom_image to add the images\n\n\ndog_prints_plot &lt;-\n  \n  ggplot(my_df,aes(id,value,group=Breed))+\n  \n  # this is the largest part of the paw\n  ggforce::geom_ellipse(aes(x0=id,y0=value-0.2,a=0.2,b=0.25,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  # the following four ellipses are small extrems of the paw\n  ggforce::geom_ellipse(aes(x0=id+0.22,y0=value+0.2,a=0.1,b=0.12, \n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id-0.2,y0=value+0.15,a=0.1,b=0.12,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id+0.01,y0=value+0.25,a=0.1,b=0.1,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id-0.32,y0=value-0.15,a=0.1,b=0.1,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  # this is the grey part of the paw  \n  ggforce::geom_ellipse(aes(x0=id,y0=value-0.3,a=0.08,b=0.08,\n                            angle=50),color=\"grey65\",size=0.02,fill=\"grey65\",alpha=0.1) +\n  # The segment is to make the arrow up\n  geom_segment(x=0.43,xend=0.43,y=0,yend=6, size=3,color=\"grey65\",\n               arrow = arrow(length = unit(c(0, 0, 0.4, 0.4), 'cm')))+\n  \n  # description of the factors\n  geom_text(aes(label=factor),check_overlap = T, vjust=5.5, hjust=0.5, size=2.5, color=\"midnightblue\",family=family)+\n  \n  # The breed's names\n  geom_text(aes(x=id,y=6.5,label=Breed),color=\"black\",size=5.5,family=family,face=\"bold\")+\n  \n  # Images of the breeds\n  ggimage::geom_image(aes(x=id,y=-1.4,image=Image),size=0.2)+\n  \n  # expand the canvas to make the paws standing out\n  scale_y_continuous(expand = c(0,1))+\n  ylim(-1.5,7)+\n  coord_cartesian()+\n  labs(title=\"Top 5 selected Breeds: Was Goofy a dog?\", \n       caption=\"Datasource: American Kennel Club | #TidyTuesday 2022-w5 | Viz: Federica Gazzelloni\")+\n  ggthemes::theme_fivethirtyeight()+\n  theme(text = element_text(family = family,size=14),\n        plot.title = element_text(size=18,hjust=0.1),\n        plot.caption = element_text(face = \"bold\",vjust=0.5),\n        panel.grid.major.x = element_line(size=38),\n        panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_line(size=4),\n        plot.background = element_rect(color=\"white\",fill=\"white\"),\n        panel.background = element_rect(color=\"white\",fill=\"white\"),\n        legend.position = \"none\")\n\n\n\nlibrary(cowplot)\n\nfinal &lt;- ggdraw()+\n  draw_plot(dog_prints_plot)+\n  draw_image(\"print.png\",x=0.3,y=0.43,scale=0.16)+\n  draw_image(\"print.png\",x=0.4,y=0.43,scale=0.16)\n\nggsave(\"dog_prints_plot.png\",final)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/dogbreeds/index.html#resources",
    "href": "content/blog/posts/dataviz/posts/dogbreeds/index.html#resources",
    "title": "Dog breeds",
    "section": "Resources:",
    "text": "Resources:\n\nplotting-math-functions-in-r"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/ggmap/index.html",
    "href": "content/blog/posts/dataviz/posts/ggmap/index.html",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "For this #30DayMapChallenge 2023 Day6 - Asia let‚Äôs explore the Population estimation for Regions and major Cities, from different sources.\nAlso, we will be looking at how to get started with ggmap to find the geocodes for the major cities in Asia.\n\n\nAsia Population\n\n\nLet‚Äôs scrap the table of the Major Cities in Asia along with the Population level from Wikipedia.org.\nLoad the first set of libraries:\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tidyverse)\n\n\nhtml.population &lt;- read_html('https://en.wikipedia.org/wiki/List_of_Asian_cities_by_population_within_city_limits')\n\ndf.asia_cities &lt;- html.population %&gt;%\n  html_nodes(\"table\") %&gt;%\n  .[[1]] %&gt;%\n  html_table(fill = TRUE)\n\ndf.asia_cities %&gt;% names()\n\n\ndf.asia_cities%&gt;%select(1,2,4)%&gt;%head\n\nSelect only the vectors of interest and clean data.\n\ndf.asia_cities &lt;- df.asia_cities[c(1,2,4)]\n\nasia_cities &lt;- df.asia_cities %&gt;%\n    mutate(Population = str_replace_all(Population, \"\\\\[.*\\\\]\",\"\") %&gt;% parse_number(),\n           City_full= str_c(df.asia_cities$City, df.asia_cities$Nation, sep = ', ')) %&gt;%\n    select(City, Nation, City_full, Population)%&gt;%\n    filter(!str_detect(Nation,\"Russia|Turkey\"),\n           !is.na(Population))\n\n\nasia_cities %&gt;% head()\n\n\nTo find the Asia City Geocodes we use the geocode() function from the {ggmap} package.\n\n\n\n\n\n\nIn order to get started with {ggmap} follow these steps:\n\n\nInstall the package from source:\nremotes::install_github(‚Äúdkahle/ggmap‚Äù)\n\nGet started with Google Maps Platform https://developers.google.com/maps all you need to do, if you do not have access to the platform yet, is to get started a free trial by adding your bank account information (if you do not want to continue after the trials ends you can stop it without charges)\nGo to on the left-side bar menu and select Overview then ENABLE APIs\nGo to APIs & Services to check enabled APIs\nGo to Keys and Credentials and click +CREATE CREDENTIALS on the top-side bar\n\nCopy the API key and paste it in the register_google() function, the option write = T will save the credentials for future use in your .Renviron file:\nggmap::register_google(, write = T)\n\n\n\n\n\nOnce you are all set try:\n\nlibrary(ggmap)\n\n?ggmap::geocode\n\ndata.geo &lt;- geocode(c(\"waco, texas\"))\n\ngeocode(\"waco texas\", output = \"latlona\")\n\n\ndata.geo%&gt;%head\n\n\nasia_cities_full &lt;- cbind(asia_cities, data.geo)\n# inspect\nasia_cities_full %&gt;% head() \n\n\nLet‚Äôs have a look at the map of Asia with {ggmap}.\n\nmap.asia &lt;- get_map('Asia', zoom = 3)\nmap.asia %&gt;% ggmap()\n\nFor this challenge we will be using another package for the polygons of Asia, the {rworldmap} package.\ninstall.packages(\"rworldmap\")\n\nlibrary(rworldmap)\n\n\nworldmap &lt;- rworldmap::getMap(resolution = \"high\")\ndim(worldmap)\n\nHave a look at the regions and choose Asia.\n\nt(t(table(worldmap$REGION)))\n\n\nasia &lt;- worldmap[which(worldmap$REGION==\"Asia\"),]\nasia%&gt;%class\n\nAs it is a spatial polygon dataframe, and we‚Äôd like to use the geom_sf() function from the ggplot2 package, we transform it to a simple feature object with st_as_sf() function from the sf package.\n\nlibrary(sf)\n\n\nasia_sf &lt;- asia %&gt;%\n  st_as_sf()\n\nasia_sf %&gt;% class()\n\n\nTo map the continent with population estimation by state we can set the option fill= POP_EST.\n\nasia_sf %&gt;%\n  ggplot()+\n  geom_sf(aes(fill=POP_EST))+\n  scale_fill_continuous()\n\n\nInteresting is looking at a different classification of the population classes, and we do this by using the classIntervals() function from the classInt package for classifying the Population Estimation by quantile.\nLet‚Äôs have a look at the population quantiles first. What we can see are the min and the max levels, and the values of the three quantiles, 25%, 50% (median), and the 75%. Which estimation of population follow in each quantile class.\nThe median population estimate for Asia is around 18 million, with some regions having populations of less than 1.5 billion people.\n\nquantile(asia_sf$POP_EST, na.rm=TRUE)\n\n\nasia_sf%&gt;%\n  ggplot(aes(POP_EST))+\n  geom_histogram(aes(fill=SOVEREIGNT),bins = 20)+\n  geom_vline(aes(xintercept = mean(POP_EST)),color=\"lightblue\")+\n  geom_vline(aes(xintercept = median(POP_EST)),color=\"midnightblue\")+\n  geom_text(aes(x=9000000,y=9,label=\"median\"),size=2)+\n  geom_text(aes(x=50000000,y=9,label=\"mean\"),size=2)+\n  scale_x_log10(labels=scales::comma_format(scale = 1/1000),n.breaks =8)+\n  scale_fill_viridis_d()+\n  labs(x=\"Population Estimation (Thousands)\",\n       title=\"Asia Population Distribution\",\n       caption=\"DataSource: {rworldmap} | Graphic: @fgazzelloni\")+\n  ggthemes::theme_clean()+\n  theme(legend.text = element_text(size=5),\n        legend.key.size = unit(5,units = \"pt\"))\n\nWe use the classInt package to find custom intervals of the population. And set up a new object called brks.\n\nlibrary(classInt)\n\n\nbrks &lt;- classIntervals(asia_sf$POP_EST,\n                       n=10, \n                       style=\"quantile\")\nbrks\n\nSet the color scheme:\n\nbrks &lt;- brks$brks\ncolors &lt;- RColorBrewer::brewer.pal(length(brks), \"Spectral\")\n\nFinalize the dataset to use for the map with the population estimation interval cuts.\n\nregion_pop &lt;- asia_sf%&gt;%\n  select(POP_EST)%&gt;%\n  mutate(breaks=case_when(POP_EST &gt; 0 & POP_EST &lt; 625493.5 ~ \"[0,625493.5)\",\n                          POP_EST &gt;= 625493.5 & POP_EST &lt; 2691158 ~ \"[625493.5,2691158)\",\n                          POP_EST &gt;= 2691158 & POP_EST &lt; 4728016 ~ \"[2691158,4728016)\",\n                          POP_EST &gt;= 4728016 & POP_EST &lt; 6834942 ~ \"[4728016,6834942)\",\n                          POP_EST &gt;= 6834942 & POP_EST &lt; 17788961 ~ \"[6834942,17788961)\",\n                          POP_EST &gt;= 17788961 & POP_EST &lt; 23822783 ~ \"[17788961,23822783)\",\n                          POP_EST &gt;= 23822783 & POP_EST &lt; 28625005 ~ \"[23822783,28625005)\",\n                          POP_EST &gt;= 28625005 & POP_EST &lt; 65905410 ~ \"[28625005,65905410)\",\n                          POP_EST &gt;= 65905410 & POP_EST &lt; 141564781 ~ \"[65905410,141564781)\",\n                          POP_EST &gt;= 141564781 & POP_EST &lt;= 1338612968 ~ \"[141564781,1338612968]\"))\n\nSet some information about Asia Population on a text box with the geom_textbox() function from the ggtext package.\n\ntext &lt;- tibble(asia_text=c(\"As of 2022, Asia's 4.6B population thrives in diverse urban centers. Mumbai's density soars at 20.7K/km¬≤, while Tokyo boasts 6.3K/km¬≤. Asia's remarkable density and cultural richness make it the world's most populous and dynamic continent.\"))\n\n\n\nregion_pop %&gt;%\n  ggplot()+\n  geom_sf(aes(fill=breaks))+\n  scale_fill_manual(breaks=c(\"[0,625493.5)\",\"[625493.5,2691158)\",\n                               \"[2691158,4728016)\",\"[4728016,6834942)\",\n                             \n                             \"[6834942,17788961)\",\"[17788961,23822783)\",\n                               \"[23822783,28625005)\",\"[28625005,65905410)\",\n                               \"[65905410,141564781)\",\"[141564781,1338612968]\"),\n                      values=rev(colors))+\n  geom_point(data=asia_cities_full,\n             mapping=aes(lon,lat,size=Population),\n             shape=21,stroke=0.5,\n             alpha=0.7,\n             color=\"grey90\",\n             inherit.aes = F)+\n  scale_size_continuous(labels=scales::comma_format())+\n  geom_text(data=asia_cities_full,\n             mapping=aes(lon,lat,label=City),fontface=\"bold\",\n            check_overlap = T,\n            size=2.1,color=\"white\")+\n  ggtext::geom_textbox(data=text,\n                       mapping=aes(x=60,y=-6,label=text),\n                       size=1.8,width = 0.4,fill=\"grey90\",\n                       family = \"Gill Sans\",\n                       inherit.aes = F)+\n  geom_curve(x=50,xend=67,y=0,yend=20,\n               linewidth=0.2,curvature = -0.5,\n               arrow = arrow(angle=30, \n                             length = unit(0.1, \"inches\"),\n                             ends = \"last\", type = \"open\"),\n      color=\"white\")+\n    geom_curve(x=86,xend=140,y=-5,yend=33,\n               linewidth=0.2,\n               arrow = arrow(angle=30, \n                             length = unit(0.1, \"inches\"),\n                             ends = \"last\", type = \"open\"),\n      color=\"white\")+\n  labs(fill=\"Regions Population\",\n       size=\"Cities Population\",\n       title=\"Asia - Population Level\",\n       caption=\"#30DayMapChallenge 2023 Day6 - ASIA\\nDataSource: Wikipedia & ggmap | Map @fgazzelloni\")+\n  ggthemes::theme_map()+\n  theme(text=element_text(color=\"white\", family = \"Gill Sans\"),\n        plot.title = element_text(face=\"bold\",size=14),\n        plot.caption = element_text(hjust = 0),\n        plot.background = element_rect(fill=\"#4A4A4A\",color=\"#4A4A4A\"),\n        panel.background = element_rect(fill=\"#4A4A4A\",color=\"#4A4A4A\"),\n        legend.background = element_blank(),\n        legend.key = element_rect(color=\"#4A4A4A\",fill=\"#4A4A4A\"),\n        legend.position = \"right\",\n        legend.text = element_text(size=5.5),\n        legend.key.size = unit(5.5,units = \"pt\"))\n\n\nggsave(\"day6_asia.png\",\n       width = 7,height = 4,\n       bg=\"#4A4A4A\")\n\n\n\nhttps://cran.r-project.org/web/packages/ggmap/readme/README.html\nhttps://www.r-bloggers.com/2017/09/mapping-the-largest-cities-in-asia-using-r/"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/ggmap/index.html#asia-cities-and-population-by-wikipedia.org",
    "href": "content/blog/posts/dataviz/posts/ggmap/index.html#asia-cities-and-population-by-wikipedia.org",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "Let‚Äôs scrap the table of the Major Cities in Asia along with the Population level from Wikipedia.org.\nLoad the first set of libraries:\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tidyverse)\n\n\nhtml.population &lt;- read_html('https://en.wikipedia.org/wiki/List_of_Asian_cities_by_population_within_city_limits')\n\ndf.asia_cities &lt;- html.population %&gt;%\n  html_nodes(\"table\") %&gt;%\n  .[[1]] %&gt;%\n  html_table(fill = TRUE)\n\ndf.asia_cities %&gt;% names()\n\n\ndf.asia_cities%&gt;%select(1,2,4)%&gt;%head\n\nSelect only the vectors of interest and clean data.\n\ndf.asia_cities &lt;- df.asia_cities[c(1,2,4)]\n\nasia_cities &lt;- df.asia_cities %&gt;%\n    mutate(Population = str_replace_all(Population, \"\\\\[.*\\\\]\",\"\") %&gt;% parse_number(),\n           City_full= str_c(df.asia_cities$City, df.asia_cities$Nation, sep = ', ')) %&gt;%\n    select(City, Nation, City_full, Population)%&gt;%\n    filter(!str_detect(Nation,\"Russia|Turkey\"),\n           !is.na(Population))\n\n\nasia_cities %&gt;% head()"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/ggmap/index.html#use-ggmap",
    "href": "content/blog/posts/dataviz/posts/ggmap/index.html#use-ggmap",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "To find the Asia City Geocodes we use the geocode() function from the {ggmap} package.\n\n\n\n\n\n\nIn order to get started with {ggmap} follow these steps:\n\n\nInstall the package from source:\nremotes::install_github(‚Äúdkahle/ggmap‚Äù)\n\nGet started with Google Maps Platform https://developers.google.com/maps all you need to do, if you do not have access to the platform yet, is to get started a free trial by adding your bank account information (if you do not want to continue after the trials ends you can stop it without charges)\nGo to on the left-side bar menu and select Overview then ENABLE APIs\nGo to APIs & Services to check enabled APIs\nGo to Keys and Credentials and click +CREATE CREDENTIALS on the top-side bar\n\nCopy the API key and paste it in the register_google() function, the option write = T will save the credentials for future use in your .Renviron file:\nggmap::register_google(, write = T)\n\n\n\n\n\nOnce you are all set try:\n\nlibrary(ggmap)\n\n?ggmap::geocode\n\ndata.geo &lt;- geocode(c(\"waco, texas\"))\n\ngeocode(\"waco texas\", output = \"latlona\")\n\n\ndata.geo%&gt;%head\n\n\nasia_cities_full &lt;- cbind(asia_cities, data.geo)\n# inspect\nasia_cities_full %&gt;% head()"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/ggmap/index.html#mapping-asia-polygons",
    "href": "content/blog/posts/dataviz/posts/ggmap/index.html#mapping-asia-polygons",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "Let‚Äôs have a look at the map of Asia with {ggmap}.\n\nmap.asia &lt;- get_map('Asia', zoom = 3)\nmap.asia %&gt;% ggmap()\n\nFor this challenge we will be using another package for the polygons of Asia, the {rworldmap} package.\ninstall.packages(\"rworldmap\")\n\nlibrary(rworldmap)\n\n\nworldmap &lt;- rworldmap::getMap(resolution = \"high\")\ndim(worldmap)\n\nHave a look at the regions and choose Asia.\n\nt(t(table(worldmap$REGION)))\n\n\nasia &lt;- worldmap[which(worldmap$REGION==\"Asia\"),]\nasia%&gt;%class\n\nAs it is a spatial polygon dataframe, and we‚Äôd like to use the geom_sf() function from the ggplot2 package, we transform it to a simple feature object with st_as_sf() function from the sf package.\n\nlibrary(sf)\n\n\nasia_sf &lt;- asia %&gt;%\n  st_as_sf()\n\nasia_sf %&gt;% class()\n\n\nTo map the continent with population estimation by state we can set the option fill= POP_EST.\n\nasia_sf %&gt;%\n  ggplot()+\n  geom_sf(aes(fill=POP_EST))+\n  scale_fill_continuous()\n\n\nInteresting is looking at a different classification of the population classes, and we do this by using the classIntervals() function from the classInt package for classifying the Population Estimation by quantile.\nLet‚Äôs have a look at the population quantiles first. What we can see are the min and the max levels, and the values of the three quantiles, 25%, 50% (median), and the 75%. Which estimation of population follow in each quantile class.\nThe median population estimate for Asia is around 18 million, with some regions having populations of less than 1.5 billion people.\n\nquantile(asia_sf$POP_EST, na.rm=TRUE)\n\n\nasia_sf%&gt;%\n  ggplot(aes(POP_EST))+\n  geom_histogram(aes(fill=SOVEREIGNT),bins = 20)+\n  geom_vline(aes(xintercept = mean(POP_EST)),color=\"lightblue\")+\n  geom_vline(aes(xintercept = median(POP_EST)),color=\"midnightblue\")+\n  geom_text(aes(x=9000000,y=9,label=\"median\"),size=2)+\n  geom_text(aes(x=50000000,y=9,label=\"mean\"),size=2)+\n  scale_x_log10(labels=scales::comma_format(scale = 1/1000),n.breaks =8)+\n  scale_fill_viridis_d()+\n  labs(x=\"Population Estimation (Thousands)\",\n       title=\"Asia Population Distribution\",\n       caption=\"DataSource: {rworldmap} | Graphic: @fgazzelloni\")+\n  ggthemes::theme_clean()+\n  theme(legend.text = element_text(size=5),\n        legend.key.size = unit(5,units = \"pt\"))\n\nWe use the classInt package to find custom intervals of the population. And set up a new object called brks.\n\nlibrary(classInt)\n\n\nbrks &lt;- classIntervals(asia_sf$POP_EST,\n                       n=10, \n                       style=\"quantile\")\nbrks\n\nSet the color scheme:\n\nbrks &lt;- brks$brks\ncolors &lt;- RColorBrewer::brewer.pal(length(brks), \"Spectral\")\n\nFinalize the dataset to use for the map with the population estimation interval cuts.\n\nregion_pop &lt;- asia_sf%&gt;%\n  select(POP_EST)%&gt;%\n  mutate(breaks=case_when(POP_EST &gt; 0 & POP_EST &lt; 625493.5 ~ \"[0,625493.5)\",\n                          POP_EST &gt;= 625493.5 & POP_EST &lt; 2691158 ~ \"[625493.5,2691158)\",\n                          POP_EST &gt;= 2691158 & POP_EST &lt; 4728016 ~ \"[2691158,4728016)\",\n                          POP_EST &gt;= 4728016 & POP_EST &lt; 6834942 ~ \"[4728016,6834942)\",\n                          POP_EST &gt;= 6834942 & POP_EST &lt; 17788961 ~ \"[6834942,17788961)\",\n                          POP_EST &gt;= 17788961 & POP_EST &lt; 23822783 ~ \"[17788961,23822783)\",\n                          POP_EST &gt;= 23822783 & POP_EST &lt; 28625005 ~ \"[23822783,28625005)\",\n                          POP_EST &gt;= 28625005 & POP_EST &lt; 65905410 ~ \"[28625005,65905410)\",\n                          POP_EST &gt;= 65905410 & POP_EST &lt; 141564781 ~ \"[65905410,141564781)\",\n                          POP_EST &gt;= 141564781 & POP_EST &lt;= 1338612968 ~ \"[141564781,1338612968]\"))\n\nSet some information about Asia Population on a text box with the geom_textbox() function from the ggtext package.\n\ntext &lt;- tibble(asia_text=c(\"As of 2022, Asia's 4.6B population thrives in diverse urban centers. Mumbai's density soars at 20.7K/km¬≤, while Tokyo boasts 6.3K/km¬≤. Asia's remarkable density and cultural richness make it the world's most populous and dynamic continent.\"))"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/ggmap/index.html#make-the-map",
    "href": "content/blog/posts/dataviz/posts/ggmap/index.html#make-the-map",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "region_pop %&gt;%\n  ggplot()+\n  geom_sf(aes(fill=breaks))+\n  scale_fill_manual(breaks=c(\"[0,625493.5)\",\"[625493.5,2691158)\",\n                               \"[2691158,4728016)\",\"[4728016,6834942)\",\n                             \n                             \"[6834942,17788961)\",\"[17788961,23822783)\",\n                               \"[23822783,28625005)\",\"[28625005,65905410)\",\n                               \"[65905410,141564781)\",\"[141564781,1338612968]\"),\n                      values=rev(colors))+\n  geom_point(data=asia_cities_full,\n             mapping=aes(lon,lat,size=Population),\n             shape=21,stroke=0.5,\n             alpha=0.7,\n             color=\"grey90\",\n             inherit.aes = F)+\n  scale_size_continuous(labels=scales::comma_format())+\n  geom_text(data=asia_cities_full,\n             mapping=aes(lon,lat,label=City),fontface=\"bold\",\n            check_overlap = T,\n            size=2.1,color=\"white\")+\n  ggtext::geom_textbox(data=text,\n                       mapping=aes(x=60,y=-6,label=text),\n                       size=1.8,width = 0.4,fill=\"grey90\",\n                       family = \"Gill Sans\",\n                       inherit.aes = F)+\n  geom_curve(x=50,xend=67,y=0,yend=20,\n               linewidth=0.2,curvature = -0.5,\n               arrow = arrow(angle=30, \n                             length = unit(0.1, \"inches\"),\n                             ends = \"last\", type = \"open\"),\n      color=\"white\")+\n    geom_curve(x=86,xend=140,y=-5,yend=33,\n               linewidth=0.2,\n               arrow = arrow(angle=30, \n                             length = unit(0.1, \"inches\"),\n                             ends = \"last\", type = \"open\"),\n      color=\"white\")+\n  labs(fill=\"Regions Population\",\n       size=\"Cities Population\",\n       title=\"Asia - Population Level\",\n       caption=\"#30DayMapChallenge 2023 Day6 - ASIA\\nDataSource: Wikipedia & ggmap | Map @fgazzelloni\")+\n  ggthemes::theme_map()+\n  theme(text=element_text(color=\"white\", family = \"Gill Sans\"),\n        plot.title = element_text(face=\"bold\",size=14),\n        plot.caption = element_text(hjust = 0),\n        plot.background = element_rect(fill=\"#4A4A4A\",color=\"#4A4A4A\"),\n        panel.background = element_rect(fill=\"#4A4A4A\",color=\"#4A4A4A\"),\n        legend.background = element_blank(),\n        legend.key = element_rect(color=\"#4A4A4A\",fill=\"#4A4A4A\"),\n        legend.position = \"right\",\n        legend.text = element_text(size=5.5),\n        legend.key.size = unit(5.5,units = \"pt\"))\n\n\nggsave(\"day6_asia.png\",\n       width = 7,height = 4,\n       bg=\"#4A4A4A\")"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/ggmap/index.html#resources",
    "href": "content/blog/posts/dataviz/posts/ggmap/index.html#resources",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "https://cran.r-project.org/web/packages/ggmap/readme/README.html\nhttps://www.r-bloggers.com/2017/09/mapping-the-largest-cities-in-asia-using-r/"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/yarn/index.html",
    "href": "content/blog/posts/dataviz/posts/yarn/index.html",
    "title": "Yarn: hierarchical edge bundling visualization",
    "section": "",
    "text": "This post is all about hierarchical edge bundling visualization, the dataset comes from #TidyTuesday 2022 week 41 Ravelry data.\nThe picture below is the result of the hierarchical edge bundling visualization.\n\n\nHierarchical Edge Bundling Visualization\n\nFirst thing load the libraries and set the fonts:\n\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(RColorBrewer)\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(extrafont)\n\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nfont_add_google(name=\"Pangolin\",family=\"pangolin\")\n\nHelpful tip is how to set the dpi option inside the showtext::showtext_opts function. This sets the size of your text, and it can be very useful when used in conjunction with the same option inside the ggsave function. If showtxet dpi is of a certain value, then you should set the ggsave dpi lower than that value to balance the text size outcome in your final .png file.\nA perfect result comes from a nice balance trade-off between the dpi of the two functions.\nLet‚Äôs have a look at the data, there are 100000 observation and 24 variables referring to the various types of yarns, companies, names, yardage, weights, textures, ratings, ‚Ä¶\n\nyarn &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-10-11/yarn.csv')\n\nyarn%&gt;%dim\n\n\nyarn%&gt;%names\n\nLet‚Äôs select the names, the textures and the yardage for the length of the yarn, which is different from type to type.\n\n  yarn %&gt;%\n  select(yarn_weight_name,\n         texture_clean,\n         texture_clean,yardage)%&gt;%\n  head()\n\nAnd, tidy the texture_clean a bit more, grouping for most common texture names such as merino, acrylic, cotton, nylon, aran, cashmere, wool, silk, jersey,‚Ä¶and calculate the yardage average.\nTidy data:\n\ndf &lt;- yarn%&gt;%\n  mutate(\n    texture_clean=case_when(str_detect(texture_clean,\n                                       \"merino\")~\"merino\",\n                            str_detect(texture_clean,\n                                            \"ply|plied|play|plies\")~\"ply\",\n                            str_detect(texture_clean,\n                                       \"acrylique|acrylic|polyacryl|acrilyc|acryt\")~\"acrylic\",\n                            str_detect(texture_clean,\"nylon\")~\"nylon\",\n                            str_detect(texture_clean,\"cotton\")~\"cotton\",\n                            str_detect(texture_clean,\"wool\")~\"wool\",\n                            str_detect(texture_clean,\"polyamide|polyamid\")~\"polyamid\",\n                            str_detect(texture_clean,\"angora\")~\"angora\",\n                            str_detect(texture_clean,\"cashmere\")~\"cashmere\",\n                            str_detect(texture_clean,\"aran\")~\"aran\",\n                            str_detect(texture_clean,\"silk\")~\"silk\",\n                            str_detect(texture_clean,\"jersey\")~\"jersey\",\n                            TRUE~texture_clean))%&gt;%\n  filter(str_detect(texture_clean,\n                    c(\"merino|ply|acrylic|nylon|cotton|wool|angora|cashmere|aran|silk|jersey\")))%&gt;%\n  count(texture_clean,yarn_weight_name,yardage,grams) %&gt;%\n  mutate(yarn_weight_name=case_when(yarn_weight_name==\"Aran / Worsted\"~\"Aran\",\n                                    yarn_weight_name==\"DK / Sport\"~\"DK\",\n                                    yarn_weight_name==\"Light Fingering\"~\"Fingering\",\n                                    yarn_weight_name==\"Super Bulky\"~\"Bulky\",\n                                    TRUE~yarn_weight_name))%&gt;%\n  filter(!yarn_weight_name==\"No weight specified\",!is.na(yarn_weight_name))%&gt;%\n  filter(!is.na(yardage),!is.na(grams))%&gt;%\n  select(-n)%&gt;%\n  group_by(yarn_weight_name,texture_clean)%&gt;%\n  summarise_all(.funs=mean)%&gt;%\n  select(yarn_weight_name,texture_clean,yardage)\n\n\ndf%&gt;%head\n\n\nNow, for setting the data ready for being used inside one of the ggraph functions, a new vector is created named YARN. This is done to have a central point to all the yarns‚Äô types.\nSo, what is needed is a dataframe with two columns from and to. Actually, what is needed are two dataframe hierarchy and vertices. As follow:\n\nd1&lt;- df%&gt;%\n  select(-texture_clean,-yardage)%&gt;%\n  mutate(from = \"YARN\",.before=everything())%&gt;%\n  rename(to = yarn_weight_name)\n  \nd2 &lt;- df%&gt;%\n  select(-yardage) %&gt;%\n  rename(from = yarn_weight_name, \n         to = texture_clean)\n  \n\nhierarchy &lt;- rbind(d1, d2)\nvertices &lt;- data.frame(name = unique(c(as.character(hierarchy$from), \n                                       as.character(hierarchy$to))) ) \n\nHierarchy:\n\nhierarchy%&gt;%head\n\nVertices:\n\nvertices%&gt;%head\n\nThen create the graph and the layout with graph_from_data_frame() function.\n\nmygraph &lt;- graph_from_data_frame(hierarchy, vertices=vertices )\n\n\nggraph(mygraph, layout = 'dendrogram', circular = F) + \n  geom_edge_diagonal()\n\n\nggraph(mygraph, layout = 'dendrogram', circular = T) + \n  geom_edge_diagonal()+\n  geom_node_point(color=\"navy\",size=5)\n\nI can even filter the leafs out to point just the main nodes:\n\nggraph(mygraph, layout = 'dendrogram', circular = T) + \n  geom_edge_diagonal()+\n  geom_node_point(aes(filter=!leaf),\n                  color=\"navy\",\n                  size=5)\n\nI like the circular type, and we can have a look at the inside calculation of the function with create_layout() specifying the type of layout as a dendrogram. It is a dataframe graph and it has the x, and y vectors, the leafs, and the names and other specifications.\n\ndf1 &lt;- create_layout(mygraph, layout = 'dendrogram')\ndf1%&gt;%class\n\n\ndf1%&gt;%head()\n\nTo add the labels to the leafs they would need to be oriented by a specific angle level, the reason for this is that the subgroups are not all the same.\nWhat is needed is a function for node angle adjustments and another similar function to adjust the horizontal distance of the text around the dendrogram. Likely the {ggraph} package provides a function for calculating the angles of your data:\n- node_angle(x,y)\n\nnode_angle(df1$x,df1$y,degrees = T)%&gt;%head()\n\nThese values need to be adjusted:\n\nnode_ang_adj &lt;- function(x,y) {\n  ifelse(node_angle(x,y) &gt; 90 & node_angle(x,y) &lt; 270 , \n         node_angle(x,y) + 180, node_angle(x,y))\n  }\n\nnode_hjust_adj &lt;- function(x,y) {\n  ifelse(node_angle(x,y) &gt; 90 & node_angle(x,y) &lt; 270 , 1,0)\n}\n\nFinally, we can make the hierarchical edge bundling visualization type circular dendrogram:\n\nggraph(mygraph, layout = 'dendrogram', circular = TRUE) + \n  geom_edge_diagonal(aes(color=factor(x)),\n                     alpha=0.9,\n                     show.legend = F) +\n  geom_node_point(aes(color=factor(x)),\n                  size=10,\n                  show.legend = F)+\n  geom_node_point(aes(color=factor(x)),\n                  size=10,\n                  shape=8,\n                  show.legend = F)+\n  geom_node_label(aes(filter=!leaf,label=name,color=factor(x)),\n                  label.padding = unit(0.1, \"lines\"),\n                  label.r = unit(0.1, \"lines\"),\n                  label.size = 0.1,\n                  family = \"pangolin\",\n                  fontface=\"bold\",\n                  show.legend = F,\n                  size=4, \n                  alpha=1)+\n  geom_node_text(aes(x = x*1.1, \n                     y=y*1.1, \n                     hjust = node_hjust_adj(x,y),\n                     angle=node_ang_adj(x,y),\n                     filter = leaf, \n                     label=name,\n                     color=factor(x)),\n                 family = \"pangolin\",\n                 fontface=\"bold\",\n                 show.legend = F,\n                 size=4, \n                 alpha=1)+\n  scale_color_manual(values = rep(RColorBrewer::brewer.pal(10,\"Paired\"),10))+\n  scale_x_discrete(expand = c(0,0.3))+\n  scale_y_discrete(expand = c(0,0.3))+\n  coord_fixed()+\n  labs(caption=\"What's inside your YARN?\\ntextures for each type\\n\\nDataSource: #TidyTuesday 2022 week41 Ravelry data\\nDataViz: Federica Gazzelloni (FG) Twitter: @fgazzelloni\\n\",\n       alt=\"Infographics\") +\n  theme_graph()+\n  theme(plot.margin = margin(5,5,5,5,unit = \"pt\"),\n        plot.caption = element_text(face=\"bold\",family=\"pangolin\"))\n\nSave it with setting dpi:\n\nggsave(\"featured.png\",\n      dpi=280,\n      bg=\"white\",\n      width = 9,height = 9)\n\n\n\nHierarchical edge bundling\nggplot extensions\nggraph\ntidygraph\nigraph"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/yarn/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/yarn/index.html#overview",
    "title": "Yarn: hierarchical edge bundling visualization",
    "section": "",
    "text": "This post is all about hierarchical edge bundling visualization, the dataset comes from #TidyTuesday 2022 week 41 Ravelry data.\nThe picture below is the result of the hierarchical edge bundling visualization.\n\n\nHierarchical Edge Bundling Visualization\n\nFirst thing load the libraries and set the fonts:\n\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(RColorBrewer)\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(extrafont)\n\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nfont_add_google(name=\"Pangolin\",family=\"pangolin\")\n\nHelpful tip is how to set the dpi option inside the showtext::showtext_opts function. This sets the size of your text, and it can be very useful when used in conjunction with the same option inside the ggsave function. If showtxet dpi is of a certain value, then you should set the ggsave dpi lower than that value to balance the text size outcome in your final .png file.\nA perfect result comes from a nice balance trade-off between the dpi of the two functions.\nLet‚Äôs have a look at the data, there are 100000 observation and 24 variables referring to the various types of yarns, companies, names, yardage, weights, textures, ratings, ‚Ä¶\n\nyarn &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-10-11/yarn.csv')\n\nyarn%&gt;%dim\n\n\nyarn%&gt;%names\n\nLet‚Äôs select the names, the textures and the yardage for the length of the yarn, which is different from type to type.\n\n  yarn %&gt;%\n  select(yarn_weight_name,\n         texture_clean,\n         texture_clean,yardage)%&gt;%\n  head()\n\nAnd, tidy the texture_clean a bit more, grouping for most common texture names such as merino, acrylic, cotton, nylon, aran, cashmere, wool, silk, jersey,‚Ä¶and calculate the yardage average.\nTidy data:\n\ndf &lt;- yarn%&gt;%\n  mutate(\n    texture_clean=case_when(str_detect(texture_clean,\n                                       \"merino\")~\"merino\",\n                            str_detect(texture_clean,\n                                            \"ply|plied|play|plies\")~\"ply\",\n                            str_detect(texture_clean,\n                                       \"acrylique|acrylic|polyacryl|acrilyc|acryt\")~\"acrylic\",\n                            str_detect(texture_clean,\"nylon\")~\"nylon\",\n                            str_detect(texture_clean,\"cotton\")~\"cotton\",\n                            str_detect(texture_clean,\"wool\")~\"wool\",\n                            str_detect(texture_clean,\"polyamide|polyamid\")~\"polyamid\",\n                            str_detect(texture_clean,\"angora\")~\"angora\",\n                            str_detect(texture_clean,\"cashmere\")~\"cashmere\",\n                            str_detect(texture_clean,\"aran\")~\"aran\",\n                            str_detect(texture_clean,\"silk\")~\"silk\",\n                            str_detect(texture_clean,\"jersey\")~\"jersey\",\n                            TRUE~texture_clean))%&gt;%\n  filter(str_detect(texture_clean,\n                    c(\"merino|ply|acrylic|nylon|cotton|wool|angora|cashmere|aran|silk|jersey\")))%&gt;%\n  count(texture_clean,yarn_weight_name,yardage,grams) %&gt;%\n  mutate(yarn_weight_name=case_when(yarn_weight_name==\"Aran / Worsted\"~\"Aran\",\n                                    yarn_weight_name==\"DK / Sport\"~\"DK\",\n                                    yarn_weight_name==\"Light Fingering\"~\"Fingering\",\n                                    yarn_weight_name==\"Super Bulky\"~\"Bulky\",\n                                    TRUE~yarn_weight_name))%&gt;%\n  filter(!yarn_weight_name==\"No weight specified\",!is.na(yarn_weight_name))%&gt;%\n  filter(!is.na(yardage),!is.na(grams))%&gt;%\n  select(-n)%&gt;%\n  group_by(yarn_weight_name,texture_clean)%&gt;%\n  summarise_all(.funs=mean)%&gt;%\n  select(yarn_weight_name,texture_clean,yardage)\n\n\ndf%&gt;%head\n\n\nNow, for setting the data ready for being used inside one of the ggraph functions, a new vector is created named YARN. This is done to have a central point to all the yarns‚Äô types.\nSo, what is needed is a dataframe with two columns from and to. Actually, what is needed are two dataframe hierarchy and vertices. As follow:\n\nd1&lt;- df%&gt;%\n  select(-texture_clean,-yardage)%&gt;%\n  mutate(from = \"YARN\",.before=everything())%&gt;%\n  rename(to = yarn_weight_name)\n  \nd2 &lt;- df%&gt;%\n  select(-yardage) %&gt;%\n  rename(from = yarn_weight_name, \n         to = texture_clean)\n  \n\nhierarchy &lt;- rbind(d1, d2)\nvertices &lt;- data.frame(name = unique(c(as.character(hierarchy$from), \n                                       as.character(hierarchy$to))) ) \n\nHierarchy:\n\nhierarchy%&gt;%head\n\nVertices:\n\nvertices%&gt;%head\n\nThen create the graph and the layout with graph_from_data_frame() function.\n\nmygraph &lt;- graph_from_data_frame(hierarchy, vertices=vertices )\n\n\nggraph(mygraph, layout = 'dendrogram', circular = F) + \n  geom_edge_diagonal()\n\n\nggraph(mygraph, layout = 'dendrogram', circular = T) + \n  geom_edge_diagonal()+\n  geom_node_point(color=\"navy\",size=5)\n\nI can even filter the leafs out to point just the main nodes:\n\nggraph(mygraph, layout = 'dendrogram', circular = T) + \n  geom_edge_diagonal()+\n  geom_node_point(aes(filter=!leaf),\n                  color=\"navy\",\n                  size=5)\n\nI like the circular type, and we can have a look at the inside calculation of the function with create_layout() specifying the type of layout as a dendrogram. It is a dataframe graph and it has the x, and y vectors, the leafs, and the names and other specifications.\n\ndf1 &lt;- create_layout(mygraph, layout = 'dendrogram')\ndf1%&gt;%class\n\n\ndf1%&gt;%head()\n\nTo add the labels to the leafs they would need to be oriented by a specific angle level, the reason for this is that the subgroups are not all the same.\nWhat is needed is a function for node angle adjustments and another similar function to adjust the horizontal distance of the text around the dendrogram. Likely the {ggraph} package provides a function for calculating the angles of your data:\n- node_angle(x,y)\n\nnode_angle(df1$x,df1$y,degrees = T)%&gt;%head()\n\nThese values need to be adjusted:\n\nnode_ang_adj &lt;- function(x,y) {\n  ifelse(node_angle(x,y) &gt; 90 & node_angle(x,y) &lt; 270 , \n         node_angle(x,y) + 180, node_angle(x,y))\n  }\n\nnode_hjust_adj &lt;- function(x,y) {\n  ifelse(node_angle(x,y) &gt; 90 & node_angle(x,y) &lt; 270 , 1,0)\n}\n\nFinally, we can make the hierarchical edge bundling visualization type circular dendrogram:\n\nggraph(mygraph, layout = 'dendrogram', circular = TRUE) + \n  geom_edge_diagonal(aes(color=factor(x)),\n                     alpha=0.9,\n                     show.legend = F) +\n  geom_node_point(aes(color=factor(x)),\n                  size=10,\n                  show.legend = F)+\n  geom_node_point(aes(color=factor(x)),\n                  size=10,\n                  shape=8,\n                  show.legend = F)+\n  geom_node_label(aes(filter=!leaf,label=name,color=factor(x)),\n                  label.padding = unit(0.1, \"lines\"),\n                  label.r = unit(0.1, \"lines\"),\n                  label.size = 0.1,\n                  family = \"pangolin\",\n                  fontface=\"bold\",\n                  show.legend = F,\n                  size=4, \n                  alpha=1)+\n  geom_node_text(aes(x = x*1.1, \n                     y=y*1.1, \n                     hjust = node_hjust_adj(x,y),\n                     angle=node_ang_adj(x,y),\n                     filter = leaf, \n                     label=name,\n                     color=factor(x)),\n                 family = \"pangolin\",\n                 fontface=\"bold\",\n                 show.legend = F,\n                 size=4, \n                 alpha=1)+\n  scale_color_manual(values = rep(RColorBrewer::brewer.pal(10,\"Paired\"),10))+\n  scale_x_discrete(expand = c(0,0.3))+\n  scale_y_discrete(expand = c(0,0.3))+\n  coord_fixed()+\n  labs(caption=\"What's inside your YARN?\\ntextures for each type\\n\\nDataSource: #TidyTuesday 2022 week41 Ravelry data\\nDataViz: Federica Gazzelloni (FG) Twitter: @fgazzelloni\\n\",\n       alt=\"Infographics\") +\n  theme_graph()+\n  theme(plot.margin = margin(5,5,5,5,unit = \"pt\"),\n        plot.caption = element_text(face=\"bold\",family=\"pangolin\"))\n\nSave it with setting dpi:\n\nggsave(\"featured.png\",\n      dpi=280,\n      bg=\"white\",\n      width = 9,height = 9)\n\n\n\nHierarchical edge bundling\nggplot extensions\nggraph\ntidygraph\nigraph"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/dubois2024/challenge01/index.html",
    "href": "content/blog/posts/dataviz/posts/dubois2024/challenge01/index.html",
    "title": "#DuboisChallenge2024 Challenge 01",
    "section": "",
    "text": "My contributions to the #DuboisChallenge2024.\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(patchwork)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/dubois2024/challenge01/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/dubois2024/challenge01/index.html#overview",
    "title": "#DuboisChallenge2024 Challenge 01",
    "section": "",
    "text": "My contributions to the #DuboisChallenge2024.\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(patchwork)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/dubois2024/challenge01/index.html#data-ready-from-the-github-repo",
    "href": "content/blog/posts/dataviz/posts/dubois2024/challenge01/index.html#data-ready-from-the-github-repo",
    "title": "#DuboisChallenge2024 Challenge 01",
    "section": "Data ready from the GitHub Repo",
    "text": "Data ready from the GitHub Repo\nDownload the georgia-1880-county-shapefile.zip file from: https://github.com/ajstarks/dubois-data-portraits/tree/master/challenge/2024/challenge01\n\ngeorgia_shp &lt;- sf::read_sf(\"data/georgia-1880-county-shapefile\")\n\n# georgia_shp%&gt;%head\n\n\ngeorgia_shp%&gt;%\n  ggplot()+\n  geom_sf()\n\n\ndat_sf &lt;- georgia_shp%&gt;%\n  janitor::clean_names()%&gt;%\n  separate(data1870,into=c(\"up70\",\"down70\"))%&gt;%\n  separate(data1880_p,into=c(\"up80\",\"down80\"))%&gt;%\n  mutate(# pop 1870\n         up70=ifelse(up70==\"\",0,up70),\n         down70=ifelse(is.na(down70),0,down70),\n         up70=as.numeric(up70),\n         down70=as.numeric(down70),\n         # pop 1880\n         up80=ifelse(up80==\"\",0,up80),\n         down80=ifelse(is.na(down80),0,down80),\n         up80=as.numeric(up80),\n         down80=as.numeric(down80))%&gt;%\n  rowwise()%&gt;%\n  mutate(pop70=mean(up70,down70),\n         pop80=mean(up80,down80))%&gt;%\n  arrange(pop70,pop80)\n\n\ndata &lt;- dat_sf%&gt;%select(county=icpsrnam,\n                pop70,pop80)%&gt;%\n  mutate(id=case_when(pop70 == 0 ~ 7,\n                      pop70 == 1000 ~ 6,\n                      pop70 == 2500 ~ 5,\n                      pop70 == 5000 ~ 4,\n                      pop70 == 10000 ~ 3,\n                      pop70 == 15000 ~ 2,\n                      pop70 == 20000 ~ 1),\n         pop70=case_when(pop70 == 0 ~ \"UNDER 1,000\",\n                         pop70 == 1000 ~ \"1000 TO 2,500\",\n                         pop70 == 2500 ~ \"2,500 TO 5,000\",\n                         pop70 == 5000 ~ \"5,000 TO 10,000\",\n                         pop70 == 10000 ~ \"10,000 TO 15,000\",\n                         pop70 == 15000 ~ \"15,000 TO 20,000\",\n                         pop70 == 20000 ~ \"20,000 TO 30,000\"))%&gt;%\n  # pop80\n    mutate(id=case_when(pop80 == 0 ~ 7,\n                      pop80 == 1000 ~ 6,\n                      pop80 == 2500 ~ 5,\n                      pop80 == 5000 ~ 4,\n                      pop80 == 10000 ~ 3,\n                      pop80 == 15000 ~ 2,\n                      pop80 == 20000 ~ 1),\n         pop80=case_when(pop80 == 0 ~ \"UNDER 1,000\",\n                         pop80 == 1000 ~ \"1000 TO 2,500\",\n                         pop80 == 2500 ~ \"2,500 TO 5,000\",\n                         pop80 == 5000 ~ \"5,000 TO 10,000\",\n                         pop80 == 10000 ~ \"10,000 TO 15,000\",\n                         pop80 == 15000 ~ \"15,000 TO 20,000\",\n                         pop80 == 20000 ~ \"20,000 TO 30,000\"))\ndata%&gt;%count(id,pop80)\n\nDybois Style\nFonts:\n\nlibrary(sysfonts)\nlibrary(showtext)\nsysfonts::font_add_google(\"Public Sans\",\"Public Sans\")\n# font_add_google(\"Carter One\", \"Carter One\")\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\n\nColors:\nBackground:\n\n\"#e7d6c5\"\n\nText:\n\nc(\"#483c32\",\"#bbaa98\")\n\n\nlegend_colors &lt;- c(\"#372c59\",\"#7a5039\",\"#c29e84\",\"#d63352\",\n  \"#e79d96\",\"#edb456\",\"#4b5c4f\")\n\nBounding box: xmin: 939223.1 ymin: -701249.8 xmax: 1425004 ymax: -200888.5\n\npop70_map &lt;- data%&gt;%\n  ggplot()+\n  geom_sf(aes(fill=pop70),\n          show.legend = F,\n          color=\"#483c32\",alpha=0.9,\n          linewidth=0.1)+\n  scale_fill_manual(values=c(\"UNDER 1,000\"=\"#4b5c4f\",\n                             \"1000 TO 2,500\"=\"#edb456\",\n                             \"2,500 TO 5,000\"=\"#e79d96\",\n                             \"5,000 TO 10,000\"=\"#d63352\",\n                             \"10,000 TO 15,000\"=\"#c29e84\",\n                             \"15,000 TO 20,000\"=\"#7a5039\",\n                             \"20,000 TO 30,000\"=\"#372c59\"),na.value = \"#e0cebb\")+\n    annotate(\"text\", x = -84.45, y = 35.1,\n           label = \"1870\",\n           size = 3.5,color=\"#483c32\",\n           fontface = \"bold\",\n           family =  \"Public Sans\" ) +\n   coord_sf(crs=4326,clip = \"off\")+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\n  \npop70_map\n\n\npop80_map &lt;- data%&gt;%\n  ggplot()+\n  geom_sf(aes(fill=pop80),\n          show.legend = F,\n          color=\"#483c32\",alpha=0.9,\n          linewidth=0.1)+\n  scale_fill_manual(values=c(\"UNDER 1,000\"=\"#4b5c4f\",\n                             \"1000 TO 2,500\"=\"#edb456\",\n                             \"2,500 TO 5,000\"=\"#e79d96\",\n                             \"5,000 TO 10,000\"=\"#d63352\",\n                             \"10,000 TO 15,000\"=\"#c29e84\",\n                             \"15,000 TO 20,000\"=\"#7a5039\",\n                             \"20,000 TO 30,000\"=\"#372c59\"),na.value = \"#e0cebb\")+\n      annotate(\"text\", x = -84.45, y = 35.1,\n           label = \"1880\",\n           size = 3.5,color=\"#483c32\",\n           fontface = \"bold\",\n           family =  \"Public Sans\" ) +\n  coord_sf(crs=4326,clip = \"off\")+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\n  \npop80_map\n\nPlot layout\nsource: https://ggplot2-book.org/arranging-plots\n\npop70_map+ ggplot() + ggplot()+ pop80_map + plot_layout(ncol = 2,nrow = 2)\n\n\nlegend1 &lt;- tibble(x=0,y=c(4,3,2,1),\n       label=c(\"5,000 TO 10,000\",\n                   \"2,500 TO 5,000\",\n                    \"1000 TO 2,500\",\n                   \"UNDER 1,000\"),\n       pal=c(\"#d63352\",\"#e79d96\",\"#edb456\",\"#4b5c4f\"))%&gt;%\n  mutate(pal=as.factor(pal))\nlegend1\n\n\nlegend1_plot &lt;- legend1%&gt;%\n  ggplot(aes(x,y))+\n  geom_point(aes(fill=label),\n             shape=21,stroke=0.1,\n             size=8.5,\n             show.legend = F)+\n  scale_fill_manual(values=c(\"5,000 TO 10,000\"=\"#d63352\",\n                                \"2,500 TO 5,000\"=\"#e79d96\",\n                                \"1000 TO 2,500\"=\"#edb456\",\n                                \"UNDER 1,000\"=\"#4b5c4f\"))+\n  geom_text(aes(label=label),\n            family=\"Public Sans\",\n            size=3.5,color=\"#7a5039\",\n            nudge_x = 0,hjust=-0.2)+\n  coord_cartesian(xlim=c(-0.2,1),ylim =c(-0,5) )+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\nlegend1_plot\n\n\nlegend2 &lt;- tibble(x=0,y=c(1,2,3),\n       label=c(\"10,000 TO 15,000\",\n               \"15,000 TO 20,000\",\n               \"BETWEEN 20,000 AND 30,000\"),\n       color=c(\"#c29e84\",\"#7a5039\",\"#372c59\"))                             \n\n\nlegend2_plot &lt;- legend2%&gt;%\n  ggplot(aes(x,y))+\n  geom_point(aes(fill=label),\n             shape=21,stroke=0.1,\n             size=8.5,\n             show.legend = F)+\n  scale_fill_manual(values=c(\"10,000 TO 15,000\"=\"#c29e84\",\n                             \"15,000 TO 20,000\"=\"#7a5039\",\n                             \"BETWEEN 20,000 AND 30,000\"=\"#372c59\"))+\n  geom_text(aes(label=label),\n            size=3.5,color=\"#7a5039\",\n            family=\"Public Sans\",\n            nudge_x = 0.5,hjust=0)+\n  coord_cartesian(xlim=c(-0.2,7),ylim =c(-1,4) )+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\n\n\nlegend2_plot\n\n\npop70_map+ legend2_plot + legend1_plot+ pop80_map + plot_layout(ncol = 2,nrow = 2)+plot_annotation(\n  title = \"NEGRO POPULATION OF GEORGIA BY COUNTIES.\",\n  caption=\"#DuboisChallenge24| Week1 | by Federica Gazzelloni\",\n  theme = theme_void(base_family = \"Public Sans\"))&\n  theme(text=element_text(color=\"#483c32\",face=\"bold\"),\n        plot.title = element_text(hjust=0.5),\n        plot.caption = element_text(size=9),\n        plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\n\n\nggsave(\"challenge01.png\",bg=\"#e7d6c5\",height = 8.8)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/network/index.html",
    "href": "content/blog/posts/dataviz/posts/network/index.html",
    "title": "Network of game mechanics",
    "section": "",
    "text": "I‚Äôve chosen this graphic for my blog because it turned out to be very interesting. As you can see reading through the article, the shape of the network changes along with the change of the parameters.\nThe dataset I‚Äôve used for making this network comes from #TidyTuesday 2022 week 4 Board games.\nThe picture below is the result of the network visualization.\n\n\nBoard games\n\nThe first step is to load the library needed for making the manipulations. I usually load {tidyverse} package because it contains a series of sub packages and functions that are all that is needed for this first part of the data wrangling. Also, it provides the pipe %&gt;% operator, which is useful for linking different functions through subsetting the dataset.\n\nlibrary(tidyverse)\n\nThe data sets provided can be loaded from the source like this:\n\nratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv')\ndetails &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv')\n\nI‚Äôve also added few line of code for backing the original datasets by saving them as .RDS files, a light file format to store information in.\n\nsaveRDS(ratings,\"ratings.rds\")\nsaveRDS(details,\"details.rds\")\n\nAnd assigned them to new variables:\n\nrat &lt;- readRDS(\"ratings.rds\")\ndet &lt;- readRDS(\"details.rds\")\n\n\n\nLet‚Äôs see the variable‚Äôs names inside the sets.\n\nnames(rat)\n\n\nnames(det)\n\nBased on the variables in the data sets, I‚Äôve started googling for some information nad/or visualizations about Board games, to see if I could find any inspiration from past submissions, and in fact found this source of inspiration: https://www.thewayir.com/blog/boardgames/. Looking through the article found the code and the type of visualization I had in mind, so started replicating the code from the article. My surprise was that data updating and my manipulation slightly changed the output of the plot.\nLet‚Äôs go a bit more in deep about that. I‚Äôll go through the steps for replicatiing the network but then sligtly change the output to what you can see in the picture.\nAmong the required libraries found {widyr} package which was very new to me.\n\nEncapsulates the pattern of untidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several operations such as co-occurrence counts, correlations, or clustering that are mathematically convenient.\n\nAnd then the other packages such as {igraph}, {ggraph}, and {ggforce}, all packages for making networks of data, and for making extra features.\n\nrequire(widyr)\nrequire(igraph)\nrequire(ggraph)\nrequire(ggforce)\n\n\n\nWhat‚Äôs the best manipulation for making a graph?\nHere is the first part of the data-wrangling\n\nboard_games &lt;- rat %&gt;%\n  select(id,name) %&gt;%\n  left_join(select(det,id,boardgamemechanic),by=\"id\") %&gt;%\n  rename(mechanic=boardgamemechanic) %&gt;%\n  tidyr::separate_rows(mechanic, sep = \",\") %&gt;% \n  mutate(mechanic = str_remove_all(mechanic, \"[[:punct:]]\"),\n         mechanic = str_trim(mechanic),\n         mechanic = gsub(\"^and \",\"\",mechanic)) %&gt;% \n  filter(!is.na(mechanic))\n\n\nkableExtra::kable(head(board_games))\n\nHere is the second part of the wrangling\n\nmechanic &lt;- board_games %&gt;% \n  count(mechanic,sort=T) %&gt;%\n  mutate(mechanic_pct=round(n/sum(n)*100,2))%&gt;%\n  left_join(select(board_games,name,mechanic),by=\"mechanic\") %&gt;%\n  mutate(name=as.factor(name),mechanic=as.factor(mechanic)) %&gt;% \n   distinct() \n\nThis part is for setting the fonts\n\nlibrary(extrafont)\nlibrary(showtext)\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nlibrary(sysfonts)\n#font_families_google()\nfont_add_google(name=\"Piedra\",family=\"games\")\n\nfamily = \"games\"\n\nSelect the first 50 games\n\nboard_games50 &lt;-board_games%&gt;%\n  select(name,mechanic)%&gt;%\n  count(name,sort=T) %&gt;%\n  slice(1:50)\n\n\nThe interesting part is here: if we change the filtering level of the mechanic_pct and/or the widyr::pairwise_cor() from the widyr package we can see the graph changing along with it. More changes if the level of correlation changes to a lower value more than if set to a higher value.\n\ndf &lt;- board_games50%&gt;%\n  left_join(mechanic,by=\"name\") %&gt;%\n  filter(mechanic_pct &gt; 1) %&gt;%\n  pairwise_cor(mechanic, name, sort = T) %&gt;% \n  filter(correlation &gt; .1)\n\nThe function igraph::graph_from_data_frame() transform data frames into igraph graphs. In addition the funtion igraph::tkplot() can be useful for looking at the graph under different perspectives.\n\np&lt;-networkD3::simpleNetwork(df)\n# save the widget\nlibrary(htmlwidgets)\nsaveWidget(p, file=paste0( getwd(), \"/networkInteractive1.html\"))\n\n\n# manipulate(Edge=picker(\"correlation\",\"from\",\"to\"),correlation=slider(0,1,0.1)\nlibrary(tidygraph)\n df %&gt;% \n  tidygraph::as_tbl_graph(mode = \"out\") %&gt;%\n    activate(edges)%&gt;%#as_tibble()\n  ggraph() +\n  geom_edge_link(linejoin = \"round\",\n                 edge_colour=\"grey5\",\n                 edge_width=0.5,\n                 edge_linetype=\"solid\") +\n  geom_node_point(color=\"midnightblue\",size=20,alpha=0.4)\n\nThis is the final version of the plot:\n\nplot &lt;- df %&gt;% \n  igraph::graph_from_data_frame() %&gt;% \n  ggraph() +\n  geom_edge_link(linejoin = \"round\",\n                 color=\"grey5\",\n                 edge_colour=\"red\",\n                 edge_width=0.5,\n                 edge_linetype=\"solid\") +\n  geom_node_point(color=\"midnightblue\",size=40,alpha=0.4) +\n  geom_node_text(aes(label = name), \n                 repel = T,\n                 size=5,\n                 nudge_y = 0,\n                 color=\"orange\",\n                 family=family) + \n  theme_void() +\n   theme(text = element_text(family=family),\n         plot.background = element_rect(color=\"beige\",fill=\"beige\"))\n\n\n\n\nAdding some features and save\n\nlibrary(cowplot)\n\nfinal &lt;-ggdraw()+\n  draw_plot(plot) +\n  draw_label(\"Network of game \\nmechanics\",x=0.5,y=0.85,size=55,fontfamily=family)+\n  draw_label(\"Sliced by the first 50 games by frequency, \n             filtered mechanics greater than 2% proportion of total,\n             then finally taken just the most highly correlated ones\",\n             x=0.8,y=0.12,size=11,fontfamily=family) +\n  draw_label(\"DataSource: Kaggle & Board Games Geek | Viz: Federica Gazzelloni\",\n             x=0.8,y=0.03,angle=0,size=11,alpha=0.5,fontfamily=family) +\n   draw_image(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/static/plot_logo.png\",x=0.09,y=-0.47,scale=0.05)\n\n\nggsave(\"w4_board_games.png\",\n        plot =final,\n        bg=\"white\",\n        dpi = 320,\n        width = 11,\n        height = 6\n       )\n\n\n\ntidygraph\nigraph\nggnet"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/network/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/network/index.html#overview",
    "title": "Network of game mechanics",
    "section": "",
    "text": "I‚Äôve chosen this graphic for my blog because it turned out to be very interesting. As you can see reading through the article, the shape of the network changes along with the change of the parameters.\nThe dataset I‚Äôve used for making this network comes from #TidyTuesday 2022 week 4 Board games.\nThe picture below is the result of the network visualization.\n\n\nBoard games\n\nThe first step is to load the library needed for making the manipulations. I usually load {tidyverse} package because it contains a series of sub packages and functions that are all that is needed for this first part of the data wrangling. Also, it provides the pipe %&gt;% operator, which is useful for linking different functions through subsetting the dataset.\n\nlibrary(tidyverse)\n\nThe data sets provided can be loaded from the source like this:\n\nratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv')\ndetails &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv')\n\nI‚Äôve also added few line of code for backing the original datasets by saving them as .RDS files, a light file format to store information in.\n\nsaveRDS(ratings,\"ratings.rds\")\nsaveRDS(details,\"details.rds\")\n\nAnd assigned them to new variables:\n\nrat &lt;- readRDS(\"ratings.rds\")\ndet &lt;- readRDS(\"details.rds\")\n\n\n\nLet‚Äôs see the variable‚Äôs names inside the sets.\n\nnames(rat)\n\n\nnames(det)\n\nBased on the variables in the data sets, I‚Äôve started googling for some information nad/or visualizations about Board games, to see if I could find any inspiration from past submissions, and in fact found this source of inspiration: https://www.thewayir.com/blog/boardgames/. Looking through the article found the code and the type of visualization I had in mind, so started replicating the code from the article. My surprise was that data updating and my manipulation slightly changed the output of the plot.\nLet‚Äôs go a bit more in deep about that. I‚Äôll go through the steps for replicatiing the network but then sligtly change the output to what you can see in the picture.\nAmong the required libraries found {widyr} package which was very new to me.\n\nEncapsulates the pattern of untidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several operations such as co-occurrence counts, correlations, or clustering that are mathematically convenient.\n\nAnd then the other packages such as {igraph}, {ggraph}, and {ggforce}, all packages for making networks of data, and for making extra features.\n\nrequire(widyr)\nrequire(igraph)\nrequire(ggraph)\nrequire(ggforce)\n\n\n\nWhat‚Äôs the best manipulation for making a graph?\nHere is the first part of the data-wrangling\n\nboard_games &lt;- rat %&gt;%\n  select(id,name) %&gt;%\n  left_join(select(det,id,boardgamemechanic),by=\"id\") %&gt;%\n  rename(mechanic=boardgamemechanic) %&gt;%\n  tidyr::separate_rows(mechanic, sep = \",\") %&gt;% \n  mutate(mechanic = str_remove_all(mechanic, \"[[:punct:]]\"),\n         mechanic = str_trim(mechanic),\n         mechanic = gsub(\"^and \",\"\",mechanic)) %&gt;% \n  filter(!is.na(mechanic))\n\n\nkableExtra::kable(head(board_games))\n\nHere is the second part of the wrangling\n\nmechanic &lt;- board_games %&gt;% \n  count(mechanic,sort=T) %&gt;%\n  mutate(mechanic_pct=round(n/sum(n)*100,2))%&gt;%\n  left_join(select(board_games,name,mechanic),by=\"mechanic\") %&gt;%\n  mutate(name=as.factor(name),mechanic=as.factor(mechanic)) %&gt;% \n   distinct() \n\nThis part is for setting the fonts\n\nlibrary(extrafont)\nlibrary(showtext)\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nlibrary(sysfonts)\n#font_families_google()\nfont_add_google(name=\"Piedra\",family=\"games\")\n\nfamily = \"games\"\n\nSelect the first 50 games\n\nboard_games50 &lt;-board_games%&gt;%\n  select(name,mechanic)%&gt;%\n  count(name,sort=T) %&gt;%\n  slice(1:50)\n\n\nThe interesting part is here: if we change the filtering level of the mechanic_pct and/or the widyr::pairwise_cor() from the widyr package we can see the graph changing along with it. More changes if the level of correlation changes to a lower value more than if set to a higher value.\n\ndf &lt;- board_games50%&gt;%\n  left_join(mechanic,by=\"name\") %&gt;%\n  filter(mechanic_pct &gt; 1) %&gt;%\n  pairwise_cor(mechanic, name, sort = T) %&gt;% \n  filter(correlation &gt; .1)\n\nThe function igraph::graph_from_data_frame() transform data frames into igraph graphs. In addition the funtion igraph::tkplot() can be useful for looking at the graph under different perspectives.\n\np&lt;-networkD3::simpleNetwork(df)\n# save the widget\nlibrary(htmlwidgets)\nsaveWidget(p, file=paste0( getwd(), \"/networkInteractive1.html\"))\n\n\n# manipulate(Edge=picker(\"correlation\",\"from\",\"to\"),correlation=slider(0,1,0.1)\nlibrary(tidygraph)\n df %&gt;% \n  tidygraph::as_tbl_graph(mode = \"out\") %&gt;%\n    activate(edges)%&gt;%#as_tibble()\n  ggraph() +\n  geom_edge_link(linejoin = \"round\",\n                 edge_colour=\"grey5\",\n                 edge_width=0.5,\n                 edge_linetype=\"solid\") +\n  geom_node_point(color=\"midnightblue\",size=20,alpha=0.4)\n\nThis is the final version of the plot:\n\nplot &lt;- df %&gt;% \n  igraph::graph_from_data_frame() %&gt;% \n  ggraph() +\n  geom_edge_link(linejoin = \"round\",\n                 color=\"grey5\",\n                 edge_colour=\"red\",\n                 edge_width=0.5,\n                 edge_linetype=\"solid\") +\n  geom_node_point(color=\"midnightblue\",size=40,alpha=0.4) +\n  geom_node_text(aes(label = name), \n                 repel = T,\n                 size=5,\n                 nudge_y = 0,\n                 color=\"orange\",\n                 family=family) + \n  theme_void() +\n   theme(text = element_text(family=family),\n         plot.background = element_rect(color=\"beige\",fill=\"beige\"))\n\n\n\n\nAdding some features and save\n\nlibrary(cowplot)\n\nfinal &lt;-ggdraw()+\n  draw_plot(plot) +\n  draw_label(\"Network of game \\nmechanics\",x=0.5,y=0.85,size=55,fontfamily=family)+\n  draw_label(\"Sliced by the first 50 games by frequency, \n             filtered mechanics greater than 2% proportion of total,\n             then finally taken just the most highly correlated ones\",\n             x=0.8,y=0.12,size=11,fontfamily=family) +\n  draw_label(\"DataSource: Kaggle & Board Games Geek | Viz: Federica Gazzelloni\",\n             x=0.8,y=0.03,angle=0,size=11,alpha=0.5,fontfamily=family) +\n   draw_image(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/static/plot_logo.png\",x=0.09,y=-0.47,scale=0.05)\n\n\nggsave(\"w4_board_games.png\",\n        plot =final,\n        bg=\"white\",\n        dpi = 320,\n        width = 11,\n        height = 6\n       )\n\n\n\ntidygraph\nigraph\nggnet"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/Statistics/index.html",
    "href": "content/blog/posts/dataviz/posts/Statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "This is one of my favourite visualizations. It looks like very simple, and straight forward with the use of the ggdist::stat_dots function to make dotted distributions of the wages by highest educational status reached.\n\n\nWage Distribution vs Education\n\nThe Tidyverse libraries needed for the data manipulation:\n\nlibrary(tidyverse)\n\nThe data set is the Wage dataset from the {ISLR2} package. This package contains a variety of datasets used for statistical analysis in An Introduction to Statistical Learning book.\n\nlibrary(ISLR2)\ndata(Wage)\nwage_h &lt;- Wage%&gt;%group_by(education)%&gt;%summarize(avg_wage=mean(wage))\nkableExtra::kable(wage_h,row.names = F)\n\n\nA bit of data wrangling to group by education and calculate the mean value and the standard deviation of the wage.\n\nWage1 &lt;- Wage %&gt;%\n  mutate(education=gsub(\"\\\\d. \",\"\",education)) %&gt;% #count(year)\n  group_by(education)%&gt;%\n  mutate(mean=mean(wage),\n         sd=sd(wage)) %&gt;%\n  ungroup() %&gt;% # pull(mean)%&gt;%summary\n  select(education,mean,sd) %&gt;%\n  distinct()\n\n\n\nlibrary(extrafont)\n# loadfonts()\n\nFor this visualization I used: family = ‚ÄúChelsea Market‚Äù\n\n\nggdist::stat_dots to make the dots ditribution\ndistributional::dist_normal to normalize the data\n\n\nlibrary(ggdist)\nlibrary(distributional)\n\n\nWage1 %&gt;%\nggplot(aes(y=fct_reorder(education,mean),\n             xdist = dist_normal(mean, sd),\n             layout = \"weave\",\n             fill = stat(x &lt; 111.70))) + \n  stat_dots(position = \"dodge\", color = \"grey70\")+\n  geom_vline(xintercept = 111.70, alpha = 0.25) +\n  scale_x_continuous(breaks = c(20,60,90,112,140,180,220)) +\n  tvthemes::scale_fill_hilda()+\n  # add a title / subtitle and a caption ------\n  labs(x=\"Wage values from 2003 to 2009\",\n       y=\"\",color=\"Race\",fill=\"wage &lt; avg\",\n       title=\"Wage distribution vs education 2003-2009\",\n       subtitle=\"Normalized values\",\n       caption=\"#30DayChartChallenge 2022 #day9 - Distribution/Statistics - v2\\nDataSource: {ISLR2} Wage dataset | DataViz: Federica Gazzelloni\") +\n  # set a customized theme -------\n  tvthemes::theme_avatar() +\n  theme(text = element_text(family=\"Chelsea Market\"),\n        legend.background = element_blank(),\n        legend.box.background = element_blank(),\n        legend.key = element_blank(),\n        legend.key.width = unit(0.5,units=\"cm\"),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.8,0.1))\n\nIf you‚Äôd like to save it as .png you can do it with ggsave()\n\nggsave(\"day9_statistics_v2.png\",\n       dpi=320,\n       width = 9,\n       height = 6)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/Statistics/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/Statistics/index.html#overview",
    "title": "Statistics",
    "section": "",
    "text": "This is one of my favourite visualizations. It looks like very simple, and straight forward with the use of the ggdist::stat_dots function to make dotted distributions of the wages by highest educational status reached.\n\n\nWage Distribution vs Education\n\nThe Tidyverse libraries needed for the data manipulation:\n\nlibrary(tidyverse)\n\nThe data set is the Wage dataset from the {ISLR2} package. This package contains a variety of datasets used for statistical analysis in An Introduction to Statistical Learning book.\n\nlibrary(ISLR2)\ndata(Wage)\nwage_h &lt;- Wage%&gt;%group_by(education)%&gt;%summarize(avg_wage=mean(wage))\nkableExtra::kable(wage_h,row.names = F)\n\n\nA bit of data wrangling to group by education and calculate the mean value and the standard deviation of the wage.\n\nWage1 &lt;- Wage %&gt;%\n  mutate(education=gsub(\"\\\\d. \",\"\",education)) %&gt;% #count(year)\n  group_by(education)%&gt;%\n  mutate(mean=mean(wage),\n         sd=sd(wage)) %&gt;%\n  ungroup() %&gt;% # pull(mean)%&gt;%summary\n  select(education,mean,sd) %&gt;%\n  distinct()\n\n\n\nlibrary(extrafont)\n# loadfonts()\n\nFor this visualization I used: family = ‚ÄúChelsea Market‚Äù\n\n\nggdist::stat_dots to make the dots ditribution\ndistributional::dist_normal to normalize the data\n\n\nlibrary(ggdist)\nlibrary(distributional)\n\n\nWage1 %&gt;%\nggplot(aes(y=fct_reorder(education,mean),\n             xdist = dist_normal(mean, sd),\n             layout = \"weave\",\n             fill = stat(x &lt; 111.70))) + \n  stat_dots(position = \"dodge\", color = \"grey70\")+\n  geom_vline(xintercept = 111.70, alpha = 0.25) +\n  scale_x_continuous(breaks = c(20,60,90,112,140,180,220)) +\n  tvthemes::scale_fill_hilda()+\n  # add a title / subtitle and a caption ------\n  labs(x=\"Wage values from 2003 to 2009\",\n       y=\"\",color=\"Race\",fill=\"wage &lt; avg\",\n       title=\"Wage distribution vs education 2003-2009\",\n       subtitle=\"Normalized values\",\n       caption=\"#30DayChartChallenge 2022 #day9 - Distribution/Statistics - v2\\nDataSource: {ISLR2} Wage dataset | DataViz: Federica Gazzelloni\") +\n  # set a customized theme -------\n  tvthemes::theme_avatar() +\n  theme(text = element_text(family=\"Chelsea Market\"),\n        legend.background = element_blank(),\n        legend.box.background = element_blank(),\n        legend.key = element_blank(),\n        legend.key.width = unit(0.5,units=\"cm\"),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.8,0.1))\n\nIf you‚Äôd like to save it as .png you can do it with ggsave()\n\nggsave(\"day9_statistics_v2.png\",\n       dpi=320,\n       width = 9,\n       height = 6)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/Statistics/index.html#resources",
    "href": "content/blog/posts/dataviz/posts/Statistics/index.html#resources",
    "title": "Statistics",
    "section": "Resources:",
    "text": "Resources:\n\nAn Introduction to Statistical Learning"
  },
  {
    "objectID": "content/books/posts/hmsid/index.html",
    "href": "content/books/posts/hmsid/index.html",
    "title": "Health Metrics and the Spread of Infectious Diseases (HMSID)",
    "section": "",
    "text": "‚ÄúHealth Metrics and the Spread of Infectious Diseases (HMSID): Machine Learning Applications and Spatial Modelling Analysis with R‚Äù is a comprehensive resource that delves into the critical aspects of health metrics and the dynamics of infectious disease spread. This book provides valuable insights for public health professionals, epidemiologists, and researchers interested in understanding and managing infectious diseases."
  },
  {
    "objectID": "content/books/posts/hmsid/index.html#overview",
    "href": "content/books/posts/hmsid/index.html#overview",
    "title": "Health Metrics and the Spread of Infectious Diseases (HMSID)",
    "section": "",
    "text": "‚ÄúHealth Metrics and the Spread of Infectious Diseases (HMSID): Machine Learning Applications and Spatial Modelling Analysis with R‚Äù is a comprehensive resource that delves into the critical aspects of health metrics and the dynamics of infectious disease spread. This book provides valuable insights for public health professionals, epidemiologists, and researchers interested in understanding and managing infectious diseases."
  },
  {
    "objectID": "content/books/posts/hmsid/index.html#key-features",
    "href": "content/books/posts/hmsid/index.html#key-features",
    "title": "Health Metrics and the Spread of Infectious Diseases (HMSID)",
    "section": "Key Features",
    "text": "Key Features\n\nIn-depth Analysis: The book offers a thorough examination of various health metrics used to monitor and evaluate the spread of infectious diseases.\nCase Studies: Real-world case studies illustrate the application of health metrics in tracking and controlling disease outbreaks.\nData Visualization: The book emphasizes the importance of data visualization in interpreting health metrics and communicating findings effectively.\nSpatial Modelling: It explores spatial modelling techniques to analyze the geographical spread of infectious diseases.\nPractical Applications: It provides practical guidance on how to implement health metrics in public health strategies and policies."
  },
  {
    "objectID": "content/books/posts/hmsid/index.html#target-audience",
    "href": "content/books/posts/hmsid/index.html#target-audience",
    "title": "Health Metrics and the Spread of Infectious Diseases (HMSID)",
    "section": "Target Audience",
    "text": "Target Audience\nThis book is ideal for public health officials, epidemiologists, healthcare professionals, and students in the fields of public health and epidemiology who seek to enhance their understanding of health metrics and infectious disease dynamics."
  },
  {
    "objectID": "content/books/posts/hmsid/index.html#availability",
    "href": "content/books/posts/hmsid/index.html#availability",
    "title": "Health Metrics and the Spread of Infectious Diseases (HMSID)",
    "section": "Availability",
    "text": "Availability\n‚ÄúHealth Metrics and the Spread of Infectious Diseases (HMSID)‚Äù is available for purchase on major online platforms and bookstores. For more information, please visit the publisher‚Äôs website."
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html",
    "href": "content/books/posts/hmsid/posts/062025/index.html",
    "title": "Writing a book in R",
    "section": "",
    "text": "In this post, I share my experience of writing a book entirely in R, and the challenges I faced along the way. I walk through the tools I used, the workflow I followed, and the lessons I learned while preparing a full-length technical book supported by datasets, code, and reproducible examples.\nIt has been an incredible journey. Now that the book is fully published, I feel both the excitement of sharing it with readers and a sense that a long creative process has reached its conclusion.\nBelow, I explain how I structured the book, organised the code, automated the rendering, and ensured the material remained reproducible from the first draft to the final version.\n\n\n\nHMSIDr Book"
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html#overview",
    "href": "content/books/posts/hmsid/posts/062025/index.html#overview",
    "title": "Writing a book in R",
    "section": "",
    "text": "In this post, I share my experience of writing a book entirely in R, and the challenges I faced along the way. I walk through the tools I used, the workflow I followed, and the lessons I learned while preparing a full-length technical book supported by datasets, code, and reproducible examples.\nIt has been an incredible journey. Now that the book is fully published, I feel both the excitement of sharing it with readers and a sense that a long creative process has reached its conclusion.\nBelow, I explain how I structured the book, organised the code, automated the rendering, and ensured the material remained reproducible from the first draft to the final version.\n\n\n\nHMSIDr Book"
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html#setting-up-the-project",
    "href": "content/books/posts/hmsid/posts/062025/index.html#setting-up-the-project",
    "title": "Writing a book in R",
    "section": "Setting Up the Project",
    "text": "Setting Up the Project\nI started by creating a dedicated RStudio project. Keeping the book isolated in its own project was essential for reproducibility, file organisation, and version control.\nCreate a new book project structure\n\nusethis::create_project(\"hmsid-book\")\n\nSet up a Git repository to track changes:\n\n# Initialise git\n`usethis::use_git()`\n\nThen I added a basic Quarto/bookdown structure:\n\nquarto::quarto_create(\"hmsidwR\", template = \"book\")\n\nThis generated the _quarto.yml file, a structure for chapters, and a place for images and data.\nA minimal _quarto.yml looked like this:\n\n\n\n\n\n\n\nproject:\n  type: book\n\nbook:\n  title: \"Health Metrics and the Spread of Infectious Diseases\"\n  author: \"Federica Gazzelloni\"\n  chapters:\n    - index.qmd\n    - 01_introduction.qmd\n    - 02_data.qmd\n    - 03_methods.qmd\n    - 04_modeling.qmd\n    - 05_spatial_analysis.qmd\n    - 06_results.qmd\n    - 07_conclusions.qmd\n\nformat:\n  html:\n    theme: cosmo\n  pdf:\n    documentclass: scrreprt"
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html#managing-data-inside-an-r-package",
    "href": "content/books/posts/hmsid/posts/062025/index.html#managing-data-inside-an-r-package",
    "title": "Writing a book in R",
    "section": "Managing Data Inside an R Package",
    "text": "Managing Data Inside an R Package\nTo keep the workflow clean and reproducible, I organised all datasets into a companion R package (hmsidwR). This meant readers could reproduce every example with:\n\ninstall.packages(\"hmsidwR\")\nlibrary(hmsidwR)\n\nLoading a dataset was as simple as:\n\ndata(\"infectious_diseases\")\nhead(infectious_diseases)\n\nPackaging the data required a few lines in the development workflow:\n\nusethis::use_data(tb_data, overwrite = TRUE)\n\nThis approach saved hours of manual file handling in later stages of writing."
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html#writing-chapters-with-embedded-code",
    "href": "content/books/posts/hmsid/posts/062025/index.html#writing-chapters-with-embedded-code",
    "title": "Writing a book in R",
    "section": "Writing Chapters with Embedded Code",
    "text": "Writing Chapters with Embedded Code\nMost chapters combined narrative text with code chunks. A typical section looked like this:"
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html#life-expectancy-trends",
    "href": "content/books/posts/hmsid/posts/062025/index.html#life-expectancy-trends",
    "title": "Writing a book in R",
    "section": "Life Expectancy Trends",
    "text": "Life Expectancy Trends\nWe begin by exploring changes in life expectancy across countries.\n\nlibrary(tidyverse)\nlibrary(hmsidwR)\nset.seed=04122025\nhmsidwR::gho_le_hale %&gt;%\n  filter(indicator==\"Life expectancy at birth (years)\",\n         sex==\"both sexes\") %&gt;%\n  group_by(region, year) %&gt;%\n  reframe(le=mean(value, na.rm=TRUE)) %&gt;%\n  ggplot(aes(year, le, \n             colour = region)) +\n  geomtextpath::geom_textline(aes(label = region), \n                              position = \"jitter\",\n                              orientation = \"x\",\n                              hjust=0,size=3,\n                              show.legend = F) +\n  ggthemes::scale_color_economist()+\n  labs(title = \"Life Expectancy Over Time\",\n       x=\"Time(year)\",\n       y=\"Life expectancy at birth (years)\")+\n  theme_minimal(base_size = 14, \n                base_rect_size = 0.2,\n                base_line_size = 1,\n                accent = \"red\",ink = \"grey25\") +\n  theme(text=element_text(face=\"bold\"),\n        axis.title = element_text(size=9))\n\n\n\n\n\n\nFigure¬†1: Regions‚Äô Life Expectancy Over Time\n\n\nKeeping all figures generated directly from the code ensured:\n\nNo manual image editing\nFully up-to-date plots on each render\nTransparent modelling steps"
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html#version-control-and-collaboration",
    "href": "content/books/posts/hmsid/posts/062025/index.html#version-control-and-collaboration",
    "title": "Writing a book in R",
    "section": "Version Control and Collaboration",
    "text": "Version Control and Collaboration\nGit and GitHub supported the entire project:\n\nusethis::use_github()\n\nEach chapter was committed incrementally:\n  git add 03_methods.qmd\n  git commit -m \"Expand description of spatial interpolation\"\n  git push\nVersion control helped keep track of: - structure revisions - code changes - feedback from early readers - chapter reorganisations\nDesigning Figures and Reproducible Maps\nSpatial chapters required more complex workflows. For example:\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\n\nworld &lt;- vect(hmsidwR::idDALY_map_data)\ntb &lt;- hmsidwR::idDALY_map_data\n\ntm_shape(world) +\n  tm_borders(fill=\"grey90\",lwd=0.5) +\n  tm_shape(tb) +\n  tm_dots(col = \"DALYs\", \n          shape=21,lwd = 0.1,\n          palette = \"matplotlib.reds\") +\n  tm_layout(frame = FALSE)\n\n\n\nDalys\n\nEvery map in the book was generated this way, ensuring readers could run the exact same code."
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html#lessons-learned",
    "href": "content/books/posts/hmsid/posts/062025/index.html#lessons-learned",
    "title": "Writing a book in R",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nWriting a technical book is both exciting and demanding. I dedicated countless hours to selecting the right material and presenting it in a way that early-career researchers could follow. One of the most difficult decisions was choosing what to include and what to leave out, always aiming for clarity over completeness.\nEven now that the book is published, ideas continue to appear. There is always the thought of adding more material, improving explanations, or strengthening examples. At some point, you must trust the work and release it.\nI am grateful for all the support I received throughout this process. The conversations, reviews, and early feedback played a key role in shaping the final version."
  },
  {
    "objectID": "content/books/posts/hmsid/posts/062025/index.html#whats-next",
    "href": "content/books/posts/hmsid/posts/062025/index.html#whats-next",
    "title": "Writing a book in R",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nAlthough this project is complete, my writing plans certainly are not. I will continue developing extensions, datasets, and methodological notes. New projects are already emerging, and this experience has strengthened my commitment to open, reproducible, and accessible research.\nFor now, I invite you to explore the book, use the examples, and share your thoughts. Your insights will help me shape future work and teaching material.\n\n‚ÄúThank you for following this journey.‚Äù"
  },
  {
    "objectID": "content/proj/posts/hindex/index.html",
    "href": "content/proj/posts/hindex/index.html",
    "title": "Health Index Projects",
    "section": "",
    "text": "Health Index\n\n\n\nHealth\n\nHealth Index\n\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Health Performance\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/proj/posts/hindex/posts/health_index/index.html",
    "href": "content/proj/posts/hindex/posts/health_index/index.html",
    "title": "Health Index",
    "section": "",
    "text": "Process of building an index to show the dimension of health with an index. Data used is the life expectancy at birth data from the IHME GBD 2021 Global Burden of Disease Study 2021 (GBD 2021) Mortality and Life Expectancy Forecasts 2022-2050 dataset.\n\n\n\n\nsource: https://ghdx.healthdata.org/record/ihme-data/global-life-expectancy-all-cause-mortality-and-cause-specific-mortality-forecasts-2022-2050 you‚Äôll need to login for accessing the data.\n\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readxl)\n\n\nraw_le2022 &lt;- read_excel(\"data/IHME_GBD_2021_MORT_LE_FORECASTS_2022_2050_TABLES_0/le.XLSX\",skip = 1)\nhead(raw_le2022)\n\n\n\n\nfemale_le2022 &lt;- raw_le2022$`Reference scenario life expectancy at birth` %&gt;%\n  str_replace_all(\"¬∑\", \".\") %&gt;%     \n  str_extract_all(\"\\\\d+\\\\.?\\\\d*\", simplify = TRUE)\n\n\n\nlocation_name &lt;- raw_le2022$...1\n\n\n\nfemale_le2022 &lt;- data.frame(location_name,female_le2022) %&gt;%\n  select(location_name, le = X1) %&gt;%\n  mutate(le = as.numeric(le),\n         sex=\"female\") %&gt;%\n  drop_na()\n\nfemale_le2022\n\n\n\nmale_le2022 &lt;- raw_le2022$...5 %&gt;%\n  str_replace_all(\"¬∑\", \".\") %&gt;%     \n  str_extract_all(\"\\\\d+\\\\.?\\\\d*\", simplify = TRUE)\n\n\nmale_le2022 &lt;- data.frame(location_name,male_le2022) %&gt;%\n  select(location_name, le = X1) %&gt;%\n  mutate(le = as.numeric(le),\n         sex=\"male\") %&gt;%\n  drop_na()\n\nmale_le2022\n\n\n\nle2022_data &lt;- bind_rows(female_le2022,male_le2022) %&gt;%\n  group_by(location_name) %&gt;%\n  reframe(le_avg2022=round(mean(le)))%&gt;%\n  distinct()\n\nle2022_data%&gt;%head\n\n\n\nraw_le_hale2022 &lt;- read_excel(\"data/IHME_GBD_2021_MORT_LE_FORECASTS_2022_2050_TABLES_0/le_hale.XLSX\")\nhead(raw_le_hale2022)\n\n\nlocation_name &lt;- raw_le_hale2022$`Supplemental Results Table S2. Life expectancy and healthy life expectancy (HALE) in 2022 and 2050 (reference scenario) by location for both sexes. Estimates are listed as means with 95% uncertainty intervals in parentheses. Highlighted rows indicate region and super region results from the GBD location hierarchy.`\n\n\nhale2022 &lt;- raw_le_hale2022$...5%&gt;%\n  str_replace_all(\"¬∑\", \".\") %&gt;%      \n  str_extract_all(\"\\\\d+\\\\.?\\\\d*\", simplify = TRUE)\n\n\nhale2022 &lt;- data.frame(location_name,hale2022) %&gt;%\n  select(location_name, hale = X1) %&gt;%\n  mutate(hale = as.numeric(hale)) %&gt;%\n  drop_na()%&gt;%\n  distinct()\n\nhale2022\n\n\n\nyll_yld2022_raw &lt;- hmsidwR::getunz(\"https://dl.healthdata.org:443/gbd-api-2023-public/7bae287bc4f06482be6332f797f3ebc2_files/IHME-GBD_2023_DATA-7bae287b-1.zip\")\n\nyll_yld2022 &lt;- yll_yld2022_raw[[1]] %&gt;% \n  select(location_name=location,\n         measure,val)\n\n\nyld2022_data &lt;- yll_yld2022%&gt;%\n  filter(measure==\"YLDs (Years Lived with Disability)\")%&gt;%\n  rename(yld=val)%&gt;%\n  select(-measure) %&gt;%\n  distinct()\nyll2022_data &lt;- yll_yld2022%&gt;%\n  filter(measure==\"YLLs (Years of Life Lost)\")%&gt;%\n  rename(yll=val)%&gt;%\n  select(-measure) %&gt;%\n  distinct()\n  \nyll_yld2022_data&lt;- merge(yld2022_data,yll2022_data)\n\n\n\nindex_data2022 &lt;- le2022_data %&gt;%\n  left_join(hale2022, by=\"location_name\") %&gt;%\n  left_join(yll_yld2022_data ,by=\"location_name\") %&gt;%\n  drop_na()\n\nthe dimension index is calculated as:\n\\[\n\\text{dimension index} = \\frac{\\alpha\\text{le} + (1-\\alpha)\\text{hale}}{(1-\\hat{yll}) + (1-\\hat{yld})} * 100\n\\] where scaled yll and scaled yld are the standardized values of yll and yld respectively.\nIn literature a dimension index is used to measure the quality of life in a location, taking into account not only life expectancy but also the burden of disease and disability.\nIn particular, the dimension index combines life expectancy at birth (le_avg2022) and healthy life expectancy (hale) in the numerator, reflecting both the quantity and quality of life. The denominator incorporates the scaled values of years of life lost (yll) and years lived with disability (yld), which represent the burden of disease and disability in the population.\nParticular attention is put on the values of yll and yld, which are scaled to ensure comparability across different locations. The scaling process standardizes these values, allowing for a more accurate assessment of the overall health status of the population.\nfor reference see:\n\nhttps://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2025.1566469/full\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC4140376/\n\nscale between 0 and 1\n\n# scale &lt;- function(x){\n#   (x - min(x)) / (max(x) - min(x))\n# }\n# \n\n\nindex_data2022%&gt;%summary()\n\n\nindex_data2022%&gt;%\n  filter(is.na(yll) | is.na(yld))\n\n?scale\n\nindex_data2022%&gt;%\n  mutate(yll_scaled=scale(yll,center = F),\n         yld_scaled=scale(yld,center = F)) %&gt;%\n  mutate(dimension_index = round((le_avg2022 + hale)/((1-yll_scaled) + (1-yld_scaled))*100,2),\n         .after=location_name) \n\n\\[\nhealth_dim_index = (LE_scaled + HALE_scaled + (1 - YLL_scaled) + (1 - YLD_scaled)) / 4\n\\]\n\nindex_data2022 %&gt;%\n  mutate(across(where(is.numeric), ~ as.numeric(scale(.x, center = F)),\n                .names = \"{.col}_scaled\")) %&gt;%\n  mutate(dimension_index = round((le_avg2022 + hale)/((1-yll_scaled) + (1-yld_scaled))*100,2),\n         dimension_index2 = round((le_avg2022_scaled + hale_scaled + (1-yll_scaled) + (1-yld_scaled))/4,2),\n         dimension_index3 = round(((le_avg2022_scaled + hale_scaled)/ ((1-yll_scaled) + (1-yld_scaled))/2)*100,2),\n         dimension_index3_geo=((le_avg2022 + hale)/(((1-yll_scaled)+(1-yld_scaled))/2))^(1/4) *100,\n         .after=location_name) %&gt;%\n  filter(is.na(dimension_index3_geo))"
  },
  {
    "objectID": "content/proj/posts/hindex/posts/health_index/index.html#overview",
    "href": "content/proj/posts/hindex/posts/health_index/index.html#overview",
    "title": "Health Index",
    "section": "",
    "text": "Process of building an index to show the dimension of health with an index. Data used is the life expectancy at birth data from the IHME GBD 2021 Global Burden of Disease Study 2021 (GBD 2021) Mortality and Life Expectancy Forecasts 2022-2050 dataset.\n\n\n\n\nsource: https://ghdx.healthdata.org/record/ihme-data/global-life-expectancy-all-cause-mortality-and-cause-specific-mortality-forecasts-2022-2050 you‚Äôll need to login for accessing the data.\n\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readxl)\n\n\nraw_le2022 &lt;- read_excel(\"data/IHME_GBD_2021_MORT_LE_FORECASTS_2022_2050_TABLES_0/le.XLSX\",skip = 1)\nhead(raw_le2022)\n\n\n\n\nfemale_le2022 &lt;- raw_le2022$`Reference scenario life expectancy at birth` %&gt;%\n  str_replace_all(\"¬∑\", \".\") %&gt;%     \n  str_extract_all(\"\\\\d+\\\\.?\\\\d*\", simplify = TRUE)\n\n\n\nlocation_name &lt;- raw_le2022$...1\n\n\n\nfemale_le2022 &lt;- data.frame(location_name,female_le2022) %&gt;%\n  select(location_name, le = X1) %&gt;%\n  mutate(le = as.numeric(le),\n         sex=\"female\") %&gt;%\n  drop_na()\n\nfemale_le2022\n\n\n\nmale_le2022 &lt;- raw_le2022$...5 %&gt;%\n  str_replace_all(\"¬∑\", \".\") %&gt;%     \n  str_extract_all(\"\\\\d+\\\\.?\\\\d*\", simplify = TRUE)\n\n\nmale_le2022 &lt;- data.frame(location_name,male_le2022) %&gt;%\n  select(location_name, le = X1) %&gt;%\n  mutate(le = as.numeric(le),\n         sex=\"male\") %&gt;%\n  drop_na()\n\nmale_le2022\n\n\n\nle2022_data &lt;- bind_rows(female_le2022,male_le2022) %&gt;%\n  group_by(location_name) %&gt;%\n  reframe(le_avg2022=round(mean(le)))%&gt;%\n  distinct()\n\nle2022_data%&gt;%head\n\n\n\nraw_le_hale2022 &lt;- read_excel(\"data/IHME_GBD_2021_MORT_LE_FORECASTS_2022_2050_TABLES_0/le_hale.XLSX\")\nhead(raw_le_hale2022)\n\n\nlocation_name &lt;- raw_le_hale2022$`Supplemental Results Table S2. Life expectancy and healthy life expectancy (HALE) in 2022 and 2050 (reference scenario) by location for both sexes. Estimates are listed as means with 95% uncertainty intervals in parentheses. Highlighted rows indicate region and super region results from the GBD location hierarchy.`\n\n\nhale2022 &lt;- raw_le_hale2022$...5%&gt;%\n  str_replace_all(\"¬∑\", \".\") %&gt;%      \n  str_extract_all(\"\\\\d+\\\\.?\\\\d*\", simplify = TRUE)\n\n\nhale2022 &lt;- data.frame(location_name,hale2022) %&gt;%\n  select(location_name, hale = X1) %&gt;%\n  mutate(hale = as.numeric(hale)) %&gt;%\n  drop_na()%&gt;%\n  distinct()\n\nhale2022\n\n\n\nyll_yld2022_raw &lt;- hmsidwR::getunz(\"https://dl.healthdata.org:443/gbd-api-2023-public/7bae287bc4f06482be6332f797f3ebc2_files/IHME-GBD_2023_DATA-7bae287b-1.zip\")\n\nyll_yld2022 &lt;- yll_yld2022_raw[[1]] %&gt;% \n  select(location_name=location,\n         measure,val)\n\n\nyld2022_data &lt;- yll_yld2022%&gt;%\n  filter(measure==\"YLDs (Years Lived with Disability)\")%&gt;%\n  rename(yld=val)%&gt;%\n  select(-measure) %&gt;%\n  distinct()\nyll2022_data &lt;- yll_yld2022%&gt;%\n  filter(measure==\"YLLs (Years of Life Lost)\")%&gt;%\n  rename(yll=val)%&gt;%\n  select(-measure) %&gt;%\n  distinct()\n  \nyll_yld2022_data&lt;- merge(yld2022_data,yll2022_data)\n\n\n\nindex_data2022 &lt;- le2022_data %&gt;%\n  left_join(hale2022, by=\"location_name\") %&gt;%\n  left_join(yll_yld2022_data ,by=\"location_name\") %&gt;%\n  drop_na()\n\nthe dimension index is calculated as:\n\\[\n\\text{dimension index} = \\frac{\\alpha\\text{le} + (1-\\alpha)\\text{hale}}{(1-\\hat{yll}) + (1-\\hat{yld})} * 100\n\\] where scaled yll and scaled yld are the standardized values of yll and yld respectively.\nIn literature a dimension index is used to measure the quality of life in a location, taking into account not only life expectancy but also the burden of disease and disability.\nIn particular, the dimension index combines life expectancy at birth (le_avg2022) and healthy life expectancy (hale) in the numerator, reflecting both the quantity and quality of life. The denominator incorporates the scaled values of years of life lost (yll) and years lived with disability (yld), which represent the burden of disease and disability in the population.\nParticular attention is put on the values of yll and yld, which are scaled to ensure comparability across different locations. The scaling process standardizes these values, allowing for a more accurate assessment of the overall health status of the population.\nfor reference see:\n\nhttps://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2025.1566469/full\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC4140376/\n\nscale between 0 and 1\n\n# scale &lt;- function(x){\n#   (x - min(x)) / (max(x) - min(x))\n# }\n# \n\n\nindex_data2022%&gt;%summary()\n\n\nindex_data2022%&gt;%\n  filter(is.na(yll) | is.na(yld))\n\n?scale\n\nindex_data2022%&gt;%\n  mutate(yll_scaled=scale(yll,center = F),\n         yld_scaled=scale(yld,center = F)) %&gt;%\n  mutate(dimension_index = round((le_avg2022 + hale)/((1-yll_scaled) + (1-yld_scaled))*100,2),\n         .after=location_name) \n\n\\[\nhealth_dim_index = (LE_scaled + HALE_scaled + (1 - YLL_scaled) + (1 - YLD_scaled)) / 4\n\\]\n\nindex_data2022 %&gt;%\n  mutate(across(where(is.numeric), ~ as.numeric(scale(.x, center = F)),\n                .names = \"{.col}_scaled\")) %&gt;%\n  mutate(dimension_index = round((le_avg2022 + hale)/((1-yll_scaled) + (1-yld_scaled))*100,2),\n         dimension_index2 = round((le_avg2022_scaled + hale_scaled + (1-yll_scaled) + (1-yld_scaled))/4,2),\n         dimension_index3 = round(((le_avg2022_scaled + hale_scaled)/ ((1-yll_scaled) + (1-yld_scaled))/2)*100,2),\n         dimension_index3_geo=((le_avg2022 + hale)/(((1-yll_scaled)+(1-yld_scaled))/2))^(1/4) *100,\n         .after=location_name) %&gt;%\n  filter(is.na(dimension_index3_geo))"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2026 fgazzelloni.github.io authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html",
    "href": "IMPLEMENTATION_SUMMARY.html",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "This implementation provides a complete automated system for fetching podcast episodes from an RSS feed and creating properly formatted Quarto blog posts.\n\n\n\n.github/workflows/fetch-podcasts.yml\n\nGitHub Actions workflow for automation\nRuns daily at midnight UTC\nCan be manually triggered via workflow_dispatch\nInstalls R and required packages (httr, xml2, stringr, glue, fs)\nExecutes the fetch script without requiring credentials\nCommits and pushes new episodes automatically\nSecurity: Includes explicit permissions (contents: write)\nSimplified: No API credentials needed\n\nscripts/fetch-podcasts.R\n\nComplete R script for fetching and processing episodes from RSS feed\nFeatures:\n\nRSS feed parsing using xml2 package\nFetches all episodes from RSS feed at https://anchor.fm/s/10dab65b8/podcast/rss\nDuplicate detection (checks existing folders)\nURL-friendly slug generation with validation\nAutomatic category extraction from descriptions\nKey topics extraction from episode descriptions\nEpisode cover image downloading from RSS iTunes image or media thumbnail\nComprehensive error handling and logging\n\nQuality improvements:\n\nNo authentication required\nDirect RSS parsing without API rate limits\nExtracts episode IDs from Spotify URLs when available in RSS\nFalls back to show-level embeds if episode ID not available\n\n\nscripts/README.md\n\nUpdated documentation including:\n\nRSS feed configuration instructions\nRemoved all Spotify API setup steps\nRemoved GitHub Secrets configuration\nHow the RSS-based system works\nFile structure explanation\nExample post format\nTroubleshooting guide for RSS feeds\nLocal testing instructions\nCustomization options\n\n\nIMPLEMENTATION_SUMMARY.md (this file)\n\nUpdated to reflect RSS-based implementation\nRemoved Spotify API references\nUpdated features and benefits\n\n\n\n\n\n\n\n\n\nDaily scheduled runs at midnight UTC\nManual triggering via GitHub Actions UI\nAutomatic commits and pushes of new episodes\nSmart duplicate detection prevents re-adding episodes\nNo authentication required - uses public RSS feed\n\n\n\n\n\nParses standard RSS 2.0 feed with full metadata extraction\nMatches existing format exactly (see content/podcasts/posts/hmsidr/)\nGenerates proper YAML frontmatter with all required fields\nCreates Spotify embeds (episode-specific when available, otherwise show-level)\nDownloads cover images from RSS feed (iTunes or media thumbnail)\nExtracts categories from episode descriptions\nGenerates key topics automatically\n\n\n\n\n\n‚úÖ Simplified workflow with no credentials needed\n‚úÖ More reliable (no API rate limits or authentication issues)\n‚úÖ Standard RSS parsing using xml2 package\n‚úÖ Comprehensive error handling\n‚úÖ Detailed logging for debugging\n\n\n\n\n\n\n\nThe script is configured to use:\nhttps://anchor.fm/s/10dab65b8/podcast/rss\nIf this URL needs to be changed: 1. Edit scripts/fetch-podcasts.R 2. Update the RSS_FEED_URL variable 3. Commit the change\n\n\n\nOption A: Wait for automatic run (next midnight UTC) Option B: Manually trigger: 1. Go to Actions tab 2. Select ‚ÄúFetch RSS Podcasts‚Äù 3. Click ‚ÄúRun workflow‚Äù\n\n\n\nAfter running: 1. Check Actions logs for any errors 2. Look for new folders in content/podcasts/posts/ 3. Build the site to see new episodes listed 4. Verify episode pages render correctly\n\n\n\n\nWhen the workflow runs successfully, you‚Äôll see:\ncontent/podcasts/posts/\n‚îú‚îÄ‚îÄ hmsidr/                    (existing)\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd\n‚îÇ   ‚îî‚îÄ‚îÄ featured.png\n‚îú‚îÄ‚îÄ new-episode-title/         (new)\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd\n‚îÇ   ‚îî‚îÄ‚îÄ featured.png\n‚îî‚îÄ‚îÄ another-episode/           (new)\n    ‚îú‚îÄ‚îÄ index.qmd\n    ‚îî‚îÄ‚îÄ featured.png\nEach index.qmd will contain: - YAML frontmatter matching the hmsidr format - Episode-specific Spotify embed - Episode overview section - Key topics discussed section\n\n\n\nAll customizable settings are documented in scripts/README.md, including: - RSS feed URL (to use a different podcast feed) - Show ID (for Spotify embeds) - Categories extraction logic - Embed iframe styling - File naming conventions\n\n\n\n\nSimplified Setup: No API credentials or authentication needed\nMore Reliable: No API rate limits or authentication expiration issues\nTime Savings: No manual episode post creation needed\nConsistency: All posts follow the same format\nUp-to-date: Automatically checks for new episodes daily\nMaintainable: Clear documentation and error handling\nPortable: Works with any podcast RSS feed, not just Spotify\n\n\n\n\nIf issues arise: 1. Check GitHub Actions logs (Actions tab) 2. Verify RSS feed URL is correct and publicly accessible 3. Test RSS feed in an online validator 4. Review scripts/README.md troubleshooting section 5. Test locally by running: Rscript scripts/fetch-podcasts.R\n\n\n\n\nThe Quarto listing on content/podcasts/index.qmd will automatically show new episodes\nEpisodes are identified by slug, not by ID, to prevent duplicates\nImages are optional - posts will be created even if image download fails\nThe workflow uses R 4.3.x for automatic patch updates while maintaining compatibility\nRSS feeds are more stable and don‚Äôt require maintaining API credentials"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html#what-was-implemented",
    "href": "IMPLEMENTATION_SUMMARY.html#what-was-implemented",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "This implementation provides a complete automated system for fetching podcast episodes from an RSS feed and creating properly formatted Quarto blog posts.\n\n\n\n.github/workflows/fetch-podcasts.yml\n\nGitHub Actions workflow for automation\nRuns daily at midnight UTC\nCan be manually triggered via workflow_dispatch\nInstalls R and required packages (httr, xml2, stringr, glue, fs)\nExecutes the fetch script without requiring credentials\nCommits and pushes new episodes automatically\nSecurity: Includes explicit permissions (contents: write)\nSimplified: No API credentials needed\n\nscripts/fetch-podcasts.R\n\nComplete R script for fetching and processing episodes from RSS feed\nFeatures:\n\nRSS feed parsing using xml2 package\nFetches all episodes from RSS feed at https://anchor.fm/s/10dab65b8/podcast/rss\nDuplicate detection (checks existing folders)\nURL-friendly slug generation with validation\nAutomatic category extraction from descriptions\nKey topics extraction from episode descriptions\nEpisode cover image downloading from RSS iTunes image or media thumbnail\nComprehensive error handling and logging\n\nQuality improvements:\n\nNo authentication required\nDirect RSS parsing without API rate limits\nExtracts episode IDs from Spotify URLs when available in RSS\nFalls back to show-level embeds if episode ID not available\n\n\nscripts/README.md\n\nUpdated documentation including:\n\nRSS feed configuration instructions\nRemoved all Spotify API setup steps\nRemoved GitHub Secrets configuration\nHow the RSS-based system works\nFile structure explanation\nExample post format\nTroubleshooting guide for RSS feeds\nLocal testing instructions\nCustomization options\n\n\nIMPLEMENTATION_SUMMARY.md (this file)\n\nUpdated to reflect RSS-based implementation\nRemoved Spotify API references\nUpdated features and benefits"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html#features",
    "href": "IMPLEMENTATION_SUMMARY.html#features",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "Daily scheduled runs at midnight UTC\nManual triggering via GitHub Actions UI\nAutomatic commits and pushes of new episodes\nSmart duplicate detection prevents re-adding episodes\nNo authentication required - uses public RSS feed\n\n\n\n\n\nParses standard RSS 2.0 feed with full metadata extraction\nMatches existing format exactly (see content/podcasts/posts/hmsidr/)\nGenerates proper YAML frontmatter with all required fields\nCreates Spotify embeds (episode-specific when available, otherwise show-level)\nDownloads cover images from RSS feed (iTunes or media thumbnail)\nExtracts categories from episode descriptions\nGenerates key topics automatically\n\n\n\n\n\n‚úÖ Simplified workflow with no credentials needed\n‚úÖ More reliable (no API rate limits or authentication issues)\n‚úÖ Standard RSS parsing using xml2 package\n‚úÖ Comprehensive error handling\n‚úÖ Detailed logging for debugging"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html#next-steps-for-repository-owner",
    "href": "IMPLEMENTATION_SUMMARY.html#next-steps-for-repository-owner",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "The script is configured to use:\nhttps://anchor.fm/s/10dab65b8/podcast/rss\nIf this URL needs to be changed: 1. Edit scripts/fetch-podcasts.R 2. Update the RSS_FEED_URL variable 3. Commit the change\n\n\n\nOption A: Wait for automatic run (next midnight UTC) Option B: Manually trigger: 1. Go to Actions tab 2. Select ‚ÄúFetch RSS Podcasts‚Äù 3. Click ‚ÄúRun workflow‚Äù\n\n\n\nAfter running: 1. Check Actions logs for any errors 2. Look for new folders in content/podcasts/posts/ 3. Build the site to see new episodes listed 4. Verify episode pages render correctly"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html#expected-output",
    "href": "IMPLEMENTATION_SUMMARY.html#expected-output",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "When the workflow runs successfully, you‚Äôll see:\ncontent/podcasts/posts/\n‚îú‚îÄ‚îÄ hmsidr/                    (existing)\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd\n‚îÇ   ‚îî‚îÄ‚îÄ featured.png\n‚îú‚îÄ‚îÄ new-episode-title/         (new)\n‚îÇ   ‚îú‚îÄ‚îÄ index.qmd\n‚îÇ   ‚îî‚îÄ‚îÄ featured.png\n‚îî‚îÄ‚îÄ another-episode/           (new)\n    ‚îú‚îÄ‚îÄ index.qmd\n    ‚îî‚îÄ‚îÄ featured.png\nEach index.qmd will contain: - YAML frontmatter matching the hmsidr format - Episode-specific Spotify embed - Episode overview section - Key topics discussed section"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html#customization",
    "href": "IMPLEMENTATION_SUMMARY.html#customization",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "All customizable settings are documented in scripts/README.md, including: - RSS feed URL (to use a different podcast feed) - Show ID (for Spotify embeds) - Categories extraction logic - Embed iframe styling - File naming conventions"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html#benefits",
    "href": "IMPLEMENTATION_SUMMARY.html#benefits",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "Simplified Setup: No API credentials or authentication needed\nMore Reliable: No API rate limits or authentication expiration issues\nTime Savings: No manual episode post creation needed\nConsistency: All posts follow the same format\nUp-to-date: Automatically checks for new episodes daily\nMaintainable: Clear documentation and error handling\nPortable: Works with any podcast RSS feed, not just Spotify"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html#troubleshooting",
    "href": "IMPLEMENTATION_SUMMARY.html#troubleshooting",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "If issues arise: 1. Check GitHub Actions logs (Actions tab) 2. Verify RSS feed URL is correct and publicly accessible 3. Test RSS feed in an online validator 4. Review scripts/README.md troubleshooting section 5. Test locally by running: Rscript scripts/fetch-podcasts.R"
  },
  {
    "objectID": "IMPLEMENTATION_SUMMARY.html#notes",
    "href": "IMPLEMENTATION_SUMMARY.html#notes",
    "title": "Implementation Summary: Automated RSS Podcast Fetching",
    "section": "",
    "text": "The Quarto listing on content/podcasts/index.qmd will automatically show new episodes\nEpisodes are identified by slug, not by ID, to prevent duplicates\nImages are optional - posts will be created even if image download fails\nThe workflow uses R 4.3.x for automatic patch updates while maintaining compatibility\nRSS feeds are more stable and don‚Äôt require maintaining API credentials"
  },
  {
    "objectID": "content/proj/posts/hindex/posts/performance/index.html",
    "href": "content/proj/posts/hindex/posts/performance/index.html",
    "title": "Measuring Health Performance",
    "section": "",
    "text": "These days measuring performance is very appropriate for many different topics. Thinking about health and the fast changing environments, including climate changes, require a ready tool for identifying possible future outcomes. On health, interesting simple metrics are used to classify the state of health of a population, so to be comparable with other near and far.\nHere is a spec of my latest project where I am collecting all that I learned since the start of the Covid19 pandemic on a summary of the techniques used for measuring the health status of a population when in conjunction with an extreme event. Many tools are available and ready to use for the most exceptional purpose someone might had in mind, and I had difficulties choosing one on top of the other. But, why choosing if you can combine them?\n\n\n\n\nThere are three metrics that are used for the purpose of classification in the public health, the DALYs, YLLs, and the YLDs. Respectively are the Disability Adjusted Life Years, Years of Life Lost, and Years Lived with Disabilities.\nBefore going into the calculation detail, the definition of good health and well being is required.\nThe WHO constitution states:\n\n‚ÄúHealth is a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity.‚Äù\n\nAn important implication of this definition is that mental health is more than just the absence of mental disorders or disabilities.\nLet‚Äôs load the {hmsidwR} package, still at its very early stages; a development version can be installed from GitHub:\n\ndevtools::install_github(\"Fgazzelloni/hmsidwR\")\n\n\nlibrary(tidyverse)\nlibrary(hmsidwR)\n\nIt contains some interesting datasets:\nThe Global life tables: Glifetables. A dataset provided by the World Health Organization (WHO). Global Health Observatory data repository\n\n?hmsidwR\n\nAnd the Germany lung cancer: Germany_lungc. A dataset provided by the Institute for Health Metrics and Evaluation (IHME). GBD Results\n\nhmsidwR::germany_lungc %&gt;% head\n\n# A tibble: 6 √ó 8\n  age   sex    prevalence prev_upper prev_lower    dx dx_uppe dx_lower\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 10-14 male         0.08       0.13       0.05 0.322   0.461    0.217\n2 10-14 female       0.18       0.32       0.09 0.457   0.761    0.248\n3 10-14 both         0.13       0.22       0.07 0.779   1.21     0.468\n4 15-19 male         0.48       0.77       0.29 1.27    1.75     0.916\n5 15-19 female       0.9        1.52       0.5  1.56    2.46     0.941\n6 15-19 both         0.68       1.02       0.44 2.83    3.88     2.07 \n\n\n\ngermany_lungc %&gt;%\n  ggplot(aes(age, dx, fill = sex)) +\n  geom_col() +\n  facet_wrap( ~ sex) +\n  scale_x_discrete(breaks = c(\"35-39\", \"65-69\", \"85+\")) +\n  ggthemes::scale_fill_fivethirtyeight() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"Germany lung cancer -2019\",\n       caption = \"Vis: fgazzelloni|DataSource: IHME\")\n\n\n\n\n\n\n\nThe combination of this to piece of information, the life expectancy and the expected value of lung cancer cases, in proportion of the Germany population, are combined by age class and divided by sex to obtain the YLLs, the numbers of years of life lost.\nIn this case for Germany data is available from the age class 10-14, if we would like to improve this analysis it required some missing value imputation, through data feature engineering.\n\nyll &lt;- germany_lungc %&gt;%\n  full_join(\n    gho_lifetables %&gt;%\n      filter(year == 2019,\n             indicator == \"ex\") %&gt;%\n      rename(life_expectancy = value),\n    by = c(\"age\", \"sex\")\n  ) %&gt;%\n  group_by(age, sex) %&gt;%\n  reframe(yll = dx * life_expectancy) %&gt;%\n  filter(!is.na(yll))\n\nyll %&gt;%\n  head()\n\n# A tibble: 6 √ó 3\n  age   sex      yll\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 10-14 both    51.7\n2 10-14 female  31.5\n3 10-14 male    20.6\n4 15-19 both   174. \n5 15-19 female  99.7\n6 15-19 male    75.4\n\n\n\nyll %&gt;%\n  ggplot(aes(age, yll, fill = sex)) +\n  geom_col() +\n  facet_wrap( ~ sex) +\n  scale_x_discrete(breaks = c(\"35-39\", \"65-69\", \"85+\")) +\n  ggthemes::scale_fill_fivethirtyeight() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"YLL - Germany lung cancer -2019\",\n       caption = \"Vis: fgazzelloni|DataSource: IHME & WHO\")\n\n\n\n\n\n\n\nTo build the YLDs, the numbers of years lived with a disability due to a disease or injury, we need more data: the prevalence, and the disability weights.\nsource:\n\ndisability weights\n\nThen, the sum of the YLL and the YLD provides the overall value of the DALY which is the key metric used to state the health of a population, and it is used to make comparisons among population of different countries, as well as begin used to provide a comprehensive assessment of the impact of disease and injury on a population, and help prioritize public health interventions and evaluate the effectiveness of public health programs.\nLet‚Äôs now have a look at how infectious diseases can affect the DALYs. The COVID-19 pandemic has had a significant impact on DALYs metrics worldwide.\nStill results are not fully available, but several of the risk factors and non-communicable diseases (NCDs) highlighted by the GBD study, including obesity, diabetes, and cardiovascular disease, are associated with increased risk of serious illness and death from COVID-19, and so, as a consequence linked with an increase of the overall level of DALYs. See The Lancet: Latest global disease estimates reveal perfect storm of rising chronic diseases and public health failures fuelling COVID-19 pandemic\nCOVID-19 is expected to show clearly that it has been the leading cause of global DALYs in 2020.\nMore information on the level of findings are in this interesting article: https://doi.org/10.1016/S0140-6736(20)30925-9 on the Lancet by the GBD collaborator team.\n\nLooking at the Global impact of some infectious diseases, such as: Ebola.\nThe impact of Ebola on DALYs in 2019 can be assessed by comparing the number of DALYs due to Ebola in 2019 to the DALYs caused by other diseases or conditions during the same period.\nAccording to the Global Health Data Exchange (GHDx), the estimated global DALY rate for Ebola virus disease in 2019 was 0.0005, which is relatively low compared to other leading causes of DALYs, such as cardiovascular diseases, lower respiratory infections, and neonatal disorders. See IHME Ebola ‚Äî Level 3 cause\nImport data on global burden of disease (GBD) for a given year, here I already downloaded the cvs file and save it as RData.\n\n# Subset data to only include \ndf_dalys_2019 &lt;- df_gbd_2019 %&gt;%\n  filter(location_name == \"Global\",\n         sex_name == \"Both\") %&gt;%\n  select(!contains(\"_id\"))\n\n\ndf_dalys_2019 %&gt;% count(age_name, val)\n\n# A tibble: 19 √ó 3\n   age_name        val     n\n   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;\n 1 1-4 years    24933.     1\n 2 10-14 years   8994.     1\n 3 15-19 years  12520.     1\n 4 20-24 years  15651.     1\n 5 25-29 years  17288.     1\n 6 30-34 years  19457.     1\n 7 35-39 years  22626.     1\n 8 40-44 years  26218.     1\n 9 45-49 years  30105.     1\n10 5-9 years     9136.     1\n11 50-54 years  36694.     1\n12 55-59 years  45169.     1\n13 60-64 years  55901.     1\n14 65-69 years  67373.     1\n15 70-74 years  83145.     1\n16 75-79 years  99766.     1\n17 80-84       123203.     1\n18 85+ years   151604.     1\n19 &lt;1 year     256548.     1\n\n\nLet‚Äôs have a look at the DALYs rates and consider the average value by 5 years range, then calculate the total DALYs for all ages in 2019.\n\navg_dalys_2019 &lt;- mean(df_dalys_2019$val)\navg_dalys_2019\n\n[1] 58227.96\n\n\n\ntotal_dalys_2019 &lt;- sum(df_dalys_2019$val)\ntotal_dalys_2019\n\n[1] 1106331\n\n\nImport data on infectious diseases, and select Global, Ebola, both sex.\n\nebola_global_2019 &lt;- infectious_diseases %&gt;%\n  filter(location_name == \"Global\",\n         sex_name == \"Both\",\n         cause_name == \"Ebola\") %&gt;%\n  select(!contains(\"_id\"))\n\n# Calculate total COVID-19 DALYs for 2019\ntotal_ebola_global_2019 &lt;-\n  sum(ebola_global_2019$val)\n\n# Calculate the percentage change in DALYs due to COVID-19\npercent_change_dalys &lt;- round((total_ebola_global_2019 / total_dalys_2019) *100,4)\n\n# Print the percentage change in DALYs due to COVID-19\ncat(\"Total impact of Ebola virus Globally on DALYs rates in 2019:\", percent_change_dalys, \"%\")\n\nTotal impact of Ebola virus Globally on DALYs rates in 2019: 0.0037 %\n\n\nHowever, the impact of Ebola on DALYs is more significant in certain African regions, it accounts for the whole population. For example, during the 2014-2016, the Ebola outbreak in West Africa caused an estimated 11,000 deaths and 261,000 DALYs lost.\nOverall, while the global impact of Ebola on DALYs in 2019 was relatively low, it is still an important health concern in areas where outbreaks occur, and efforts to prevent and control the disease are crucial to reducing its impact on affected populations."
  },
  {
    "objectID": "content/proj/posts/hindex/posts/performance/index.html#overview",
    "href": "content/proj/posts/hindex/posts/performance/index.html#overview",
    "title": "Measuring Health Performance",
    "section": "",
    "text": "These days measuring performance is very appropriate for many different topics. Thinking about health and the fast changing environments, including climate changes, require a ready tool for identifying possible future outcomes. On health, interesting simple metrics are used to classify the state of health of a population, so to be comparable with other near and far.\nHere is a spec of my latest project where I am collecting all that I learned since the start of the Covid19 pandemic on a summary of the techniques used for measuring the health status of a population when in conjunction with an extreme event. Many tools are available and ready to use for the most exceptional purpose someone might had in mind, and I had difficulties choosing one on top of the other. But, why choosing if you can combine them?\n\n\n\n\nThere are three metrics that are used for the purpose of classification in the public health, the DALYs, YLLs, and the YLDs. Respectively are the Disability Adjusted Life Years, Years of Life Lost, and Years Lived with Disabilities.\nBefore going into the calculation detail, the definition of good health and well being is required.\nThe WHO constitution states:\n\n‚ÄúHealth is a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity.‚Äù\n\nAn important implication of this definition is that mental health is more than just the absence of mental disorders or disabilities.\nLet‚Äôs load the {hmsidwR} package, still at its very early stages; a development version can be installed from GitHub:\n\ndevtools::install_github(\"Fgazzelloni/hmsidwR\")\n\n\nlibrary(tidyverse)\nlibrary(hmsidwR)\n\nIt contains some interesting datasets:\nThe Global life tables: Glifetables. A dataset provided by the World Health Organization (WHO). Global Health Observatory data repository\n\n?hmsidwR\n\nAnd the Germany lung cancer: Germany_lungc. A dataset provided by the Institute for Health Metrics and Evaluation (IHME). GBD Results\n\nhmsidwR::germany_lungc %&gt;% head\n\n# A tibble: 6 √ó 8\n  age   sex    prevalence prev_upper prev_lower    dx dx_uppe dx_lower\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 10-14 male         0.08       0.13       0.05 0.322   0.461    0.217\n2 10-14 female       0.18       0.32       0.09 0.457   0.761    0.248\n3 10-14 both         0.13       0.22       0.07 0.779   1.21     0.468\n4 15-19 male         0.48       0.77       0.29 1.27    1.75     0.916\n5 15-19 female       0.9        1.52       0.5  1.56    2.46     0.941\n6 15-19 both         0.68       1.02       0.44 2.83    3.88     2.07 \n\n\n\ngermany_lungc %&gt;%\n  ggplot(aes(age, dx, fill = sex)) +\n  geom_col() +\n  facet_wrap( ~ sex) +\n  scale_x_discrete(breaks = c(\"35-39\", \"65-69\", \"85+\")) +\n  ggthemes::scale_fill_fivethirtyeight() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"Germany lung cancer -2019\",\n       caption = \"Vis: fgazzelloni|DataSource: IHME\")\n\n\n\n\n\n\n\nThe combination of this to piece of information, the life expectancy and the expected value of lung cancer cases, in proportion of the Germany population, are combined by age class and divided by sex to obtain the YLLs, the numbers of years of life lost.\nIn this case for Germany data is available from the age class 10-14, if we would like to improve this analysis it required some missing value imputation, through data feature engineering.\n\nyll &lt;- germany_lungc %&gt;%\n  full_join(\n    gho_lifetables %&gt;%\n      filter(year == 2019,\n             indicator == \"ex\") %&gt;%\n      rename(life_expectancy = value),\n    by = c(\"age\", \"sex\")\n  ) %&gt;%\n  group_by(age, sex) %&gt;%\n  reframe(yll = dx * life_expectancy) %&gt;%\n  filter(!is.na(yll))\n\nyll %&gt;%\n  head()\n\n# A tibble: 6 √ó 3\n  age   sex      yll\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 10-14 both    51.7\n2 10-14 female  31.5\n3 10-14 male    20.6\n4 15-19 both   174. \n5 15-19 female  99.7\n6 15-19 male    75.4\n\n\n\nyll %&gt;%\n  ggplot(aes(age, yll, fill = sex)) +\n  geom_col() +\n  facet_wrap( ~ sex) +\n  scale_x_discrete(breaks = c(\"35-39\", \"65-69\", \"85+\")) +\n  ggthemes::scale_fill_fivethirtyeight() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"YLL - Germany lung cancer -2019\",\n       caption = \"Vis: fgazzelloni|DataSource: IHME & WHO\")\n\n\n\n\n\n\n\nTo build the YLDs, the numbers of years lived with a disability due to a disease or injury, we need more data: the prevalence, and the disability weights.\nsource:\n\ndisability weights\n\nThen, the sum of the YLL and the YLD provides the overall value of the DALY which is the key metric used to state the health of a population, and it is used to make comparisons among population of different countries, as well as begin used to provide a comprehensive assessment of the impact of disease and injury on a population, and help prioritize public health interventions and evaluate the effectiveness of public health programs.\nLet‚Äôs now have a look at how infectious diseases can affect the DALYs. The COVID-19 pandemic has had a significant impact on DALYs metrics worldwide.\nStill results are not fully available, but several of the risk factors and non-communicable diseases (NCDs) highlighted by the GBD study, including obesity, diabetes, and cardiovascular disease, are associated with increased risk of serious illness and death from COVID-19, and so, as a consequence linked with an increase of the overall level of DALYs. See The Lancet: Latest global disease estimates reveal perfect storm of rising chronic diseases and public health failures fuelling COVID-19 pandemic\nCOVID-19 is expected to show clearly that it has been the leading cause of global DALYs in 2020.\nMore information on the level of findings are in this interesting article: https://doi.org/10.1016/S0140-6736(20)30925-9 on the Lancet by the GBD collaborator team.\n\nLooking at the Global impact of some infectious diseases, such as: Ebola.\nThe impact of Ebola on DALYs in 2019 can be assessed by comparing the number of DALYs due to Ebola in 2019 to the DALYs caused by other diseases or conditions during the same period.\nAccording to the Global Health Data Exchange (GHDx), the estimated global DALY rate for Ebola virus disease in 2019 was 0.0005, which is relatively low compared to other leading causes of DALYs, such as cardiovascular diseases, lower respiratory infections, and neonatal disorders. See IHME Ebola ‚Äî Level 3 cause\nImport data on global burden of disease (GBD) for a given year, here I already downloaded the cvs file and save it as RData.\n\n# Subset data to only include \ndf_dalys_2019 &lt;- df_gbd_2019 %&gt;%\n  filter(location_name == \"Global\",\n         sex_name == \"Both\") %&gt;%\n  select(!contains(\"_id\"))\n\n\ndf_dalys_2019 %&gt;% count(age_name, val)\n\n# A tibble: 19 √ó 3\n   age_name        val     n\n   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;\n 1 1-4 years    24933.     1\n 2 10-14 years   8994.     1\n 3 15-19 years  12520.     1\n 4 20-24 years  15651.     1\n 5 25-29 years  17288.     1\n 6 30-34 years  19457.     1\n 7 35-39 years  22626.     1\n 8 40-44 years  26218.     1\n 9 45-49 years  30105.     1\n10 5-9 years     9136.     1\n11 50-54 years  36694.     1\n12 55-59 years  45169.     1\n13 60-64 years  55901.     1\n14 65-69 years  67373.     1\n15 70-74 years  83145.     1\n16 75-79 years  99766.     1\n17 80-84       123203.     1\n18 85+ years   151604.     1\n19 &lt;1 year     256548.     1\n\n\nLet‚Äôs have a look at the DALYs rates and consider the average value by 5 years range, then calculate the total DALYs for all ages in 2019.\n\navg_dalys_2019 &lt;- mean(df_dalys_2019$val)\navg_dalys_2019\n\n[1] 58227.96\n\n\n\ntotal_dalys_2019 &lt;- sum(df_dalys_2019$val)\ntotal_dalys_2019\n\n[1] 1106331\n\n\nImport data on infectious diseases, and select Global, Ebola, both sex.\n\nebola_global_2019 &lt;- infectious_diseases %&gt;%\n  filter(location_name == \"Global\",\n         sex_name == \"Both\",\n         cause_name == \"Ebola\") %&gt;%\n  select(!contains(\"_id\"))\n\n# Calculate total COVID-19 DALYs for 2019\ntotal_ebola_global_2019 &lt;-\n  sum(ebola_global_2019$val)\n\n# Calculate the percentage change in DALYs due to COVID-19\npercent_change_dalys &lt;- round((total_ebola_global_2019 / total_dalys_2019) *100,4)\n\n# Print the percentage change in DALYs due to COVID-19\ncat(\"Total impact of Ebola virus Globally on DALYs rates in 2019:\", percent_change_dalys, \"%\")\n\nTotal impact of Ebola virus Globally on DALYs rates in 2019: 0.0037 %\n\n\nHowever, the impact of Ebola on DALYs is more significant in certain African regions, it accounts for the whole population. For example, during the 2014-2016, the Ebola outbreak in West Africa caused an estimated 11,000 deaths and 261,000 DALYs lost.\nOverall, while the global impact of Ebola on DALYs in 2019 was relatively low, it is still an important health concern in areas where outbreaks occur, and efforts to prevent and control the disease are crucial to reducing its impact on affected populations."
  },
  {
    "objectID": "content/proj/index.html",
    "href": "content/proj/index.html",
    "title": "Overview",
    "section": "",
    "text": "Welcome to the Projects section! Here, I showcase a variety of projects and resources that I‚Äôve worked on, focusing on health index performance and metrics.\nIn this section, you‚Äôll find:\n\nHealth Index Projects: Detailed explorations of health index performance, including methodologies, analyses, and findings.\n\nFeel free to explore the projects and resources available here. I hope you‚Äôll find something valuable that can aid you in your own work or studies.\nDive in and discover the insights and tools I‚Äôve created!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Index Projects\n\n\nIn this section I share my projects and resources related to health index Performance and Metrics\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/books/posts/hmsid/posts/012026/index.html",
    "href": "content/books/posts/hmsid/posts/012026/index.html",
    "title": "The Hidden Math That Shapes Our Health: 5 Surprising Truths from Data Science",
    "section": "",
    "text": "How do we measure the health of a nation? The first number that often comes to mind is life expectancy. Japan, for example, boasts one of the world‚Äôs highest, suggesting a simple story of success. Yet, this single figure can be misleading. A long life is not the same as a healthy life, and the story of our collective well-being is far more complex than one metric can capture.\nBeneath the surface of headlines and familiar statistics, a sophisticated set of metrics and models gives us a much truer picture of public health. This isn‚Äôt just about how long we live, but how well we live, what truly puts us at risk, and where our health is most vulnerable. The science of health metrics moves beyond simple averages to quantify the true burden of disease.\n\n\n\nHealth Metrics - AI created image\n\n\nThis article explores five of the most surprising and impactful ideas from the cutting edge of health data science. These concepts are reshaping how we understand illness, risk, and what it truly means for a population to be healthy.\n\n\nFor decades, mortality rates were the primary way to track public health crises. But what about the millions who live for years with chronic pain, mobility issues, or mental health disorders? To capture this, experts developed a more holistic metric: the Disability-Adjusted Life Year (DALY). This is now the standard for measuring the total ‚Äúburden of disease.‚Äù\nThe DALY is a single number composed of two parts:\n\nYears of Life Lost (YLLs): This is the more traditional part, accounting for the years lost when someone dies prematurely.\nYears Lived with Disability (YLDs): This is the revolutionary component. It measures the years people live in a state of less-than-ideal health due to a disease or injury.\n\nThe hidden math that makes this metric so powerful lies in disability weights. Each health condition, from hearing loss to severe depression, is assigned a numerical weight based on its severity. This allows scientists to mathematically quantify the impact of different illnesses, revealing a counter-intuitive truth: a very long life filled with chronic illness can represent a greater public health burden than a shorter one.\nBy combining mortality (death) and morbidity (illness), this metric shifts the entire goal of public health. It‚Äôs no longer just about keeping people alive; it‚Äôs about maximizing the healthy, functional years they get to live‚Äîand it provides a single, powerful number to measure our success.\n\n\n\nHow do you measure the harm caused by something like air pollution or a poor diet? It‚Äôs a surprisingly tricky question. The answer lies in a fascinating concept used by public health scientists: the Theoretical Minimum-Risk Exposure Level (TMREL).\nInstead of just tracking the negative effects of a risk factor, scientists first define a hypothetical, ideal scenario. They imagine a perfect world where exposure to that risk is at the absolute lowest, most plausible level. For example, what would our health look like if everyone had the optimal diet, or if air quality was pristine?\nThe true ‚Äúburden‚Äù of that risk is then calculated as the gap between our current reality and that ‚Äúperfect world‚Äù scenario. This connects directly back to DALYs; the total burden of air pollution, for instance, is the difference in DALYs between our current world and one with theoretically clean air. This is a powerful reframing. It moves public health from a goal of small, incremental improvements to measuring the full potential for health that is lost due to a given risk.\n\n\n\nWhen a new virus emerges, how do scientists predict its spread? Historically, they have relied on ‚Äúmechanistic models,‚Äù but a new approach driven by artificial intelligence is changing the game.\n\nMechanistic Models (like SIR): Think of these as a ‚Äúplumber‚Äôs blueprint.‚Äù They are ‚Äúrule-based‚Äù and require knowing exactly how all the pipes (rules of transmission and recovery) are connected to predict the water flow (disease spread). The classic SIR model, for example, uses differential equations to simulate how a population moves between being Susceptible, Infected, and Recovered. These models need a deep understanding of a disease‚Äôs biology to work.\nMachine Learning Models: These models are ‚Äúempirically driven‚Äù‚Äîthey are powerful pattern-finders. Imagine a mysterious box that has observed thousands of plumbing systems. It doesn‚Äôt need the blueprint; it just learns the patterns of when and where leaks are most likely to occur by analyzing massive datasets (infection rates, demographics, temperature, etc.) to find complex relationships that predict outcomes.\n\nThe key distinction is profound and highlights a major shift in epidemiology:\nMechanistic models rely on known relationships and equations, while machine learning models learn patterns from data.\nThis is a game-changer because machine learning can uncover unexpected drivers of disease that traditional, rule-based models might completely miss, offering a powerful new tool in the fight against pandemics.\n\n\n\nModern society is saturated with information about potential threats, from global pandemics and terrorism to environmental disasters. This constant exposure shapes our perception of risk, but our fears don‚Äôt always align with the statistical reality of what harms us.\n‚Äú‚Ä¶fear is the most pervasive emotion of modern society‚Ä¶‚Äù\nA look at how risks have evolved shows a significant shift. Historical risks were dominated by immediate threats like starvation, infectious diseases, and violent conflicts. Today, while perceived threats like pandemics capture our attention, the greatest ‚Äúburden of disease‚Äù in many parts of the world comes from less dramatic but far more widespread sources.\nAs the Global Burden of Disease study shows, modern risks are largely driven by four key categories of risk factors: Behavioral, Environmental, Occupational, and Metabolic. These include lifestyle choices like a poor diet, physical inactivity, tobacco use, and excessive alcohol consumption. Conditions like heart disease and obesity contribute massively to the global DALY count. This reveals a critical disconnect: we often fear the sudden and dramatic, but the slow, chronic, and everyday factors are what statistically cause the most harm to our collective health.\n\n\n\nWhere you live can be just as important as who you are when it comes to health risks. Modern health analysis is increasingly spatial, looking not just at who gets sick, but where outbreaks are most likely to occur.\nUsing spatial modeling, scientists can feed data like temperature, elevation, and population density into powerful models to create detailed, geographic risk maps. These maps can identify disease ‚Äúhotspots‚Äù with incredible precision.\nOne of the sophisticated techniques used for this is Kriging. Think of it like creating a weather map: you have temperature readings from a few dozen weather stations (the known points). Kriging is the statistical method that intelligently fills in all the gaps, creating a smooth, continuous heat map of the entire region. Health officials use the same principle to map disease ‚Äúhotspots‚Äù from a limited number of clinic reports. This isn‚Äôt just theoretical; the source material dedicates an entire chapter to using these spatial models to map malaria outbreaks. For public health officials, it means they can target interventions‚Äîlike distributing bed nets‚Äîto the precise areas where they will save the most lives and resources, turning a map into a life-saving tool."
  },
  {
    "objectID": "content/books/posts/hmsid/posts/012026/index.html#overview",
    "href": "content/books/posts/hmsid/posts/012026/index.html#overview",
    "title": "The Hidden Math That Shapes Our Health: 5 Surprising Truths from Data Science",
    "section": "",
    "text": "How do we measure the health of a nation? The first number that often comes to mind is life expectancy. Japan, for example, boasts one of the world‚Äôs highest, suggesting a simple story of success. Yet, this single figure can be misleading. A long life is not the same as a healthy life, and the story of our collective well-being is far more complex than one metric can capture.\nBeneath the surface of headlines and familiar statistics, a sophisticated set of metrics and models gives us a much truer picture of public health. This isn‚Äôt just about how long we live, but how well we live, what truly puts us at risk, and where our health is most vulnerable. The science of health metrics moves beyond simple averages to quantify the true burden of disease.\n\n\n\nHealth Metrics - AI created image\n\n\nThis article explores five of the most surprising and impactful ideas from the cutting edge of health data science. These concepts are reshaping how we understand illness, risk, and what it truly means for a population to be healthy.\n\n\nFor decades, mortality rates were the primary way to track public health crises. But what about the millions who live for years with chronic pain, mobility issues, or mental health disorders? To capture this, experts developed a more holistic metric: the Disability-Adjusted Life Year (DALY). This is now the standard for measuring the total ‚Äúburden of disease.‚Äù\nThe DALY is a single number composed of two parts:\n\nYears of Life Lost (YLLs): This is the more traditional part, accounting for the years lost when someone dies prematurely.\nYears Lived with Disability (YLDs): This is the revolutionary component. It measures the years people live in a state of less-than-ideal health due to a disease or injury.\n\nThe hidden math that makes this metric so powerful lies in disability weights. Each health condition, from hearing loss to severe depression, is assigned a numerical weight based on its severity. This allows scientists to mathematically quantify the impact of different illnesses, revealing a counter-intuitive truth: a very long life filled with chronic illness can represent a greater public health burden than a shorter one.\nBy combining mortality (death) and morbidity (illness), this metric shifts the entire goal of public health. It‚Äôs no longer just about keeping people alive; it‚Äôs about maximizing the healthy, functional years they get to live‚Äîand it provides a single, powerful number to measure our success.\n\n\n\nHow do you measure the harm caused by something like air pollution or a poor diet? It‚Äôs a surprisingly tricky question. The answer lies in a fascinating concept used by public health scientists: the Theoretical Minimum-Risk Exposure Level (TMREL).\nInstead of just tracking the negative effects of a risk factor, scientists first define a hypothetical, ideal scenario. They imagine a perfect world where exposure to that risk is at the absolute lowest, most plausible level. For example, what would our health look like if everyone had the optimal diet, or if air quality was pristine?\nThe true ‚Äúburden‚Äù of that risk is then calculated as the gap between our current reality and that ‚Äúperfect world‚Äù scenario. This connects directly back to DALYs; the total burden of air pollution, for instance, is the difference in DALYs between our current world and one with theoretically clean air. This is a powerful reframing. It moves public health from a goal of small, incremental improvements to measuring the full potential for health that is lost due to a given risk.\n\n\n\nWhen a new virus emerges, how do scientists predict its spread? Historically, they have relied on ‚Äúmechanistic models,‚Äù but a new approach driven by artificial intelligence is changing the game.\n\nMechanistic Models (like SIR): Think of these as a ‚Äúplumber‚Äôs blueprint.‚Äù They are ‚Äúrule-based‚Äù and require knowing exactly how all the pipes (rules of transmission and recovery) are connected to predict the water flow (disease spread). The classic SIR model, for example, uses differential equations to simulate how a population moves between being Susceptible, Infected, and Recovered. These models need a deep understanding of a disease‚Äôs biology to work.\nMachine Learning Models: These models are ‚Äúempirically driven‚Äù‚Äîthey are powerful pattern-finders. Imagine a mysterious box that has observed thousands of plumbing systems. It doesn‚Äôt need the blueprint; it just learns the patterns of when and where leaks are most likely to occur by analyzing massive datasets (infection rates, demographics, temperature, etc.) to find complex relationships that predict outcomes.\n\nThe key distinction is profound and highlights a major shift in epidemiology:\nMechanistic models rely on known relationships and equations, while machine learning models learn patterns from data.\nThis is a game-changer because machine learning can uncover unexpected drivers of disease that traditional, rule-based models might completely miss, offering a powerful new tool in the fight against pandemics.\n\n\n\nModern society is saturated with information about potential threats, from global pandemics and terrorism to environmental disasters. This constant exposure shapes our perception of risk, but our fears don‚Äôt always align with the statistical reality of what harms us.\n‚Äú‚Ä¶fear is the most pervasive emotion of modern society‚Ä¶‚Äù\nA look at how risks have evolved shows a significant shift. Historical risks were dominated by immediate threats like starvation, infectious diseases, and violent conflicts. Today, while perceived threats like pandemics capture our attention, the greatest ‚Äúburden of disease‚Äù in many parts of the world comes from less dramatic but far more widespread sources.\nAs the Global Burden of Disease study shows, modern risks are largely driven by four key categories of risk factors: Behavioral, Environmental, Occupational, and Metabolic. These include lifestyle choices like a poor diet, physical inactivity, tobacco use, and excessive alcohol consumption. Conditions like heart disease and obesity contribute massively to the global DALY count. This reveals a critical disconnect: we often fear the sudden and dramatic, but the slow, chronic, and everyday factors are what statistically cause the most harm to our collective health.\n\n\n\nWhere you live can be just as important as who you are when it comes to health risks. Modern health analysis is increasingly spatial, looking not just at who gets sick, but where outbreaks are most likely to occur.\nUsing spatial modeling, scientists can feed data like temperature, elevation, and population density into powerful models to create detailed, geographic risk maps. These maps can identify disease ‚Äúhotspots‚Äù with incredible precision.\nOne of the sophisticated techniques used for this is Kriging. Think of it like creating a weather map: you have temperature readings from a few dozen weather stations (the known points). Kriging is the statistical method that intelligently fills in all the gaps, creating a smooth, continuous heat map of the entire region. Health officials use the same principle to map disease ‚Äúhotspots‚Äù from a limited number of clinic reports. This isn‚Äôt just theoretical; the source material dedicates an entire chapter to using these spatial models to map malaria outbreaks. For public health officials, it means they can target interventions‚Äîlike distributing bed nets‚Äîto the precise areas where they will save the most lives and resources, turning a map into a life-saving tool."
  },
  {
    "objectID": "content/books/posts/hmsid/posts/012026/index.html#conclusion-the-power-of-a-clearer-picture",
    "href": "content/books/posts/hmsid/posts/012026/index.html#conclusion-the-power-of-a-clearer-picture",
    "title": "The Hidden Math That Shapes Our Health: 5 Surprising Truths from Data Science",
    "section": "Conclusion: The Power of a Clearer Picture",
    "text": "Conclusion: The Power of a Clearer Picture\nThe way we measure, model, and visualize health data is becoming more powerful every year. By moving beyond simple metrics like life expectancy, we are gaining a much clearer, more honest picture of the complex challenges we face‚Äîfrom the quiet burden of chronic disease to the geographic patterns of an epidemic.\n\n\n\n\n\n\nThis clearer picture isn‚Äôt just an academic exercise; it equips us to act more effectively and equitably. As our ability to quantify the burden of disease becomes more precise, what is our collective responsibility to lighten it?\n\n\n\nFor those interested in diving deeper into these concepts, I highly recommend exploring the comprehensive resource: ‚ÄúHealth Metrics and the Spread of Infectious Diseases Machine Learning Applications and Spatial Modelling Analysis with R‚Äù available at fgazzelloni.github.io/hmsidR/. This book provides an in-depth look at the methodologies and theories that underpin modern health metrics, offering valuable insights for anyone passionate about public health and data science."
  },
  {
    "objectID": "content/books/index.html",
    "href": "content/books/index.html",
    "title": "Books Writing",
    "section": "",
    "text": "Health Metrics and the Spread of Infectious Diseases (HMSID)\n\n\nA comprehensive resource on health metrics and infectious disease spread.\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/ggstream/index.html",
    "href": "content/blog/posts/dataviz/posts/ggstream/index.html",
    "title": "Retail Sales with ggstream",
    "section": "",
    "text": "This post is all about Retail Sales with ggstream, the dataset comes from #TidyTuesday 2022 week 50 Monthly State Retail Sales.\nThe picture below is the result of the ggstream visualization.\n\n\n\nRetail Sales with ggstream\n\n\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(fuzzyjoin)\nlibrary(ggstream)\nlibrary(colorspace)\n\nSet the theme\n\ntheme_set(theme_minimal(base_family = \"Roboto Condensed\",\n                        base_size = 12))\n\n\ntheme_update(\n  plot.title = element_text(\n    size = 20,\n    face = \"bold\",\n    hjust = .5,\n    margin = margin(10, 0, 30, 0)\n  ),\n  plot.caption = element_text(\n    size = 9,\n    color = \"grey40\",\n    hjust = .5,\n    margin = margin(20, 0, 5, 0)\n  ),\n  axis.text.y = element_blank(),\n  axis.title = element_blank(),\n  plot.background = element_rect(fill = \"grey88\", color = NA),\n  panel.background = element_rect(fill = NA, color = NA),\n  panel.grid = element_blank(),\n  panel.spacing.y = unit(0, \"lines\"),\n  strip.text.y = element_text(angle = 0),\n  legend.position = \"bottom\",\n  legend.text = element_text(size = 9, color = \"grey40\"),\n  legend.box.margin = margin(t = 30),\n  legend.background = element_rect(\n    color = \"grey40\",\n    linewidth = .3,\n    fill = \"grey95\"\n  ),\n  legend.key.height = unit(.25, \"lines\"),\n  legend.key.width = unit(2.5, \"lines\"),\n  plot.margin = margin(rep(20, 4))\n)\n\nAnd the color palette\n\npal &lt;- c(\"#FFB400\",\n         \"#C20008\",\n         \"#13AFEF\",\n         \"#8E038E\")\n\nLoad the data\n\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 50)\ncoverage_codes &lt;- tuesdata$coverage_codes\nstate_retail &lt;- tuesdata$state_retail\n\nAdd the states‚Äô names\n\nfipcodes &lt;- tigris::fips_codes %&gt;%\n  select(state, state_name)\n\nJoin all sets\n\nmy_df &lt;- state_retail %&gt;%\n  left_join(fipcodes, by = c(\"state_abbr\" = \"state\")) %&gt;%\n  mutate(state_name = ifelse(state_abbr == \"USA\", \"USA\", state_name)) %&gt;%\n  distinct() %&gt;%\n  merge(coverage_codes, by = \"coverage_code\") %&gt;%\n  arrange()\n\nmy_df %&gt;% head\n\nData wrangling\n\nmy_df1 &lt;- my_df %&gt;%\n  select(-naics) %&gt;%\n  mutate(\n    coverage = case_when(\n      coverage == \"non-imputed coverage is greater than or equal to 10% and less than 25% of the state/NAICS total\" ~\n        \"greater than or equal 10% and less than 25% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is greater than or equal to 25% and less than 50% of the state/NAICS total\" ~\n        \"greater than or equal to 25% and less than 50% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is greater than or equal to 50% of the state/NAICS total.\" ~\n        \"greater than or equal to 50% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is less than 10% of the state/NAICS total.\" ~\n        \"less than 10% of the state/NAICS total\",\n      TRUE ~ coverage\n    ),\n    month = as.character(month),\n    year = zoo::as.yearmon(paste0(year, \"-\", month)),\n    change_yoy = ifelse(change_yoy == \"S\", 0, change_yoy),\n    change_yoy_se = ifelse(change_yoy_se == \"S\", 0, change_yoy_se),\n    change_yoy = as.numeric(change_yoy),\n    change_yoy_se = as.numeric(change_yoy_se),\n    coverage = as.factor(coverage),\n    coverage = paste(coverage_code, \"-\", coverage)\n  ) %&gt;%\n  filter(state_abbr %in% c(\"USA\", \"PA\", \"MD\", \"MT\")) %&gt;%\n  filter(!coverage_code == \"S\") %&gt;%\n  group_by(state_name, coverage, year) %&gt;%\n  summarise_if(is.numeric, sum, na.rm = TRUE) %&gt;%\n  mutate(change_yoy = scale(change_yoy, center = FALSE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = as.POSIXct(year),\n         year = as.Date(year))\n\nMake the plot\n\nmy_df1 %&gt;%\n  ggplot(aes(\n    x = year,\n    y = change_yoy,\n    color = coverage,\n    fill = coverage\n  )) +\n  geom_stream(\n    geom = \"contour\",\n    color = \"white\",\n    linewidth = 1.25,\n    bw = .45 # Controls smoothness\n  ) +\n  geom_stream(geom = \"polygon\",\n              bw = .45,\n              linewidth = 0.2) +\n  facet_grid(state_name ~ .,\n             scales = \"free_y\",\n             space = \"free\") +\n  scale_y_continuous(trans = scales::modulus_trans(0.1, 1)) +\n  scale_x_date(date_breaks = \"6 months\",\n               date_labels = \"%b-%Y\",\n               expand = c(0, 0)) +\n  scale_color_manual(expand = c(0, 0),\n                     values = pal,\n                     guide = \"none\") +\n  scale_fill_manual(values = pal,\n                    name = NULL) +\n  labs(title = \"Total Year-Over-Year percent change\\nin monthly retail sales value\",\n       subtitle = \"North American Industry Classification System (NAICS) top YoY states\",\n       caption = \"DataSource: #TidyTuesday 2022 Week50 | Monthly State Retail Sales | DataViz: Fgazzelloni\") +\n  theme(legend.direction = \"vertical\")\n\n\nggsave(\"w50_retail_sales.png\")\n\n\n\nggstream\nGraph Gallery"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/ggstream/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/ggstream/index.html#overview",
    "title": "Retail Sales with ggstream",
    "section": "",
    "text": "This post is all about Retail Sales with ggstream, the dataset comes from #TidyTuesday 2022 week 50 Monthly State Retail Sales.\nThe picture below is the result of the ggstream visualization.\n\n\n\nRetail Sales with ggstream\n\n\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(fuzzyjoin)\nlibrary(ggstream)\nlibrary(colorspace)\n\nSet the theme\n\ntheme_set(theme_minimal(base_family = \"Roboto Condensed\",\n                        base_size = 12))\n\n\ntheme_update(\n  plot.title = element_text(\n    size = 20,\n    face = \"bold\",\n    hjust = .5,\n    margin = margin(10, 0, 30, 0)\n  ),\n  plot.caption = element_text(\n    size = 9,\n    color = \"grey40\",\n    hjust = .5,\n    margin = margin(20, 0, 5, 0)\n  ),\n  axis.text.y = element_blank(),\n  axis.title = element_blank(),\n  plot.background = element_rect(fill = \"grey88\", color = NA),\n  panel.background = element_rect(fill = NA, color = NA),\n  panel.grid = element_blank(),\n  panel.spacing.y = unit(0, \"lines\"),\n  strip.text.y = element_text(angle = 0),\n  legend.position = \"bottom\",\n  legend.text = element_text(size = 9, color = \"grey40\"),\n  legend.box.margin = margin(t = 30),\n  legend.background = element_rect(\n    color = \"grey40\",\n    linewidth = .3,\n    fill = \"grey95\"\n  ),\n  legend.key.height = unit(.25, \"lines\"),\n  legend.key.width = unit(2.5, \"lines\"),\n  plot.margin = margin(rep(20, 4))\n)\n\nAnd the color palette\n\npal &lt;- c(\"#FFB400\",\n         \"#C20008\",\n         \"#13AFEF\",\n         \"#8E038E\")\n\nLoad the data\n\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 50)\ncoverage_codes &lt;- tuesdata$coverage_codes\nstate_retail &lt;- tuesdata$state_retail\n\nAdd the states‚Äô names\n\nfipcodes &lt;- tigris::fips_codes %&gt;%\n  select(state, state_name)\n\nJoin all sets\n\nmy_df &lt;- state_retail %&gt;%\n  left_join(fipcodes, by = c(\"state_abbr\" = \"state\")) %&gt;%\n  mutate(state_name = ifelse(state_abbr == \"USA\", \"USA\", state_name)) %&gt;%\n  distinct() %&gt;%\n  merge(coverage_codes, by = \"coverage_code\") %&gt;%\n  arrange()\n\nmy_df %&gt;% head\n\nData wrangling\n\nmy_df1 &lt;- my_df %&gt;%\n  select(-naics) %&gt;%\n  mutate(\n    coverage = case_when(\n      coverage == \"non-imputed coverage is greater than or equal to 10% and less than 25% of the state/NAICS total\" ~\n        \"greater than or equal 10% and less than 25% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is greater than or equal to 25% and less than 50% of the state/NAICS total\" ~\n        \"greater than or equal to 25% and less than 50% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is greater than or equal to 50% of the state/NAICS total.\" ~\n        \"greater than or equal to 50% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is less than 10% of the state/NAICS total.\" ~\n        \"less than 10% of the state/NAICS total\",\n      TRUE ~ coverage\n    ),\n    month = as.character(month),\n    year = zoo::as.yearmon(paste0(year, \"-\", month)),\n    change_yoy = ifelse(change_yoy == \"S\", 0, change_yoy),\n    change_yoy_se = ifelse(change_yoy_se == \"S\", 0, change_yoy_se),\n    change_yoy = as.numeric(change_yoy),\n    change_yoy_se = as.numeric(change_yoy_se),\n    coverage = as.factor(coverage),\n    coverage = paste(coverage_code, \"-\", coverage)\n  ) %&gt;%\n  filter(state_abbr %in% c(\"USA\", \"PA\", \"MD\", \"MT\")) %&gt;%\n  filter(!coverage_code == \"S\") %&gt;%\n  group_by(state_name, coverage, year) %&gt;%\n  summarise_if(is.numeric, sum, na.rm = TRUE) %&gt;%\n  mutate(change_yoy = scale(change_yoy, center = FALSE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = as.POSIXct(year),\n         year = as.Date(year))\n\nMake the plot\n\nmy_df1 %&gt;%\n  ggplot(aes(\n    x = year,\n    y = change_yoy,\n    color = coverage,\n    fill = coverage\n  )) +\n  geom_stream(\n    geom = \"contour\",\n    color = \"white\",\n    linewidth = 1.25,\n    bw = .45 # Controls smoothness\n  ) +\n  geom_stream(geom = \"polygon\",\n              bw = .45,\n              linewidth = 0.2) +\n  facet_grid(state_name ~ .,\n             scales = \"free_y\",\n             space = \"free\") +\n  scale_y_continuous(trans = scales::modulus_trans(0.1, 1)) +\n  scale_x_date(date_breaks = \"6 months\",\n               date_labels = \"%b-%Y\",\n               expand = c(0, 0)) +\n  scale_color_manual(expand = c(0, 0),\n                     values = pal,\n                     guide = \"none\") +\n  scale_fill_manual(values = pal,\n                    name = NULL) +\n  labs(title = \"Total Year-Over-Year percent change\\nin monthly retail sales value\",\n       subtitle = \"North American Industry Classification System (NAICS) top YoY states\",\n       caption = \"DataSource: #TidyTuesday 2022 Week50 | Monthly State Retail Sales | DataViz: Fgazzelloni\") +\n  theme(legend.direction = \"vertical\")\n\n\nggsave(\"w50_retail_sales.png\")\n\n\n\nggstream\nGraph Gallery"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/dubois2023/index.html",
    "href": "content/blog/posts/dataviz/posts/dubois2023/index.html",
    "title": "DuBois Challenge 2022",
    "section": "",
    "text": "Overview\nThis week is all about #DuBoisChallenge2022, I choose plate number 14.\n\n\nPlate 14: ILLITERACY\n\nLoad #TidyTuesday 2022/07 data\n\nlibrary(tidyverse)\n\n\nplate6 &lt;- read_csv('https://raw.githubusercontent.com/ajstarks/dubois-data-portraits/master/challenge/2022/challenge06/data.csv')\n\nAdd a column with distances\n\ndf &lt;- plate6 %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(x_axis=c(1860,1860-cumsum(diff(iliteracy_rate))))\ndf\n\nCreate axis lables vectors\n\niliteracy_rate&lt;-df$iliteracy_rate\nx_axis &lt;-df$x_axis\n\nFonts\n\nlibrary(showtext)\nfont_add(family = \"Public Sans Thin\",\n         regular = \"PublicSans-Thin.ttf\")\n\nfont_add(family = \"PublicSans-Medium\",\n         regular = \"PublicSans-Medium.ttf\")\n \nshowtext_auto()\n\nMake the plot\n\nlibrary(ggstar) # for making the triangles\n  \nnumber6 &lt;- df %&gt;%\n  ggplot(aes(x = x_axis, y = iliteracy_rate))+\n  \n  # add the columns\n  geom_col(width = 1.5,fill=\"black\") +\n\n  # this is one way to add the horizontal lines\n  geom_segment(aes(x = 1858, xend = x_axis+0.1,\n                   y = iliteracy_rate, yend = iliteracy_rate),\n               size = 2.5, color = \"black\") +\n  geom_segment(aes(x = 1858, xend = x_axis+0.1,\n                   y = iliteracy_rate, yend = iliteracy_rate),\n               size = 2.4, color = \"#d9ccbf\")+\n  \n  # with ggstar add the triangular shape\n  geom_star(starshape = 20, size = 0.8,angle = 90, fill = \"black\",\n            position = position_nudge(x = -0.1, y = -0.15)) +\n  \n  # add the little round corners\n  geom_curve(aes(x = x_axis - 0.1, xend = x_axis + 0.65,\n                 y = iliteracy_rate + 1.2, yend = iliteracy_rate),\n             curvature = -0.6, size = 0.1) +\n  \n  # customize the axis values\n  scale_x_continuous(breaks = x_axis,\n                     labels = c(\"99%\",\"92%\",\"81.6%\",\"67.27%\",\"(50%?)\"),\n                     expand = expansion(0.01)) +\n  scale_y_continuous(breaks = iliteracy_rate,\n                     labels = c(1860,1870,1880,1890,\"(1900?)\"),\n                     expand = expansion(0.01)) +\n  \n  # add a title and a theme\n  labs(title = \"ILLITERACY.\\n\", subtitle = \" \", caption = \"fg\")+\n  theme_void()+\n  theme(text = element_text(size = 18, family = \"Public Sans Thin\", color = \"grey25\"),\n        plot.title = element_text(size = 24, family = \"PublicSans-Medium\",face = \"bold\", hjust = 0.5),\n        plot.background = element_rect(fill = \"#d2c2b3\", color = \"#d2c2b3\"),\n        panel.background =  element_rect(fill = \"#d2c2b3\",color = \"#d2c2b3\"),\n        axis.text.x = element_text(),\n        axis.text.y = element_text(),\n        plot.margin = margin(0,25,0,25))\n\nAssemble background, plot and annotation\nWith {cowplot} add a background image as the same as the original one and the text on the left side of the x-axis\n\nlibrary(cowplot)\nggdraw()+\n  draw_image(\"number6.png\",scale=1.1) + # set the background image \n  draw_plot(number6,scale=0.9) + # add the plot\n  draw_label(\"PERCENT OF\\nILLITERACY.\", # add info on a side \n            x=0.19,y=0.09,\n            size = 12.5,\n            fontfamily=\"Public Sans Thin\",lineheight = .3)\n\nSave the plot\n\nggsave(\"w7_Number6.png\", \n       width = 1000, height = 1350, \n       units = \"px\", dpi = 320)\n\n\n\n Back to top"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/erasmus/index.html",
    "href": "content/blog/posts/dataviz/posts/erasmus/index.html",
    "title": "Erasmus students exchange",
    "section": "",
    "text": "This week 10 of #TidyTuesday 2022 theme is #Erasmus students exchange in the European countries.\n\n\nErasmus students exchange\n\nThe data set is from Erasmus student mobility, Data.Europa.eu and Wimdu.co to discover the most popular Erasmus destinations.\nThe idea is to make a network of sending and receiving countries, let‚Äôs have a look at the data.\n\nif(!require(pacman)) install.packages(\"pacman\")\npacman::p_load(tidyverse, ggbump, cowplot, wesanderson)\n\n\nerasmus &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-08/erasmus.csv')\n\nThe set is made of information about students, such as the age, the nationality, the lenght of stay, the gender, academic year, and others. I selected some of them, to extract the information I needed to make the network.\n\ndf &lt;- erasmus%&gt;%\n  select(sending_country_code,receiving_country_code,\n         participant_nationality,participants,\n         participant_age,\n         academic_year,mobility_duration,\n         participant_gender)\n\nkableExtra::kable(head(df)) \n\nLooking at the participant_age we see that we have some misleading data:\n\ndf %&gt;% pull(participant_age) %&gt;% summary()\n\nFor this reason the best way is to filter students between 17 and 28 years old. Also, mobility_duration is quite surprising:\n\ndf %&gt;% pull(mobility_duration) %&gt;% summary()\n\nThe median value of the students‚Äô stay is ONE day, while the mean is just a little above TWO days. Very few students stay more than 10 days, but someone reaches a max of 273 days (39 weeks).\n\ndf %&gt;% count(mobility_duration) %&gt;% mutate(perc = round(n/sum(n)*100,2)) %&gt;% head()\ndf %&gt;% count(mobility_duration) %&gt;% mutate(perc = round(n/sum(n)*100,3)) %&gt;% tail()\n\n\nStudent participants are almost all solo participants as the median shows to be ONE student per observation, TWO students on average, with a max value of 279. So that, to have a picture of the phenomenon select the average value of the student participants as representative.\n\ndf %&gt;% pull(participants) %&gt;% summary(participants)\n\nFinally, gender, Females are slightly more than males, just a little above 50%.\n\ntbl &lt;-df %&gt;% pull(participant_gender) %&gt;% table()\ncbind(n=tbl,pct=round(prop.table(tbl)*100,2))\n\nThis is our new dataset on which we will build our network.\n\ndf &lt;- df %&gt;%\n  group_by(academic_year) %&gt;%\n  filter(between(x = participant_age,17,28),\n         mobility_duration&gt;3) %&gt;%\n  summarise(m_participants=mean(participants),\n            sending_country_code,receiving_country_code,\n            participant_gender,.groups=\"drop\") %&gt;%\n  ungroup() %&gt;%\n  select(-m_participants) %&gt;%\n  distinct()\nkableExtra::kable(head(df))%&gt;%\n  kableExtra::kable_styling(latex_options = \"scale_down\")\n\nAt this point I‚Äôd like to have the full country‚Äôs name, and use {ISOcodes} package. I do that because I‚Äôd like to make a spatial visualization as well. The package contains the values for the countries‚Äô abbreviations coded as ‚ÄúAlpha_2‚Äù. I needed to adjust UK and Greece. To verify this you might need to use the count() function and the str_detect() a couple of times before identifying all the values that needs an adjustment.\n\nlibrary(ISOcodes)\nisocodes&lt;-ISOcodes::ISO_3166_1\nisocodes2 &lt;- isocodes%&gt;%\n  mutate(Alpha_2=case_when(Alpha_2==\"GB\"~\"UK\",\n                           Alpha_2==\"GR\"~\"EL\",\n                           TRUE ~ Alpha_2))\n\nSome more manipulations for selecting just the countries in the dataset.\n\nsending_country_code &lt;- df %&gt;% count(sending_country_code) %&gt;% select(-n) %&gt;% unlist()\nreceiving_country_code &lt;- df %&gt;% count(receiving_country_code) %&gt;% select(-n) %&gt;% unlist()\n\nsending &lt;- isocodes2 %&gt;% filter(Alpha_2 %in% sending_country_code)\nreceiving &lt;- isocodes2 %&gt;% filter(Alpha_2 %in% receiving_country_code)\n\nsending_unlst &lt;- sending %&gt;% count(Name) %&gt;% select(-n) %&gt;% unlist()\nreceiving_unlst &lt;- receiving %&gt;% count(Name) %&gt;% select(-n) %&gt;% unlist()\n\n{ggplot2} package provides spatial data, as you might notice, students of the Erasmus programs come from all over the World. Still, some countries‚Äô name adjustments are needed.\n\nworld &lt;- map_data(\"world\")%&gt;%\n  filter(!region==\"Antarctica\")%&gt;%\n  mutate(region=case_when(region==\"UK\"~ \"United Kingdom\",\n                          region==\"Czech Republic\"~\"Czechia\",\n                          region==\"Moldova\"~\"Moldova, Republic of\",\n                          region==\"Palestine\"~\"Palestine, State of\",\n                          region==\"Russia\" ~ \"Russian Federation\",\n                          TRUE ~ region))\n\n\nsending_geo &lt;- world %&gt;% filter(region %in% sending_unlst)\n\nsending_geo_full &lt;- sending %&gt;%\n  select(Alpha_2,Name) %&gt;%\n  left_join(sending_geo,by = c(\"Name\"=\"region\"))\n\n\nreceiving_geo &lt;- world %&gt;% filter(region %in% receiving_unlst)\n\nreceiving_geo_full &lt;- receiving %&gt;%\n  select(Alpha_2,Name) %&gt;%\n  left_join(receiving_geo,by = c(\"Name\"=\"region\"))\n\nThen finally, make the centroids. We will use this in the visualization further below.\n\nsending_geo_centroids &lt;- sending_geo_full %&gt;%\n  group_by(Name) %&gt;%\n  mutate(avg_long = mean(range(long)),avg_lat = mean(range(lat))) %&gt;%\n  count(Alpha_2,Name,avg_long,avg_lat)\n\nreceiving_geo_centroids &lt;- receiving_geo_full %&gt;%\n  group_by(Name) %&gt;%\n  mutate(avg_long = mean(range(long)),avg_lat = mean(range(lat))) %&gt;%\n  count(Alpha_2,Name,avg_long,avg_lat)\n\ndf2 is our new data set, made of countries‚Äô names and spatials:\n\ndf2 &lt;- df %&gt;%\n  left_join(sending_geo_centroids,by=c(\"sending_country_code\"=\"Alpha_2\")) %&gt;%\n  left_join(receiving_geo_centroids,by=c(\"receiving_country_code\"=\"Alpha_2\")) %&gt;%\n  rename(sending_country_name=Name.x,receiving_country_name=Name.y,\n         avg_long_s=avg_long.x,avg_lat_s=avg_lat.x,\n         avg_long_r=avg_long.y,avg_lat_r=avg_lat.y)%&gt;%\n  select(-n.x,-n.y)\ndf2 %&gt;% head\n\norder_sending and order_receiving will be very useful for a third visualization in which all the sending countries will be matched with students‚Äô destinations. We will see this further below.\n\norder_sending &lt;- df2%&gt;%\n  count(sending_country_name,sort=TRUE)%&gt;%\n  mutate(index_sending=seq(1,length(sending_country_name),1),\n         index_sending=rev(index_sending))%&gt;%\n  drop_na()\n\norder_receiving &lt;- df2%&gt;%\n  count(receiving_country_name,sort=TRUE)%&gt;%\n  mutate(index_receiving=seq(1,length(receiving_country_name),1),\n         index_receiving=rev(index_receiving))%&gt;%\n  drop_na()\n\nAnd here is the data set that we will use to make the rank network:\n\nerasmus_network &lt;- df2 %&gt;%\n  select(academic_year,\n         sending_country_name,receiving_country_name) %&gt;%\n  distinct() %&gt;%\n  left_join(order_sending,by=\"sending_country_name\") %&gt;%\n  left_join(order_receiving,by=\"receiving_country_name\") %&gt;%\n  mutate(group = glue::glue(\"{sending_country_name}-{receiving_country_name}\")) %&gt;%\n  distinct() %&gt;%\n  drop_na() %&gt;%\n  arrange(index_sending) %&gt;%\n  mutate(year_id=case_when(academic_year==\"2014-2015\"~1,\n                           academic_year==\"2015-2016\"~2,\n                           academic_year==\"2016-2017\"~3,\n                           academic_year==\"2017-2018\"~4,\n                           academic_year==\"2018-2019\"~5,\n                           academic_year==\"2019-2020\"~6)) %&gt;%\n  relocate(year_id) %&gt;%\n  arrange(year_id)\n\nerasmus_network %&gt;% head\n\nLoad the packages for setting a nice font.\n\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(extrafont)\n\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\n\nfont_add_google(name=\"Noto Sans\",family=\"notosans\")\n\n\nerasmus_network%&gt;%\n  arrange(index_sending)%&gt;%\n  ggplot()+\n  \n  geom_text(aes(x = -2, y = index_sending+5, label = sending_country_name), \n            vjust=0, \n            hjust=\"left\", color = \"darkred\", size = 3) +\n  \n  ggbump::geom_sigmoid(aes(x = -2, xend = 16.1, \n                           y = index_sending+5, yend =index_receiving+18, \n                           group=factor(group),color=receiving_country_name), \n                       alpha = .6, smooth = 10, size = 0.1,show.legend = F) +\n  \n  geom_text(aes(x = 16, y = index_receiving+17.5, label = receiving_country_name), \n            vjust=-1.5, hjust=\"right\", color = \"darkred\", size = 3) +\n  coord_cartesian()+\n  theme_void()\n\nOr a simplified version:\n\n\nnetwork\n\nLet‚Äôs select Top 5 sending countries:\n\nerasmus_network2 &lt;- erasmus_network %&gt;%\n  filter(sending_country_name%in%c(\"Italy\",\n                                   \"Germany\",\n                                   \"United Kingdom\",\n                                   \"Romania\",\"Spain\")) %&gt;%\n  mutate(sending_country_name=case_when(sending_country_name==\"United Kingdom\"~\"UK\",\n                                        TRUE~sending_country_name))%&gt;%\n  count(year_id,academic_year,sending_country_name) %&gt;%\n  group_by(academic_year)%&gt;%\n  mutate(rank=rank(x=n))%&gt;%\n  ungroup()\n\nerasmus_network2 %&gt;% head()\n\n\nlibrary(ggthemes)\n\nggplot(erasmus_network2,\n       mapping=aes(academic_year,rank,\n                   group=factor(sending_country_name),\n                   color=factor(sending_country_name)))+ \n  geom_point(size = 7) +\n  geom_text(data = erasmus_network2 %&gt;% filter(year_id == min(year_id)),\n            aes(x = year_id - .1, \n                label = sending_country_name), size = 4, hjust = 1) +\n  geom_text(data = erasmus_network2 %&gt;% \n              filter(year_id == max(year_id)),\n            aes(x = year_id + .1, label = sending_country_name), \n            size = 4, hjust = 0,check_overlap = T) +\n  geom_bump(size = 2, smooth = 8) +\n  labs(y = \"RANK\",\n       x = \"Academic Year\",\n       title=\"Erasmus Top 5 student exchange countries\",\n       subtitle=\"Ranks of the highest sending frequency\",\n       caption=\"DataSource: Erasmus student mobility | Data.Europa.eu | Wimdu.co\\nDataViz: Federica Gazzelloni | #TidyTuesday Week 10 Erasmus\") +\n  scale_y_reverse() +\n  scale_color_manual(values = wesanderson::wes_palette(5, name = \"Royal2\"))+\n  cowplot::theme_minimal_grid(font_size = 14, line_size = 0) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank(),\n        plot.title = element_text(color=\"#ffc7ba\"),\n        plot.subtitle = element_text(color=\"#ffc7ba\"),\n        plot.caption = element_text(color=\"#ffc7ba\",size=8),\n        axis.text = element_text(color=\"#ffc7ba\"),\n        axis.title = element_text(color=\"#ffc7ba\"),\n        plot.background = element_rect(color=\"black\",fill=\"black\"),\n        panel.background = element_rect(color=\"black\",fill=\"black\"))\n\nHere is the final part of this post, I set the spatials for making a map visualizaton of the sending to receiving countries.\n\nsending_geo_full2&lt;-sending_geo_full%&gt;%mutate(direction=\"sending\")\nreceiving_geo_full2&lt;-receiving_geo_full%&gt;%mutate(direction=\"receiving\")\n\ngeo_full &lt;-rbind(sending_geo_full2,receiving_geo_full2)\n\n\ncentr_s&lt;- sending_geo_centroids%&gt;%mutate(direction=\"Sending\")\ncentr_r&lt;- receiving_geo_centroids%&gt;%mutate(direction=\"Receiving\")\ncentroids&lt;-rbind(centr_s,centr_r)%&gt;%\n  mutate(direction=as.factor(direction))\n  \nlevels(centroids$direction)&lt;-c(\"Sending\",\"Receiving\")\n\n\ngeo_full2&lt;- geo_full%&gt;%\n  mutate(direction=as.factor(direction))\n\nlevels(geo_full2$direction)&lt;-c(\"Receiving\",\"Sending\")\n\ngeo_full2$direction&lt;-relevel(geo_full2$direction,ref=\"Sending\")\n\n\nggplot(geo_full2)+\n  geom_polygon(data = world,\n               aes(x=long,y=lat,group=group),fill=\"grey78\",color=\"grey5\")+\n  \n  geom_polygon(aes(x=long,y=lat,group=group,fill=direction),alpha=0.3)+\n  \n  geom_point(data=centroids,\n             aes(x=avg_long, y=avg_lat,color=direction,shape=direction))+\n \n  coord_map(\"ortho\", orientation = c(33.366449, 24.022840, 0))+\n  facet_wrap(vars(direction))+\n  scale_x_continuous(\"Latitude\", expand=c(0,0)) +\n  scale_y_continuous(\"Longitude\", expand=c(0,0)) +\n  theme_void()+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/erasmus/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/erasmus/index.html#overview",
    "title": "Erasmus students exchange",
    "section": "",
    "text": "This week 10 of #TidyTuesday 2022 theme is #Erasmus students exchange in the European countries.\n\n\nErasmus students exchange\n\nThe data set is from Erasmus student mobility, Data.Europa.eu and Wimdu.co to discover the most popular Erasmus destinations.\nThe idea is to make a network of sending and receiving countries, let‚Äôs have a look at the data.\n\nif(!require(pacman)) install.packages(\"pacman\")\npacman::p_load(tidyverse, ggbump, cowplot, wesanderson)\n\n\nerasmus &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-08/erasmus.csv')\n\nThe set is made of information about students, such as the age, the nationality, the lenght of stay, the gender, academic year, and others. I selected some of them, to extract the information I needed to make the network.\n\ndf &lt;- erasmus%&gt;%\n  select(sending_country_code,receiving_country_code,\n         participant_nationality,participants,\n         participant_age,\n         academic_year,mobility_duration,\n         participant_gender)\n\nkableExtra::kable(head(df)) \n\nLooking at the participant_age we see that we have some misleading data:\n\ndf %&gt;% pull(participant_age) %&gt;% summary()\n\nFor this reason the best way is to filter students between 17 and 28 years old. Also, mobility_duration is quite surprising:\n\ndf %&gt;% pull(mobility_duration) %&gt;% summary()\n\nThe median value of the students‚Äô stay is ONE day, while the mean is just a little above TWO days. Very few students stay more than 10 days, but someone reaches a max of 273 days (39 weeks).\n\ndf %&gt;% count(mobility_duration) %&gt;% mutate(perc = round(n/sum(n)*100,2)) %&gt;% head()\ndf %&gt;% count(mobility_duration) %&gt;% mutate(perc = round(n/sum(n)*100,3)) %&gt;% tail()\n\n\nStudent participants are almost all solo participants as the median shows to be ONE student per observation, TWO students on average, with a max value of 279. So that, to have a picture of the phenomenon select the average value of the student participants as representative.\n\ndf %&gt;% pull(participants) %&gt;% summary(participants)\n\nFinally, gender, Females are slightly more than males, just a little above 50%.\n\ntbl &lt;-df %&gt;% pull(participant_gender) %&gt;% table()\ncbind(n=tbl,pct=round(prop.table(tbl)*100,2))\n\nThis is our new dataset on which we will build our network.\n\ndf &lt;- df %&gt;%\n  group_by(academic_year) %&gt;%\n  filter(between(x = participant_age,17,28),\n         mobility_duration&gt;3) %&gt;%\n  summarise(m_participants=mean(participants),\n            sending_country_code,receiving_country_code,\n            participant_gender,.groups=\"drop\") %&gt;%\n  ungroup() %&gt;%\n  select(-m_participants) %&gt;%\n  distinct()\nkableExtra::kable(head(df))%&gt;%\n  kableExtra::kable_styling(latex_options = \"scale_down\")\n\nAt this point I‚Äôd like to have the full country‚Äôs name, and use {ISOcodes} package. I do that because I‚Äôd like to make a spatial visualization as well. The package contains the values for the countries‚Äô abbreviations coded as ‚ÄúAlpha_2‚Äù. I needed to adjust UK and Greece. To verify this you might need to use the count() function and the str_detect() a couple of times before identifying all the values that needs an adjustment.\n\nlibrary(ISOcodes)\nisocodes&lt;-ISOcodes::ISO_3166_1\nisocodes2 &lt;- isocodes%&gt;%\n  mutate(Alpha_2=case_when(Alpha_2==\"GB\"~\"UK\",\n                           Alpha_2==\"GR\"~\"EL\",\n                           TRUE ~ Alpha_2))\n\nSome more manipulations for selecting just the countries in the dataset.\n\nsending_country_code &lt;- df %&gt;% count(sending_country_code) %&gt;% select(-n) %&gt;% unlist()\nreceiving_country_code &lt;- df %&gt;% count(receiving_country_code) %&gt;% select(-n) %&gt;% unlist()\n\nsending &lt;- isocodes2 %&gt;% filter(Alpha_2 %in% sending_country_code)\nreceiving &lt;- isocodes2 %&gt;% filter(Alpha_2 %in% receiving_country_code)\n\nsending_unlst &lt;- sending %&gt;% count(Name) %&gt;% select(-n) %&gt;% unlist()\nreceiving_unlst &lt;- receiving %&gt;% count(Name) %&gt;% select(-n) %&gt;% unlist()\n\n{ggplot2} package provides spatial data, as you might notice, students of the Erasmus programs come from all over the World. Still, some countries‚Äô name adjustments are needed.\n\nworld &lt;- map_data(\"world\")%&gt;%\n  filter(!region==\"Antarctica\")%&gt;%\n  mutate(region=case_when(region==\"UK\"~ \"United Kingdom\",\n                          region==\"Czech Republic\"~\"Czechia\",\n                          region==\"Moldova\"~\"Moldova, Republic of\",\n                          region==\"Palestine\"~\"Palestine, State of\",\n                          region==\"Russia\" ~ \"Russian Federation\",\n                          TRUE ~ region))\n\n\nsending_geo &lt;- world %&gt;% filter(region %in% sending_unlst)\n\nsending_geo_full &lt;- sending %&gt;%\n  select(Alpha_2,Name) %&gt;%\n  left_join(sending_geo,by = c(\"Name\"=\"region\"))\n\n\nreceiving_geo &lt;- world %&gt;% filter(region %in% receiving_unlst)\n\nreceiving_geo_full &lt;- receiving %&gt;%\n  select(Alpha_2,Name) %&gt;%\n  left_join(receiving_geo,by = c(\"Name\"=\"region\"))\n\nThen finally, make the centroids. We will use this in the visualization further below.\n\nsending_geo_centroids &lt;- sending_geo_full %&gt;%\n  group_by(Name) %&gt;%\n  mutate(avg_long = mean(range(long)),avg_lat = mean(range(lat))) %&gt;%\n  count(Alpha_2,Name,avg_long,avg_lat)\n\nreceiving_geo_centroids &lt;- receiving_geo_full %&gt;%\n  group_by(Name) %&gt;%\n  mutate(avg_long = mean(range(long)),avg_lat = mean(range(lat))) %&gt;%\n  count(Alpha_2,Name,avg_long,avg_lat)\n\ndf2 is our new data set, made of countries‚Äô names and spatials:\n\ndf2 &lt;- df %&gt;%\n  left_join(sending_geo_centroids,by=c(\"sending_country_code\"=\"Alpha_2\")) %&gt;%\n  left_join(receiving_geo_centroids,by=c(\"receiving_country_code\"=\"Alpha_2\")) %&gt;%\n  rename(sending_country_name=Name.x,receiving_country_name=Name.y,\n         avg_long_s=avg_long.x,avg_lat_s=avg_lat.x,\n         avg_long_r=avg_long.y,avg_lat_r=avg_lat.y)%&gt;%\n  select(-n.x,-n.y)\ndf2 %&gt;% head\n\norder_sending and order_receiving will be very useful for a third visualization in which all the sending countries will be matched with students‚Äô destinations. We will see this further below.\n\norder_sending &lt;- df2%&gt;%\n  count(sending_country_name,sort=TRUE)%&gt;%\n  mutate(index_sending=seq(1,length(sending_country_name),1),\n         index_sending=rev(index_sending))%&gt;%\n  drop_na()\n\norder_receiving &lt;- df2%&gt;%\n  count(receiving_country_name,sort=TRUE)%&gt;%\n  mutate(index_receiving=seq(1,length(receiving_country_name),1),\n         index_receiving=rev(index_receiving))%&gt;%\n  drop_na()\n\nAnd here is the data set that we will use to make the rank network:\n\nerasmus_network &lt;- df2 %&gt;%\n  select(academic_year,\n         sending_country_name,receiving_country_name) %&gt;%\n  distinct() %&gt;%\n  left_join(order_sending,by=\"sending_country_name\") %&gt;%\n  left_join(order_receiving,by=\"receiving_country_name\") %&gt;%\n  mutate(group = glue::glue(\"{sending_country_name}-{receiving_country_name}\")) %&gt;%\n  distinct() %&gt;%\n  drop_na() %&gt;%\n  arrange(index_sending) %&gt;%\n  mutate(year_id=case_when(academic_year==\"2014-2015\"~1,\n                           academic_year==\"2015-2016\"~2,\n                           academic_year==\"2016-2017\"~3,\n                           academic_year==\"2017-2018\"~4,\n                           academic_year==\"2018-2019\"~5,\n                           academic_year==\"2019-2020\"~6)) %&gt;%\n  relocate(year_id) %&gt;%\n  arrange(year_id)\n\nerasmus_network %&gt;% head\n\nLoad the packages for setting a nice font.\n\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(extrafont)\n\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\n\nfont_add_google(name=\"Noto Sans\",family=\"notosans\")\n\n\nerasmus_network%&gt;%\n  arrange(index_sending)%&gt;%\n  ggplot()+\n  \n  geom_text(aes(x = -2, y = index_sending+5, label = sending_country_name), \n            vjust=0, \n            hjust=\"left\", color = \"darkred\", size = 3) +\n  \n  ggbump::geom_sigmoid(aes(x = -2, xend = 16.1, \n                           y = index_sending+5, yend =index_receiving+18, \n                           group=factor(group),color=receiving_country_name), \n                       alpha = .6, smooth = 10, size = 0.1,show.legend = F) +\n  \n  geom_text(aes(x = 16, y = index_receiving+17.5, label = receiving_country_name), \n            vjust=-1.5, hjust=\"right\", color = \"darkred\", size = 3) +\n  coord_cartesian()+\n  theme_void()\n\nOr a simplified version:\n\n\nnetwork\n\nLet‚Äôs select Top 5 sending countries:\n\nerasmus_network2 &lt;- erasmus_network %&gt;%\n  filter(sending_country_name%in%c(\"Italy\",\n                                   \"Germany\",\n                                   \"United Kingdom\",\n                                   \"Romania\",\"Spain\")) %&gt;%\n  mutate(sending_country_name=case_when(sending_country_name==\"United Kingdom\"~\"UK\",\n                                        TRUE~sending_country_name))%&gt;%\n  count(year_id,academic_year,sending_country_name) %&gt;%\n  group_by(academic_year)%&gt;%\n  mutate(rank=rank(x=n))%&gt;%\n  ungroup()\n\nerasmus_network2 %&gt;% head()\n\n\nlibrary(ggthemes)\n\nggplot(erasmus_network2,\n       mapping=aes(academic_year,rank,\n                   group=factor(sending_country_name),\n                   color=factor(sending_country_name)))+ \n  geom_point(size = 7) +\n  geom_text(data = erasmus_network2 %&gt;% filter(year_id == min(year_id)),\n            aes(x = year_id - .1, \n                label = sending_country_name), size = 4, hjust = 1) +\n  geom_text(data = erasmus_network2 %&gt;% \n              filter(year_id == max(year_id)),\n            aes(x = year_id + .1, label = sending_country_name), \n            size = 4, hjust = 0,check_overlap = T) +\n  geom_bump(size = 2, smooth = 8) +\n  labs(y = \"RANK\",\n       x = \"Academic Year\",\n       title=\"Erasmus Top 5 student exchange countries\",\n       subtitle=\"Ranks of the highest sending frequency\",\n       caption=\"DataSource: Erasmus student mobility | Data.Europa.eu | Wimdu.co\\nDataViz: Federica Gazzelloni | #TidyTuesday Week 10 Erasmus\") +\n  scale_y_reverse() +\n  scale_color_manual(values = wesanderson::wes_palette(5, name = \"Royal2\"))+\n  cowplot::theme_minimal_grid(font_size = 14, line_size = 0) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank(),\n        plot.title = element_text(color=\"#ffc7ba\"),\n        plot.subtitle = element_text(color=\"#ffc7ba\"),\n        plot.caption = element_text(color=\"#ffc7ba\",size=8),\n        axis.text = element_text(color=\"#ffc7ba\"),\n        axis.title = element_text(color=\"#ffc7ba\"),\n        plot.background = element_rect(color=\"black\",fill=\"black\"),\n        panel.background = element_rect(color=\"black\",fill=\"black\"))\n\nHere is the final part of this post, I set the spatials for making a map visualizaton of the sending to receiving countries.\n\nsending_geo_full2&lt;-sending_geo_full%&gt;%mutate(direction=\"sending\")\nreceiving_geo_full2&lt;-receiving_geo_full%&gt;%mutate(direction=\"receiving\")\n\ngeo_full &lt;-rbind(sending_geo_full2,receiving_geo_full2)\n\n\ncentr_s&lt;- sending_geo_centroids%&gt;%mutate(direction=\"Sending\")\ncentr_r&lt;- receiving_geo_centroids%&gt;%mutate(direction=\"Receiving\")\ncentroids&lt;-rbind(centr_s,centr_r)%&gt;%\n  mutate(direction=as.factor(direction))\n  \nlevels(centroids$direction)&lt;-c(\"Sending\",\"Receiving\")\n\n\ngeo_full2&lt;- geo_full%&gt;%\n  mutate(direction=as.factor(direction))\n\nlevels(geo_full2$direction)&lt;-c(\"Receiving\",\"Sending\")\n\ngeo_full2$direction&lt;-relevel(geo_full2$direction,ref=\"Sending\")\n\n\nggplot(geo_full2)+\n  geom_polygon(data = world,\n               aes(x=long,y=lat,group=group),fill=\"grey78\",color=\"grey5\")+\n  \n  geom_polygon(aes(x=long,y=lat,group=group,fill=direction),alpha=0.3)+\n  \n  geom_point(data=centroids,\n             aes(x=avg_long, y=avg_lat,color=direction,shape=direction))+\n \n  coord_map(\"ortho\", orientation = c(33.366449, 24.022840, 0))+\n  facet_wrap(vars(direction))+\n  scale_x_continuous(\"Latitude\", expand=c(0,0)) +\n  scale_y_continuous(\"Longitude\", expand=c(0,0)) +\n  theme_void()+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/european_flights/index.html",
    "href": "content/blog/posts/dataviz/posts/european_flights/index.html",
    "title": "European flights",
    "section": "",
    "text": "This #TidyTuesday week 28 is all about European flights. I was looking for a visualization that would represent the differences among countries in terms of the number of airports versus number of flights. I looked on the internet for getting some inspiration and then landed on Pintarest, where I found exactly what I was hoping for: a Sankey! First of all, What is a Sankey? The answer is clearer after you make one of your own. In short, it is a network graph connecting diffent groups with a ribbon. A few things that made me think about a lot were the requirements for the type of data to combine, the consequent label results, and the grouping.\n\n\nEuropean Flights\n\n\nlibrary(tidyverse)\n\nSo, let‚Äôs have a go in making a Sankey.\nHave a quick look at the data for European flights:\nLoad the {tidyverse} and the data from: #TidyTuesday GitHub repo\n\nflights &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-12/flights.csv')\n\nJus a little cleaning for this type of data with janitor::clean_names() function, and it‚Äôs ready to use.\n\nflights &lt;- flights%&gt;%\n  janitor::clean_names()\n\nThe best way is to select a subgroup among the list of the European countries in the data set, and represent the diversity in aerial traffic.\nAs an example I choose Ukraine airports, and found that there is only one airoprt for Ukraine in the dataset. But, the interesting thing is that it covers on average almost the same aerial traffic of other European countries such as Italy which is provided with a far larger number of airports, in this dataset.\n\nflights %&gt;%\n  filter(state_name == \"Ukraine\") %&gt;%\n  count(state_name,apt_name)\n\nCompare Italy median of the total number of flights by airports with the only available airport aerial traffic in Ukraine:\n\nflights %&gt;%\n  filter(state_name %in% c(\"Ukraine\",\"Italy\")) %&gt;%\n  count(state_name,pivot_label,flt_tot_1) %&gt;%\n  group_by(state_name) %&gt;%\n  summarize(total_median = median(flt_tot_1)) \n\nThe results of this preliminary data exploration arise a question:"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/european_flights/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/european_flights/index.html#overview",
    "title": "European flights",
    "section": "",
    "text": "This #TidyTuesday week 28 is all about European flights. I was looking for a visualization that would represent the differences among countries in terms of the number of airports versus number of flights. I looked on the internet for getting some inspiration and then landed on Pintarest, where I found exactly what I was hoping for: a Sankey! First of all, What is a Sankey? The answer is clearer after you make one of your own. In short, it is a network graph connecting diffent groups with a ribbon. A few things that made me think about a lot were the requirements for the type of data to combine, the consequent label results, and the grouping.\n\n\nEuropean Flights\n\n\nlibrary(tidyverse)\n\nSo, let‚Äôs have a go in making a Sankey.\nHave a quick look at the data for European flights:\nLoad the {tidyverse} and the data from: #TidyTuesday GitHub repo\n\nflights &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-12/flights.csv')\n\nJus a little cleaning for this type of data with janitor::clean_names() function, and it‚Äôs ready to use.\n\nflights &lt;- flights%&gt;%\n  janitor::clean_names()\n\nThe best way is to select a subgroup among the list of the European countries in the data set, and represent the diversity in aerial traffic.\nAs an example I choose Ukraine airports, and found that there is only one airoprt for Ukraine in the dataset. But, the interesting thing is that it covers on average almost the same aerial traffic of other European countries such as Italy which is provided with a far larger number of airports, in this dataset.\n\nflights %&gt;%\n  filter(state_name == \"Ukraine\") %&gt;%\n  count(state_name,apt_name)\n\nCompare Italy median of the total number of flights by airports with the only available airport aerial traffic in Ukraine:\n\nflights %&gt;%\n  filter(state_name %in% c(\"Ukraine\",\"Italy\")) %&gt;%\n  count(state_name,pivot_label,flt_tot_1) %&gt;%\n  group_by(state_name) %&gt;%\n  summarize(total_median = median(flt_tot_1)) \n\nThe results of this preliminary data exploration arise a question:"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/european_flights/index.html#does-the-number-of-airports-influence-countries-aerial-traffic",
    "href": "content/blog/posts/dataviz/posts/european_flights/index.html#does-the-number-of-airports-influence-countries-aerial-traffic",
    "title": "European flights",
    "section": "Does the number of airports influence countries‚Äô aerial traffic?",
    "text": "Does the number of airports influence countries‚Äô aerial traffic?\n\nWaffle\nMake the first part of the visualization as a series of waffle, one for each selected country to show the diferences in number of airports.\nSelected are 6 countries with a varied number of airports and flights, this is done to give the idea of the differences.\n\ndf &lt;- flights%&gt;%\n  filter(state_name %in% c(\"Ukraine\",\"Belgium\",\"France\",\"Italy\",\"Spain\",\"United Kingdom\")) %&gt;%\n  group_by(state_name) %&gt;%\n  count(apt_name,sort=T) %&gt;%\n  mutate(apt_id = seq(1,length(state_name),1)) %&gt;%\n  summarise(tot = max(apt_id)) %&gt;%\n  arrange(-tot)\n\ndf\n\nLoad both libraries {waffle} and {ggsankey} to use a feature in the waffle which is provided by the ggsankey package.\n\nlibrary(waffle)\nlibrary(ggsankey)\n\nThe waffles shows the number of airports for the selected countries on a total of 100. To make the waffle we can safetly use the geom_waffle() layer\n\nwaffle &lt;- df %&gt;%\n  mutate(rr = 100 - tot) %&gt;% # this is the remaining part of the 100 set of cubes in the waffle\n  pivot_longer(cols = c(tot,rr),names_to = \"values_name\",values_to = \"values\") %&gt;% \n  arrange(state_name) %&gt;%\n  ggplot(aes(fill = values_name, values = values)) +\n  geom_waffle(n_rows = 10, \n              size = 0.33, \n              make_proportional = F,\n              colour = \"white\", \n              flip = TRUE,\n              show.legend = F) +\n  facet_wrap(~ state_name,nrow = 1)+\n  coord_equal() +\n  scale_fill_manual(values=c(\"grey60\",\"#8E0152\"))+\n  theme_sankey(base_size = 16) +\n  theme_enhance_waffle() +\n  theme(strip.text = element_blank(),\n        plot.background = element_blank(),\n        panel.background = element_blank())\n\nwaffle\n\nSankey\nThe purpose of this sankey is to show the differences among selected countries on number of airports and number of flights, from 2016 to 2022 for 6 selected countries.\nThe {ggsankey} package uses an interesting function make_long() which transform provided object into a longer data frame, with vectors named as:\n\nx, next_x, node, and next_node\n\nready to be used inside the geom_sankey.\n\nsankey &lt;- df %&gt;%\n  left_join(flights %&gt;% select(state_name,flt_tot_1), by = \"state_name\") %&gt;%\n  group_by(state_name,tot) %&gt;%\n  summarize(total_med = median(flt_tot_1),.groups = \"drop\") %&gt;%\n  ungroup() %&gt;%\n  mutate(class = cut(tot,\n                     breaks = c(0,1,5,50,65)), #) %&gt;% pull(tot) %&gt;% summary()\n         tot_class = cut(total_med,\n                         breaks = c(0,10,30,65,120,700))) %&gt;% #count(tot_class)\n  mutate(class_id = case_when(class == \"(0,1]\" ~ \"one\",\n                              class == \"(1,5]\" ~ \"up to 5\",\n                              class == \"(5,50]\" ~ \"up to 50\",\n                              TRUE ~ \"up to 65\"),\n         tot_class_id = case_when(tot_class == \"(0,10]\" ~ \"one\",\n                              tot_class == \"(10,30]\" ~ \"up to 30\",\n                              tot_class == \"(30,65]\" ~ \"up to 65\",\n                              tot_class == \"(65,120]\" ~ \"up to 120\",\n                              TRUE ~ \"up to 700\")) %&gt;%\n  mutate(state_name= ifelse(state_name==\"United Kingdom\",\"UK\",state_name)) %&gt;%\n  # function to make the object ready to be used in the geom_sankey\n  make_long(tot_class_id,class_id,state_name) %&gt;% \n  ggplot(aes(x = x, \n             label= node,\n             next_x = next_x, \n             node = node, \n             next_node = (next_node),\n             fill = factor(node))) +\n  geom_sankey(flow.alpha = 0.8, \n              node.color = 1,\n              show.legend = FALSE) +\n  geom_sankey_text(angle=0,family = \"Roboto Condensed\", size = 3)+\n  scale_fill_manual(values = colorRampPalette(RColorBrewer::brewer.pal(11, \"PiYG\"))(13))+\n  theme_sankey(base_size = 16) +\n  coord_flip(expand = F) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        plot.background = element_blank(),\n        panel.background = element_blank())\n  \nsankey\n\nEuropean Map\nThe map has been saved as map.png and sourced in the main visualization. The code can be found in a separate file named ‚Äúeu_coords.R‚Äù.\n\nknitr::include_graphics(\"map.png\")\n\n\nUse {cowplot} for assembling the plots, adding notes, the map and the logo as images.\n\nlibrary(cowplot)\n\n\n combo &lt;- ggdraw() +\n  draw_image(\"map.png\",\n             scale=0.5,\n             x=0.3,y=0.4)+\n  draw_plot(waffle,\n            scale=1,\n            x=0,y=0.2) +\n  draw_plot(sankey, \n            scale=0.7,\n            width = 1.4,\n            height = 0.85,\n            x=-0.2, y=-0.093) +\n  draw_label(\"Countries such as France and Spain have the highest number of airports while this\\nseems reasonable, other countries such as Ukraine with just one airport record\\namong the countries with the highest total number of flights. Data are released\\nwithin a range of 7 years from 2016 to 2022.\",\n             x=0.02,y=0.90,size=9,hjust=0, \n             fontfamily=\"Roboto Condensed\") +\n  draw_label(\"N. of airports\", x=0.1,y=0.4,\n             fontfamily=\"Roboto Condensed\") +\n  draw_label(\"N. of flights\\n(median values)\", x=0.1,y=0.16,\n             fontfamily=\"Roboto Condensed\") +\n  draw_image(\"eurocontrol_logo.png\",\n             scale=0.1,\n             x=-0.45,y=-0.52) +\n  draw_label(\"Eurocontrol aviation intelligence\\n(ansperformance.eu)\",\n             x=0.22,y=-0.02,size=9,fontfamily=\"Roboto Condensed\")\n\n\ncombo\n\nUse {ggpubr} for arranging the grid of the main visualization, so it can be annotate with annotate_figure() to making it a bit more standing out with spaces around the plot and top and bottmo annotations already at the right distance/position.\n\nlibrary(ggpubr)\n\nggpubr::annotate_figure() provides a framework for annotating the plot on the four sides top, bottom, left and right. It comes a handy function when you‚Äôd like to position notes or even rich text at specified positions. To use it, it requires an object from ggpubr::ggarrange().\n\nplot &lt;- ggpubr::ggarrange(combo) \n\n  ggpubr::annotate_figure(plot,\n               top = text_grob(\"Does the number of airports influence countries' aerial traffic?\", \n                               color = c(\"#8E0152\"), face = \"bold\", family = \"Roboto Condensed\", \n                               size = 18, vjust = 1.4),\n               bottom = text_grob(\"DataSource:TidyTuesday 2022 week28 European flights\\nDataViz: Federica Gazzelloni (@fgazzelloni)\",\n                                  color = \"#8E0152\",\n                                  hjust = 1, x = 1, face = \"italic\",  family = \"Roboto Condensed\", \n                                  size = 10),\n               left = text_grob(\"\", color = c(\"#7FBC41\"), rot = 90, size=10),\n               right = text_grob(bquote(\"\"), color=c(\"#DE77AE\"), rot = 90, size=10),\n                fig.lab = \"European flights\", fig.lab.face = \"bold\")\n\nThen finally, save it as .png file with ggsave() function. I specified a height a little bit more than the default values provided as I needed more space.\nThe other arguments, dpi and bg are to set the pixels and the background color.\n\nggsave(\"waffle_sankey.png\",\n       dpi=320,\n       bg = \"grey95\",\n       height = 7.2)"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/oregonfrogs/index.html",
    "href": "content/blog/posts/dataviz/posts/oregonfrogs/index.html",
    "title": "Oregon Spotted a frog: Rana Pretiosa",
    "section": "",
    "text": "This #TidyTuesday week 31 is all about Oregon Spotted a frog: Rana Pretiosa.\nLoad the {tidyverse} and the data from: #TidyTuesday GitHub repo\n\n\nOregon Spotted a Frog!\n\n\nlibrary(tidyverse)\n\n\nfrogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-02/frogs.csv')\n\nOr, as I made a package of these data, the dataset can be installed via:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"fgazzelloni/oregonfrogs\")\n\nAnd then:\n\nlibrary(oregonfrogs)\ndata(package = 'oregonfrogs')\n\nMore about Oregon Frogs and spatial modeling is in this R-Ladies DC talk:\n\nblogdown::shortcode(\"youtube\", \"oYRnA-8ncPU\")\n\n\nThis is the final code for assembling all plots saved in the container images folder using {cowplot} package. In addition, some annotations and grobs are included.\nAll separate scripts are selfcontainers:\n\nGlobe\nNetwork plot\nRoc plot\nVIP plot\n\n\ng &lt;- grid::circleGrob(gp = grid::gpar(fill = NA,color=\"gray\"))\n\n\nlibrary(cowplot)\nggdraw() +\n  draw_label(\"Oregon Spotted a Frog!\", \n             x=0.227,y=0.95,size=34,\n             fontface = \"bold\",\n             fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Captured\",x=0.7,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Visual\",x=0.6,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Frequency\",x=0.5,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Radio-telemetry is used to study frogs (Rana pretiosa)\\nat Crane Prairie Reservoir in Oregon.\\nIndividual frog location tracking occurred roughly\\nweekly between September and late November of 2018.\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.015,y=0.85,hjust=0) +\n  draw_label(label=\"On average more males are caught on radio-telemetry\\nfrequencies than females. In the map the grey circles\\nindicate the tracking location ranges based on mean\\nrange difference among frequencies in same subsite. \\n\\nDataSource: #TidyTuesday 2022 week31\\n@USGS data & @fgazzelloni | DataViz: Federica Gazzelloni\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.015,y=0.08,hjust=0,size=11) +\n    draw_label(label=\"On the left is the network\\nof subsite and water type,\\nit shows more frogs are\\ncaptured in specific locations.\\n\\nOn the right is the models\\nranking among many models.\\nRandom Forest is the best\\nperforming. Results shows on\\naverage male are twice more\\nlikely to get caught than\\nfemales. More info:\\nfedericagazzelloni.netlify.app\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.668,y=0.13,hjust=0,size=8) +\n  draw_image(\"container/images/globe.png\",\n             scale=0.18,\n             x=0.4,y=0.38)+\n  draw_image(\"container/images/network_plot.png\",\n             scale=0.29,\n             x=0.022,y=-0.38)+\n  draw_image(\"container/images/roc_plot.png\",\n             scale=0.245,\n             x=0.4,y=-0.38)+\n  draw_image(\"container/images/lake_map.png\",\n             scale=0.7,\n             x=0.14,y=0.01) +\n  draw_image(\"container/images/vip_plot.png\",\n             scale=0.62,\n             x=-0.3,y=-0.03)+\n  draw_image(\"container/images/frog_logo_visual.png\", \n              scale=0.2,\n             x=0.1, y=0.32) +\n  draw_image(\"container/images/frog_logo_captured.png\",\n              scale=0.2,\n             x=0.2, y=0.32) +\n  draw_grob(g, scale = 0.05,x = 0,y = 0.33)+\n  draw_grob(g, scale = 0.025,x = 0,y = 0.33)+\n  draw_grob(g, scale = 0.01,x = 0,y = 0.33)\n\n\n# ggsave(\"w31_frogs.png\",\n#        width=10,\n#        height = 8,\n#        dpi=320,\n#        bg = \"white\")"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/oregonfrogs/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/oregonfrogs/index.html#overview",
    "title": "Oregon Spotted a frog: Rana Pretiosa",
    "section": "",
    "text": "This #TidyTuesday week 31 is all about Oregon Spotted a frog: Rana Pretiosa.\nLoad the {tidyverse} and the data from: #TidyTuesday GitHub repo\n\n\nOregon Spotted a Frog!\n\n\nlibrary(tidyverse)\n\n\nfrogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-02/frogs.csv')\n\nOr, as I made a package of these data, the dataset can be installed via:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"fgazzelloni/oregonfrogs\")\n\nAnd then:\n\nlibrary(oregonfrogs)\ndata(package = 'oregonfrogs')\n\nMore about Oregon Frogs and spatial modeling is in this R-Ladies DC talk:\n\nblogdown::shortcode(\"youtube\", \"oYRnA-8ncPU\")\n\n\nThis is the final code for assembling all plots saved in the container images folder using {cowplot} package. In addition, some annotations and grobs are included.\nAll separate scripts are selfcontainers:\n\nGlobe\nNetwork plot\nRoc plot\nVIP plot\n\n\ng &lt;- grid::circleGrob(gp = grid::gpar(fill = NA,color=\"gray\"))\n\n\nlibrary(cowplot)\nggdraw() +\n  draw_label(\"Oregon Spotted a Frog!\", \n             x=0.227,y=0.95,size=34,\n             fontface = \"bold\",\n             fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Captured\",x=0.7,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Visual\",x=0.6,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Frequency\",x=0.5,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Radio-telemetry is used to study frogs (Rana pretiosa)\\nat Crane Prairie Reservoir in Oregon.\\nIndividual frog location tracking occurred roughly\\nweekly between September and late November of 2018.\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.015,y=0.85,hjust=0) +\n  draw_label(label=\"On average more males are caught on radio-telemetry\\nfrequencies than females. In the map the grey circles\\nindicate the tracking location ranges based on mean\\nrange difference among frequencies in same subsite. \\n\\nDataSource: #TidyTuesday 2022 week31\\n@USGS data & @fgazzelloni | DataViz: Federica Gazzelloni\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.015,y=0.08,hjust=0,size=11) +\n    draw_label(label=\"On the left is the network\\nof subsite and water type,\\nit shows more frogs are\\ncaptured in specific locations.\\n\\nOn the right is the models\\nranking among many models.\\nRandom Forest is the best\\nperforming. Results shows on\\naverage male are twice more\\nlikely to get caught than\\nfemales. More info:\\nfedericagazzelloni.netlify.app\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.668,y=0.13,hjust=0,size=8) +\n  draw_image(\"container/images/globe.png\",\n             scale=0.18,\n             x=0.4,y=0.38)+\n  draw_image(\"container/images/network_plot.png\",\n             scale=0.29,\n             x=0.022,y=-0.38)+\n  draw_image(\"container/images/roc_plot.png\",\n             scale=0.245,\n             x=0.4,y=-0.38)+\n  draw_image(\"container/images/lake_map.png\",\n             scale=0.7,\n             x=0.14,y=0.01) +\n  draw_image(\"container/images/vip_plot.png\",\n             scale=0.62,\n             x=-0.3,y=-0.03)+\n  draw_image(\"container/images/frog_logo_visual.png\", \n              scale=0.2,\n             x=0.1, y=0.32) +\n  draw_image(\"container/images/frog_logo_captured.png\",\n              scale=0.2,\n             x=0.2, y=0.32) +\n  draw_grob(g, scale = 0.05,x = 0,y = 0.33)+\n  draw_grob(g, scale = 0.025,x = 0,y = 0.33)+\n  draw_grob(g, scale = 0.01,x = 0,y = 0.33)\n\n\n# ggsave(\"w31_frogs.png\",\n#        width=10,\n#        height = 8,\n#        dpi=320,\n#        bg = \"white\")"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/malaria/index.html",
    "href": "content/blog/posts/dataviz/posts/usecases/malaria/index.html",
    "title": "The case of Malaria",
    "section": "",
    "text": "August 18, 2023 ‚ÄúA case of locally acquired #malaria has been confirmed in Maryland, Washington, D.C., area. Nine cases have been reported this summer in Florida and Texas, the first in the US in 20 years, according to the US Centers for Disease Control and Prevention. #epidemics‚Äù 1\n\n\n\nThe case of Malaria\n\nThe Malaria case is somehow a concerning case, eradicated all over the World except for some areas in the Africa‚Äôs continent, the highlight of cases of domestic origins are considered epidemics.\nLet‚Äôs dig on some data about Malaria."
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#overview",
    "href": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#overview",
    "title": "The case of Malaria",
    "section": "",
    "text": "August 18, 2023 ‚ÄúA case of locally acquired #malaria has been confirmed in Maryland, Washington, D.C., area. Nine cases have been reported this summer in Florida and Texas, the first in the US in 20 years, according to the US Centers for Disease Control and Prevention. #epidemics‚Äù 1\n\n\n\nThe case of Malaria\n\nThe Malaria case is somehow a concerning case, eradicated all over the World except for some areas in the Africa‚Äôs continent, the highlight of cases of domestic origins are considered epidemics.\nLet‚Äôs dig on some data about Malaria."
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#who-malaria-map",
    "href": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#who-malaria-map",
    "title": "The case of Malaria",
    "section": "WHO Malaria Map",
    "text": "WHO Malaria Map\nThe first source of data is the WHO Malaria Map, a collection of information about location of cases, the vector species, their invasive status, and other variables of interest such as temporal of the study, the sampling methods, and so on.\nLoad necessary libraries\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n\nmalaria_who &lt;- read_excel(\"data/who_data.xlsx\", \n    sheet = \"Data\")\nmalaria_who %&gt;% head()\n\n\ndim(malaria_who)\n\n\nmalaria_who%&gt;%glimpse\n\n\nmalaria_who &lt;- malaria_who %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(longitude=as.double(longitude),latitude=as.double(latitude))\n\nHistorical collection of data about mosquito number shows data is collected from 1985 and updated to 2022.\nLet‚Äôs omit missing values for the year_start variable and create a new variable midyear which is the middle year between the year_start and year_end of each study.\n\nmalaria_who_mid_year &lt;- malaria_who %&gt;%\n  filter(!is.na(year_start),\n         !is.na(year_end),\n         !is.na(mosquito_number),\n         !mosquito_number==\"NR\") %&gt;%\n  mutate(mosquito_number=as.numeric(mosquito_number),\n         year_end=as.double(year_end),\n         midyear=round((year_end+year_start)/2,0))\n\nAlong the time, the trend of the number of mosquito varied with ups and downs.\n\nmalaria_who_mid_year %&gt;%\ngroup_by(midyear)%&gt;%# count(midyear)\n  mutate(avg_n_mosquito=mean(mosquito_number))%&gt;%\n  ungroup()%&gt;%\n  count(country_name,midyear,avg_n_mosquito,invasive_status)%&gt;%\n  ggplot(aes(x=midyear,y=log10(avg_n_mosquito),\n             group=invasive_status,color=invasive_status))+\n  geom_line(linewidth=2)+\n  scale_color_viridis_d(labels = c(\"Invasive\", \"Native\"),\n                       guide = guide_legend(reverse=TRUE,\n                                            override.aes = list(size = 10)))+\n  coord_cartesian(clip = 'off') +\n  labs(title=\"Time series Malaria 1985 - 2022\",\n       caption=\"Graphics: FG\",color=\"Status\")+ \n  ggthemes::theme_fivethirtyeight()\n\nMosquito have been categorized as invasive species, after 2016.\n\nmalaria_who_mid_year%&gt;%\n  filter(midyear&gt;= 2010)%&gt;%\n  ggplot(aes(x=factor(midyear),y=log10(mosquito_number),\n             group=midyear,fill=invasive_status))+\n  geom_violin()+\n  scale_fill_viridis_d(labels = c(\"Invasive\", \"Native\"),\n                       guide = guide_legend(reverse=TRUE,\n                                            override.aes = list(size = 10)))+\n  ggthemes::theme_fivethirtyeight()+\n  theme(legend.position = \"top\")+\n  labs(title=\"When moqsquito became tag invasive\",\n       caption=\"Graphics: FG\",fill=\"Status\")\n\nThe location of cases revealed by consistent sentinel surveillance procedure, identify the area in the south est Africa/Asia to be the most affected by the danger of malaria virus to spread across the rest of the World.\nHere is a map of the invasive vector species in this area:\n\nworld &lt;- map_data(\"world\")%&gt;%filter(!region==\"Antarctica\")\n\nmalaria_who %&gt;%\n  ggplot(mapping=aes(x=longitude,y=latitude))+\n  geom_polygon(data=world,\n               mapping=aes(x=long,y=lat,group=group),\n               linewidth=0.2,\n               color=\"grey80\",\n               fill=\"white\")+\n  geom_point(aes(fill=invasive_status),\n             color=\"grey80\",\n             shape=21,\n             stroke=0.2,\n             size=0.7,alpha=0.5)+\n  coord_sf(xlim = c(-50, 110), expand = TRUE) +\n  scale_fill_viridis_d()+\n  labs(title=\"Invasive vector species from 1985 to 2022\",fill=\"Status\")+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"steelblue\",fill=\"steelblue\"))\n\nLet‚Äôs zoom in to the center of the mass points. Setting the mean range of the latitude and the longitude, to identify the central point, within the mass of points where mosquito were located, and setting a zoom level, a closer focus at the locations is possible, even with a specification of the new range of to be assigned to the map boundaries.2\n\nlon_avg &lt;- mean(range(malaria_who$longitude))\nlat_avg &lt;- mean(range(malaria_who$latitude))\nlon_avg;lat_avg\n\n\nzoom_to &lt;- c(lon_avg,lat_avg)  \n\nzoom_level &lt;- 1.5\n\nlon_span &lt;- 360 / 2^zoom_level\nlat_span &lt;- 180 / 2^zoom_level\n\n\nlon_bounds &lt;- c(zoom_to[1] - lon_span / 2, zoom_to[1] + lon_span / 2)\nlat_bounds &lt;- c(zoom_to[2] - lat_span / 2, zoom_to[2] + lat_span / 2)\n\n\nlibrary(sf)\n\n\nggplot()+\n  geom_polygon(data=world,\n               mapping=aes(x=long,y=lat,group=group),\n               linewidth=0.2,\n               color=\"grey80\",\n               fill=\"white\")+\n  geom_point(data= malaria_who,\n             mapping=aes(x=longitude,y=latitude,\n                         fill=invasive_status),\n             color=\"black\",\n             shape=21,\n             stroke=0.2,\n             size=3,alpha=0.5)+\n  geom_sf_text(data = st_sfc(st_point(zoom_to), crs = 4326),\n            label = '.') +\n  scale_fill_viridis_d()+\n  coord_sf(xlim = lon_bounds, ylim = lat_bounds,expand = TRUE) + \n  labs(title=\"A closer look at invasive vector species\",\n      subtitle=\"from 1985 to 2022\",\n       caption=\"DataSource: WHO Malaria Data | Graphics: FG\",\n       fill=\"Status\")+\n  ggthemes::theme_map()+\n  theme(text=element_text(family=\"Roboto Condensed\"),\n        plot.title = element_text(size = 18),\n        plot.background = element_rect(color=\"steelblue\",fill=\"steelblue\"),\n        legend.position = c(0,0.001))"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#incidence-of-malaria",
    "href": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#incidence-of-malaria",
    "title": "The case of Malaria",
    "section": "Incidence of malaria",
    "text": "Incidence of malaria\nLooking at a different data source, the Worldbank data provides a reports with the incidence of malaria from 2000 and 2010. 3\n\nmalaria_wb &lt;- read_csv(\"data/worldbank_data.csv\")\n\n\nmalaria_wb_long &lt;- malaria_wb%&gt;%\n  janitor::clean_names()%&gt;%\n  select(-series_name,-series_code) %&gt;%\n  pivot_longer(cols = c(3:14),names_to=\"year\")%&gt;%\n  mutate(value=trimws(value),\n         value=as.numeric(value),\n         value=round(value,3))%&gt;%\n  filter(!is.na(value))%&gt;%\n  mutate(year=gsub(\"_yr[0-9]+$\",\"\",year),\n         year=gsub(\"^x\",\"\",year),\n         year=as.integer(year))%&gt;%\n  arrange(year)\nmalaria_wb_long%&gt;%glimpse\n\n\nmalaria_wb_long%&gt;%\n  group_by(year)%&gt;%\n  reframe(avg=mean(value))%&gt;%\n  mutate(year=as.factor(year))%&gt;%\n  ggplot(aes(year,avg,group=1))+\n  #geom_col()+ \n  geom_line()+\n  labs(title=\"Incidence of Malaria\",\n  subtitle=\"per 1,000 population at risk\",\n  caption = \"Graphics: FG\")+\n  ggthemes::theme_fivethirtyeight()"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#case-study---antimalarial-drug-resistance",
    "href": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#case-study---antimalarial-drug-resistance",
    "title": "The case of Malaria",
    "section": "Case Study - antimalarial drug resistance",
    "text": "Case Study - antimalarial drug resistance\nTracking antimalarial drug resistance using mosquito blood meals: a cross-sectional study\n\nArticle: https://www.thelancet.com/journals/lanmic/article/PIIS2666-5247(23)00063-0/fulltext\nGitHub repository: https://github.com/hannaehrlich/maldrugres_SSA\n\n\nurl &lt;- \"https://raw.githubusercontent.com/hannaehrlich/maldrugres_SSA/main/Survey_MolecMarker_Data.csv\"\n\n\nmaldrugres &lt;- read.csv(url)\nnames(maldrugres)\n\n\nmaldrugres &lt;- maldrugres %&gt;%\n  filter(!is.na(Lat),!is.na(Drug),!Drug==\"\")\n\nmaldrugres%&gt;%count(Drug)\n\n\nworld &lt;- map_data(\"world\")%&gt;%filter(!region==\"Antarctica\")\nafrica &lt;- world%&gt;%filter(long &gt;= -50,long&lt; 60)\n\nggplot(data= maldrugres) +\n  geom_polygon(data= africa, \n               mapping=aes(x=long,y=lat,group=group),\n               linewidth=0.1,\n               color=\"grey70\",fill=\"grey90\")+\n  geom_point(mapping=aes(x=Lon,y=Lat,\n                         color=Drug,\n                         fill=Present),\n             shape=21,\n             stroke=0.2,\n             size=3)+\n  scale_color_manual(values = c(\"steelblue\",\"darkred\"))+\n  scale_fill_gradient(low=NA,high = \"darkred\")+\n  coord_sf(xlim = c(-20,50),ylim = c(-40,60),clip = \"off\")+\n    labs(title=\"antiMalarial drug resistance\",\n       caption=\"DataSource: GitHub hannaehrlich/maldrugres_SSA | Graphics: FG\")+\n  ggthemes::theme_map()+\n  theme(text = element_text(family=\"Roboto Condensed\"),\n        plot.title = element_text(size=30,hjust = 0.5,family=\"Roboto Condensed\"),\n        plot.title.position = \"plot\",\n        legend.position = c(-0.6,0.1))\n\n\nmaldrugres_new &lt;- maldrugres%&gt;%\n  mutate(MidYear = round((StartYr+EndYr)/2,0),\n         MidYear= as.factor(MidYear))%&gt;%\n  select(Country,Site,Lon,Lat,MidYear,Tested,Present,MixedPres,Drug)%&gt;%\n  janitor::clean_names()\n\nmaldrugres_new%&gt;%head\n\nTidymodels\nEDA\n\nmaldrugres_new%&gt;%\n  count(drug)%&gt;%\n  ggplot(aes(x=drug,y=n,fill=drug))+\n  geom_col()+\n  labs(title=\"Drug class imbalance\",\n       caption=\"Graphics: FG\")+\n  scale_fill_viridis_d()+\n  ggthemes::theme_fivethirtyeight()+\n  theme(legend.position = \"none\")\n\n\nmaldrugres_new%&gt;%\n  group_by(country,drug)%&gt;%\n  reframe(avg_drug=mean(present))%&gt;%\n  ggplot(aes(x=avg_drug,y=fct_reorder(country,avg_drug)))+\n  geom_col(aes(fill=drug))+\n  scale_fill_viridis_d()+\n  labs(title=\"AntiMalarial Drug Resistance Present\",\n       caption=\"Graphics: FG\",\n       x=\"Average value by Country\",y=\"\")+\n   ggthemes::theme_fivethirtyeight()+\n  theme(axis.text.x = element_text(angle=0,hjust = 1))\n\n\nmaldrugres_new %&gt;%\n  ggplot(aes(present))+\n  geom_density()+\n  labs(title=\"Density distribution of antimalarial drug resistance\",\n       caption = \"Graphics: FG\")+\n  ggthemes::theme_fivethirtyeight()\n\n\nmaldrugres_new%&gt;%\n  group_by(country,drug)%&gt;%\n  reframe(avg_drug=mean(present))%&gt;%\n  ggplot(aes(x=avg_drug,y=fct_reorder(country,avg_drug)))+\n  geom_boxplot()+\n  labs(title=\"AntiMalaria Drug Resistance Present\",\n       caption=\"Graphics: FG\",\n       x=\"Average value by Country\",y=\"\")+\n  theme(axis.text.x = element_text(angle=0,hjust = 1))+\n  ggthemes::theme_fivethirtyeight()\n\nSpending data\n\nlibrary(tidymodels)\n\n\nset.seed(123)\nsplit &lt;- initial_split(maldrugres_new)\ntraining &lt;- training(split)\ntesting &lt;- testing(split)\ncv_folds &lt;- vfold_cv(training,v = 10)\n\nFeaturing Engineering\n\nrec_pca &lt;- recipe(present ~., training) %&gt;%\n  step_dummy(all_nominal_predictors(),keep_original_cols = F)%&gt;%\n  step_corr(all_numeric_predictors())%&gt;%\n  step_normalize(all_predictors())%&gt;%\n  step_pca(all_predictors())\n  \nrec_pca_df &lt;- rec_pca %&gt;%\nprep()%&gt;%\n  juice()%&gt;%\n  cbind(drug=training$drug,country=training$country)\nrec_pca_df%&gt;%head\n\n\nrec_pca_df %&gt;%\nggplot(aes(x=PC1,PC2,group=drug,color=drug))+\n  geom_point()+\n  geom_smooth(se=F)+\n  scale_color_viridis_d()+\n  labs(title=\"Principal Components Analysis\",\n       caption = \"Graphics: FG\")+\n  ggthemes::theme_fivethirtyeight()+\n  theme(axis.title = element_text())\n\n\nrec_pca_df %&gt;%\nggplot(aes(x=PC1,y=fct_reorder(country,PC1),group=country))+\n  geom_boxplot()+\n  labs(title=\"Principal Components Analysis - boxplot\",\n       caption = \"Graphics: FG\",y=\"\")+\n  ggthemes::theme_fivethirtyeight()+\n    theme(axis.title = element_text(),\n          plot.title = element_text(hjust = 1))\n\n\nrec_pca_df %&gt;%\nggplot(aes(x=PC1,y=present))+\n  geom_point()+\n  scale_y_log10()+\n  geom_smooth(method = 'gam', formula = y ~ s(x, bs = \"cs\"))+\n  labs(title=\"Principal Components Analysis - scatterplot\",\n       caption = \"Graphics: FG\",y=\"Present\")+\n  ggthemes::theme_fivethirtyeight()+\n    theme(axis.title = element_text(),\n          plot.title = element_text(hjust = 0))\n\n\nrec_ica &lt;- recipe(present ~., training) %&gt;%\n  step_dummy(all_nominal_predictors(),keep_original_cols = F)%&gt;%\n  step_corr(all_numeric_predictors())%&gt;%\n  step_normalize(all_predictors())%&gt;%\n  step_ica(all_predictors())%&gt;%\n  prep()%&gt;%\n  juice()%&gt;%\n  cbind(drug=training$drug)\n\n\nrec_ica %&gt;%\nggplot(aes(x=IC1,IC2,group=drug,color=drug))+\n  geom_point()+\n  geom_smooth(se=F)+\n  scale_colour_viridis_d()+\n  labs(title=\"Independent Components Analysis\",\n       caption = \"Graphics: FG\")+\n  ggthemes::theme_fivethirtyeight()+\n    theme(axis.title = element_text(),\n          plot.title = element_text(hjust = 0))"
  },
  {
    "objectID": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#footnotes",
    "href": "content/blog/posts/dataviz/posts/usecases/malaria/index.html#footnotes",
    "title": "The case of Malaria",
    "section": "Footnotes",
    "text": "Footnotes\n\nworldbank_data source: https://databank.worldbank.org/reports.aspx?source=2&series=SH.MLR.INCD.P3&country=#‚Ü©Ô∏é\nworldbank_data source: https://databank.worldbank.org/reports.aspx?source=2&series=SH.MLR.INCD.P3&country=#‚Ü©Ô∏é\nworldbank_data source: https://databank.worldbank.org/reports.aspx?source=2&series=SH.MLR.INCD.P3&country=#‚Ü©Ô∏é"
  },
  {
    "objectID": "content/blog/posts/dataviz/index.html",
    "href": "content/blog/posts/dataviz/index.html",
    "title": "Data Visualizations",
    "section": "",
    "text": "Data visualization is a powerful tool that allows the transformation of complex data sets into comprehensible and insightful graphical representations. By presenting data visually, it enables the identification of patterns, trends, and correlations that might otherwise go unnoticed, making it easier to understand and communicate complex findings.\nOne of the fundamental concepts in modern data visualization is the Grammar of Graphics, which provides a structured approach to creating visualizations layer by layer. This concept is clearly interpreted in tools like the {ggplot2} R package, part of the {tidyverse} ecosystem, developed by Hadley Wickham in 2005. The Grammar of Graphics helps guide the creation of clear and concise visualizations, offering users a framework for combining data, aesthetics, and geometries.\nHere you‚Äôll find a collection of my favorite Data Visualizations, showcasing a variety of themes and data sets. Each visualization represents my approach to conveying meaningful insights through graphs and plots. Whether you are interested in exploring health metrics, economic trends, or the spread of infectious diseases, my work emphasizes the power of visual storytelling.\nFor a more comprehensive experience, feel free to explore my full collection of plots and data visualizations at my UPDVwR website: Explore all Data Visualizations"
  },
  {
    "objectID": "content/blog/posts/dataviz/index.html#overview",
    "href": "content/blog/posts/dataviz/index.html#overview",
    "title": "Data Visualizations",
    "section": "",
    "text": "Data visualization is a powerful tool that allows the transformation of complex data sets into comprehensible and insightful graphical representations. By presenting data visually, it enables the identification of patterns, trends, and correlations that might otherwise go unnoticed, making it easier to understand and communicate complex findings.\nOne of the fundamental concepts in modern data visualization is the Grammar of Graphics, which provides a structured approach to creating visualizations layer by layer. This concept is clearly interpreted in tools like the {ggplot2} R package, part of the {tidyverse} ecosystem, developed by Hadley Wickham in 2005. The Grammar of Graphics helps guide the creation of clear and concise visualizations, offering users a framework for combining data, aesthetics, and geometries.\nHere you‚Äôll find a collection of my favorite Data Visualizations, showcasing a variety of themes and data sets. Each visualization represents my approach to conveying meaningful insights through graphs and plots. Whether you are interested in exploring health metrics, economic trends, or the spread of infectious diseases, my work emphasizes the power of visual storytelling.\nFor a more comprehensive experience, feel free to explore my full collection of plots and data visualizations at my UPDVwR website: Explore all Data Visualizations"
  },
  {
    "objectID": "content/blog/posts/stats/posts/predictive_modeling/index.html",
    "href": "content/blog/posts/stats/posts/predictive_modeling/index.html",
    "title": "Predictive modeling",
    "section": "",
    "text": "This post is dedicated to make a comparison between Caret and TidyModels R packages. Data modelling with R pass through data preprocessing and parameters assessments to predicting an outcome. Both set of packages can be used to achieve same results, with the purpose of finding the best predictive performance for data specific models.\n\n\nPredictive modeling - TreeMap\n\nThe Caret package is the starting point for understanding how to manage models and produce unbiases predictions with R. As well as TidyModels meta package, it gives the opportunity to contruct a multivariate model syntax to manage several models to be applied on same set of data. TidyModels allows the use of a set of concatenated functions in partership with the TidyVerse grammar to build a structural model base which blends different models as one global model.\nThe following is an attempt to a comparison between the two predictive model structures."
  },
  {
    "objectID": "content/blog/posts/stats/posts/predictive_modeling/index.html#overview",
    "href": "content/blog/posts/stats/posts/predictive_modeling/index.html#overview",
    "title": "Predictive modeling",
    "section": "",
    "text": "This post is dedicated to make a comparison between Caret and TidyModels R packages. Data modelling with R pass through data preprocessing and parameters assessments to predicting an outcome. Both set of packages can be used to achieve same results, with the purpose of finding the best predictive performance for data specific models.\n\n\nPredictive modeling - TreeMap\n\nThe Caret package is the starting point for understanding how to manage models and produce unbiases predictions with R. As well as TidyModels meta package, it gives the opportunity to contruct a multivariate model syntax to manage several models to be applied on same set of data. TidyModels allows the use of a set of concatenated functions in partership with the TidyVerse grammar to build a structural model base which blends different models as one global model.\nThe following is an attempt to a comparison between the two predictive model structures."
  },
  {
    "objectID": "content/blog/posts/stats/posts/predictive_modeling/index.html#caret-package",
    "href": "content/blog/posts/stats/posts/predictive_modeling/index.html#caret-package",
    "title": "Predictive modeling",
    "section": "Caret package",
    "text": "Caret package\nThe most important functions for this package, grouped by steps to modeling, are:\n\n\nPreprocessing (data cleaning/wrangling)\n\npreProcess()\n\n\n\nData splitting and resampling\n\ncreateDataPartition()\n\n\ncreateResample()\ncreateTimeSlices()\n\n\n\nModel fit and prediction\n\ntrain()\n\n\npredict()\n\n\n\nModel comparison\n\nconfusionMatrix()"
  },
  {
    "objectID": "content/blog/posts/stats/posts/predictive_modeling/index.html#machine-learning-algorithms-in-r",
    "href": "content/blog/posts/stats/posts/predictive_modeling/index.html#machine-learning-algorithms-in-r",
    "title": "Predictive modeling",
    "section": "Machine learning algorithms in R",
    "text": "Machine learning algorithms in R\n\nLinear discriminant analysis\nRegression\nNaive Bayes\nSupport vector machines\nClassification and regression trees\nRandom forests\nBoosting\netc.\n\nResource: Practical Machine Learning"
  },
  {
    "objectID": "content/blog/posts/stats/posts/predictive_modeling/index.html#caret-or-tidymodels",
    "href": "content/blog/posts/stats/posts/predictive_modeling/index.html#caret-or-tidymodels",
    "title": "Predictive modeling",
    "section": "Caret or TidyModels?",
    "text": "Caret or TidyModels?\nCaret Tidymodels"
  },
  {
    "objectID": "content/blog/posts/stats/posts/predictive_modeling/index.html#caret-example-with-spam-data",
    "href": "content/blog/posts/stats/posts/predictive_modeling/index.html#caret-example-with-spam-data",
    "title": "Predictive modeling",
    "section": "Caret Example with SPAM Data",
    "text": "Caret Example with SPAM Data\n\nlibrary(caret); library(kernlab); data(spam)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\ninTrain &lt;- createDataPartition(y=spam$type,\n                              p=0.75, list=FALSE)\ntraining &lt;- spam[inTrain,]\ntesting &lt;- spam[-inTrain,]\n# dim(training)\n\nset.seed(32343)\nmodelFit &lt;- train(type ~.,data=training, method=\"glm\")\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# modelFit\n\npredictions &lt;- predict(modelFit,newdata=testing)\n# predictions\n\ncm &lt;- confusionMatrix(predictions,testing$type)\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction nonspam spam\n   nonspam     655   37\n   spam         42  416\n                                          \n               Accuracy : 0.9313          \n                 95% CI : (0.9151, 0.9452)\n    No Information Rate : 0.6061          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8564          \n                                          \n Mcnemar's Test P-Value : 0.6527          \n                                          \n            Sensitivity : 0.9397          \n            Specificity : 0.9183          \n         Pos Pred Value : 0.9465          \n         Neg Pred Value : 0.9083          \n             Prevalence : 0.6061          \n         Detection Rate : 0.5696          \n   Detection Prevalence : 0.6017          \n      Balanced Accuracy : 0.9290          \n                                          \n       'Positive' Class : nonspam         \n                                          \n\nplot(cm$table,main=\"Table\")"
  },
  {
    "objectID": "content/blog/posts/stats/posts/predictive_modeling/index.html#tidymodels-example-with-spam-data",
    "href": "content/blog/posts/stats/posts/predictive_modeling/index.html#tidymodels-example-with-spam-data",
    "title": "Predictive modeling",
    "section": "TidyModels Example with SPAM Data",
    "text": "TidyModels Example with SPAM Data\n\nlibrary(tidymodels)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.4.1 ‚îÄ‚îÄ\n\n\n‚úî broom        1.0.10     ‚úî rsample      1.3.1 \n‚úî dials        1.4.2      ‚úî tailor       0.1.0 \n‚úî dplyr        1.1.4      ‚úî tidyr        1.3.1 \n‚úî infer        1.0.9      ‚úî tune         2.0.1 \n‚úî modeldata    1.5.1      ‚úî workflows    1.3.0 \n‚úî parsnip      1.3.3      ‚úî workflowsets 1.1.1 \n‚úî purrr        1.2.0      ‚úî yardstick    1.3.2 \n‚úî recipes      1.3.1      \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n‚úñ scales::alpha()          masks kernlab::alpha(), ggplot2::alpha()\n‚úñ dials::buffer()          masks kernlab::buffer()\n‚úñ rsample::calibration()   masks caret::calibration()\n‚úñ purrr::cross()           masks kernlab::cross()\n‚úñ purrr::discard()         masks scales::discard()\n‚úñ dplyr::filter()          masks stats::filter()\n‚úñ dplyr::lag()             masks stats::lag()\n‚úñ purrr::lift()            masks caret::lift()\n‚úñ yardstick::precision()   masks caret::precision()\n‚úñ yardstick::recall()      masks caret::recall()\n‚úñ yardstick::sensitivity() masks caret::sensitivity()\n‚úñ yardstick::specificity() masks caret::specificity()\n‚úñ recipes::step()          masks stats::step()\n\ntidymodels_prefer()\nset.seed(123)\nsplit &lt;- initial_split(spam,0.75,strata=type)\ntraining &lt;- training(split)\ntesting &lt;- testing(split)\n\nmodelFit &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;%\n  fit(type~.,data=spam)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# tidy(modelFit)\n\npredictions &lt;- predict(modelFit,new_data=testing)\n# predictions\n\ntesting$pred &lt;- predictions$.pred_class\ncm &lt;- yardstick::conf_mat(data = testing, truth = type, estimate = pred)\ncm\n\n          Truth\nPrediction nonspam spam\n   nonspam     668   50\n   spam         29  404\n\nautoplot(cm)"
  },
  {
    "objectID": "content/blog/posts/stats/posts/plain_vanilla/index.html",
    "href": "content/blog/posts/stats/posts/plain_vanilla/index.html",
    "title": "Understanding Plain Vanilla - from Scratch",
    "section": "",
    "text": "In the world of artificial intelligence, understanding the fundamentals of neural networks is essential. One of the simplest yet powerful architectures is the Artificial Neural Network (ANN) with a single hidden layer, often referred to as a ‚Äúplain vanilla‚Äù network. This blog post aims to provide a comprehensive guide to building such a network from scratch, focusing on key concepts like activation functions, weights, and the backpropagation algorithm outside the library boxes.\n\n\nTarget vs Response"
  },
  {
    "objectID": "content/blog/posts/stats/posts/plain_vanilla/index.html#overview",
    "href": "content/blog/posts/stats/posts/plain_vanilla/index.html#overview",
    "title": "Understanding Plain Vanilla - from Scratch",
    "section": "",
    "text": "In the world of artificial intelligence, understanding the fundamentals of neural networks is essential. One of the simplest yet powerful architectures is the Artificial Neural Network (ANN) with a single hidden layer, often referred to as a ‚Äúplain vanilla‚Äù network. This blog post aims to provide a comprehensive guide to building such a network from scratch, focusing on key concepts like activation functions, weights, and the backpropagation algorithm outside the library boxes.\n\n\nTarget vs Response"
  },
  {
    "objectID": "content/blog/posts/stats/posts/plain_vanilla/index.html#example-of-construction-of-plain-vanilla-network-architecture",
    "href": "content/blog/posts/stats/posts/plain_vanilla/index.html#example-of-construction-of-plain-vanilla-network-architecture",
    "title": "Understanding Plain Vanilla - from Scratch",
    "section": "Example of construction of Plain vanilla network architecture",
    "text": "Example of construction of Plain vanilla network architecture\nANN‚Äôs (Artificial Neural Networks) is the simplest implementation of deep learning model architectures that mimic the human brain‚Äôs neural network. The simplest form of ANN is a single-layer network, also known as a ‚Äúplain vanilla‚Äù network. This network consists of an input layer, a hidden layer, and an output layer. The hidden layer transforms the input data into a new set of features, which are then used to predict the response variable.\n\n\n\nIn the image above, we have some predictors \\(x_i\\) that are fed into the hidden layer, which then transforms them into a new set of features \\(h_k(x)\\), which are then used to predict the response variable \\(y\\).\n\nrm(list=ls())\nsuppressMessages(library(tidyverse))\ntheme_set(theme_minimal())\n\nBuild synthetic data\nLet‚Äôs create a synthetic dataset to demonstrate the construction of a plain vanilla network. We will generate a dataset with 60 observations and two predictors, x and y, using the following steps:\n\n\nPredictors as Uniform distributed variables ranging between [-2, 2]:\n\n\nset.seed(100)\nx &lt;- runif(60, min=-2, max=2)\n\n\n\nResponse Variable as function of the predictors:\n\n\ny &lt;- function(x) {\n  Y = (cos(2*x + 1))\n  return(Y)\n}\n\n\ndata &lt;- tibble(y=y(x),x)\nhead(data)\n\n# A tibble: 6 √ó 2\n       y       x\n   &lt;dbl&gt;   &lt;dbl&gt;\n1  0.859 -0.769 \n2  0.591 -0.969 \n3  0.152  0.209 \n4 -0.829 -1.77  \n5  0.733 -0.126 \n6  0.645 -0.0649\n\n\n\ndata %&gt;% summary()\n\n       y                  x           \n Min.   :-0.99978   Min.   :-1.77447  \n 1st Qu.:-0.59934   1st Qu.:-0.89194  \n Median : 0.15559   Median :-0.04092  \n Mean   : 0.02628   Mean   : 0.01437  \n 3rd Qu.: 0.62358   3rd Qu.: 0.88138  \n Max.   : 0.99930   Max.   : 1.95826  \n\n\nEDA - Exploratory Data Analysis\nLet‚Äôs visualize our synthetic data:\n\ndata %&gt;%\n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept=0, \n             linetype=\"dashed\", color=\"grey\")\n\n\n\nSynthetic Data\n\n\n\nParameters estimation\nNow that we have the data, we attempt to replicate the distribution of this data with a model using artificial neural network technique with a single hidden layer. The model will have the following parameters:\nModel Formula:\n\\[f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kh_k(X)}\\]\nWhere, \\(h_k\\) is the hidden layer, \\(k=1,...,K\\) the number of activations, \\(\\beta_0,\\beta_1,...,\\beta_K\\) the coefficients , and \\(w_{10},...,w_{kp}\\) the weights.\nThe hidden layer computes a number of activations.\nNumber of activations\nInitialize the number of hidden neurons \\(k\\), which is the number of activations in the hidden layer:\n\nhidden_neurons = 5\n\nThe number of hidden neurons is a hyperparameter that needs to be tuned. The more neurons, the more complex the model, but it can also lead to overfitting. You can think of a neuron as a connection between the input and output layers. The more neurons, the more connections, and the more complex the model.\nWe have set to have 5 hidden neurons in this example. This means that the hidden layer will compute 5 different linear combinations of the input \\(X\\). This linear combination is then squashed through an activation function \\(g(¬∑)\\) to transform it.\nThe function that takes the input \\(X\\) and produces an output \\(A_k\\), the activation.\nActivation Function\nThe activation function is a non-linear transformation of the input layers \\(X_1,X_2,...,X_p\\) which transform to \\(h_k(X)\\) while learning during the training of the network. It is a function that decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n\\[A_k=h_k(X)=g(z)\\]\n\\(g(z)\\) is a function used in logistic regression to convert a linear function into probabilities between zero and one.\nTo be more explicit, the activation function is a function that takes the input signal and generates an output signal, but takes into account a threshold, meaning that it will only be activated if the signal is above a certain threshold.\nWe have specified 5 different activation functions to compare their performance, and we will use the sigmoid function as the activation function in this example.\nType of Activation functions\nThere are several types of activation functions, each with its own characteristics, but all have in common that they introduce non-linearity into the first level of output provided.\nSome of the most common types of activation functions are:\n\n\nSigmoid function:\n\n\\[g(z)=\\frac{e^z}{1+e^z}=\\frac{1}{1+ e^{-z}}\\]\n\nsigmoid &lt;- function(x) {\n  z = (1 / (1 + exp(-x)))\n  return(z)\n}\n\n\n\nReLU (Rectified Linear Unit) function:\n\n\\[g(z) = max(0, z)\\]\n\nrelu &lt;- function(x) {\n  z = ifelse(x &lt; 0, 0, x)\n  return(z)\n}\n\n\n\nSoftPlus Function\n\n\\[g(z) = \\log(1 + e^z)\\]\nThis is a smooth approximation to the ReLU function. Firstly introduced in 2001, Softplus is an alternative to traditional functions because it is differentiable and its derivative is easy to demonstrate (see source: https://sefiks.com/2017/08/11/softplus-as-a-neural-networks-activation-function/).\n\nsoftplus &lt;- function(x) {\n  z = log(1 + exp(x))\n  return(z)\n}\n\n\nOther types are:\n\nPolynomials/Splines: \\(x^2\\)\n\nHyperbolic tanh: \\(tanh(x) = (e^x ‚Äì e^-x) / (e^x + e^-x)\\)\n\n\n\n\nLet‚Äôs compare the activation functions:\ndata %&gt;%\n  mutate(z=sigmoid(x)) %&gt;%\n  ggplot() +\n  geom_line(aes(x, z)) +\n  ylim(0, 1)\ndata %&gt;%\n  ggplot() +\n  geom_line(aes(x, relu(x) * 1/2.4))\ndata %&gt;%\n  ggplot() +\n  geom_line(aes(x, softplus(x)))\n\n\n\n\n\nSigmoid function\n\n\n\n\n\nReLU function\n\n\n\n\n\nSoftPlus function\n\n\n\n\n\nAnd now look at how the Sigmoid differs from the ReLU function:\n\n\ndata %&gt;%\n  ggplot() +\n  geom_line(aes(x, sigmoid(x))) +\n  # relu resized for comparison\n  geom_line(aes(x, relu(x) * 1/2.4))+\n  labs(y=\"Sigmoid vs ReLU\")\n\n\n\n\n\n\nSigmoid vs ReLU\n\n\n\n\n\nOur model is a model in the model:\n\\[f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kh_k(X)}\\] \\[A_k=h_k(X)=g(z)=g(w_{k0}+\\sum_{j=1}^p{w_{kj}X_j})\\]\n\\[f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kg(w_{k0}+\\sum_{wkj}^p{X_j})}\\]\nAs you might have noticed in the formula above, the model is a linear combination of the input \\(X\\) and the weights \\(w_{kj}\\), which are adjusted during the training process. The activation function \\(g(z)\\) is applied to the linear combination of the input and weights to transform the output.\nLet‚Äôs have a look at the weights and how they are initialized.\nWeights Initialization\nThe weights are the parameters of the model that are adjusted during the training process. They can be considered as the coefficients of the hidden layer model.\nThey are initialized randomly, and the model is trained to adjust these weights during the training process. The weights are adjusted using the backpropagation algorithm, which computes the gradient of the loss function with respect to the weights. Then, the weights are updated using the gradient descent algorithm. We will see how this is done in the next section.\nThe weights are initialized randomly to break the symmetry and prevent the model from getting stuck in a local minimum. In this case we use a normal distribution with a mean of 0 and a standard deviation of 1 to initialize the weights.\nRandomly initializing the weights as i.i.d. \\(W \\sim N(0,1)\\):\n\nw1 = matrix(rnorm(2*hidden_neurons), \n            nrow=hidden_neurons, \n            ncol=2)\nw2 = matrix(rnorm(hidden_neurons + 1), \n            nrow=1, \n            ncol=(hidden_neurons + 1))\n\nThe constant term \\(w_{k0}\\) will shift the inflection point, and transform a linear function to a non-linear one. The weights are adjusted during the training process to minimize the error between the predicted and actual values.\nThe model derives five new features by computing five different linear combinations of \\(X\\), and then squashes each through an activation function \\(g(¬∑)\\) to transform it.\n\ndata %&gt;%\n  ggplot(aes(x, y)) +\n  geom_point(shape=21, \n             stroke=0.5, \n             fill=\"grey\", \n             color=\"grey20\") +\n  geom_line(linewidth=0.2) +\n  geom_smooth(method = \"lm\", \n              color=\"steelblue\", \n              se=F) +\n  geom_line(aes(x, sigmoid(y)), \n            linetype=\"dashed\", \n            color=\"steelblue\")\n\n\n\nIn this figure is shown the attempt of the ‚Äòlinear‚Äô and ‚Äòsigmoid‚Äô functions to fit our original data. The final model function, able to replicate the original pattern is the result of a continous adaptation and re-calibration of the coeffcients in the model.\n\n\n\nFeedForward\nThe meaning of feedforward is used to describe the process of moving the input data through the network to obtain the predicted output. The feedforward process is the first step in the training process of the neural network.\nHere is a function that computes the output of the model given the inputs: data, weights, and number of activations. It computes the output by multiplying the input data by the weights and applying the activation function to the result. It is a matrix multiplication (%*%), which is a common operation in unsupervised learning algorithms.\n\nfeedForward &lt;- function(x, w1, w2, activation) {\n  output &lt;- rep(0, length(x))\n\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    z1 = activation(a1)\n    a2 = w2 %*% matrix(rbind(1, z1), ncol=1)\n    output[i] = a2\n  }\n\n  return(output)\n}\n\nDerivative Activation Function\nNow, that we have the feedforward function, we need to compute the derivative of the activation function. The backpropagation algorithm multiplies the derivative of the activation function.\n\nBackpropagation algorithm multiplies the derivative of the activation function.\n\nHere is a recap of the definition of derivative formula, which is applied any time the output released by the activation function is met in the network. And so, a new minimum is found. It will be more clear through the end of the post.\n\n\n\nSo, it is fundamental to define the derivative of the activation function needed for computing the gradient. For this example, we will use the derivative of the sigmoid function.\n\nderivativeActivation &lt;- function(x) {\n  g = (sigmoid(x) * (1 - sigmoid(x)))\n  return(g)\n}\n\nModel Error\nFunction for computing model error is the sum of squared errors (SSE) between the predicted and actual values.\n\nmodelError &lt;- function(x, y, w1, w2, activation) {\n  # Predictions\n  preds &lt;- feedForward(x, w1, w2, activation)\n  # Error calculation\n  SSE &lt;- sum((y - preds) ** 2)\n  return (SSE)\n}\n\nBack-Propagation\nSo, this is the time for computing the gradients.\n\nWhat are the gradients?\n\nThe gradients are the derivatives of the cost function with respect to the weights. The backpropagation algorithm computes the gradient of the loss function with respect to the weights.\nThe gradients are then used to update the weights using the gradient descent algorithm.\n\nbackPropagation &lt;- function(x, y, w1, w2, \n                            activation, derivativeActivation) {\n  #predicted values\n  preds &lt;- feedForward(x, w1, w2, activation) \n  #Derivative of the cost function (first term)\n  derivCost &lt;- -2 * (y - preds) \n  #Gradients for the weights\n  dW1 &lt;- matrix(0, ncol=2, nrow=nrow(w1)) \n  dW2 &lt;- matrix(rep(0, length(x) * (dim(w2)[2])), nrow=length(x)) \n\n  # Computing the Gradient for W2\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    da2dW2 = matrix(rbind(1, activation(a1)), nrow=1)\n    dW2[i,] = derivCost[i] * da2dW2\n  }\n\n  # Computing the gradient for W1\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    da2da1 = derivativeActivation(a1) * matrix(w2[,-1], ncol=1)\n    da2dW1 = da2da1 %*% matrix(rbind(1, x[i]), nrow=1)\n    dW1 = dW1 + derivCost[i] * da2dW1\n  }\n\n  # Storing gradients for w1, w2 in a list\n  gradient &lt;- list(dW1, colSums(dW2))\n\n  return (gradient)\n}\n\nStochastic Gradient Descent\nDefining our Stochastic Gradient Descent algorithm which will adjust our weight matrices.\n\nSGD &lt;- function(x, y, w1, w2, activation, derivative, learnRate, epochs) {\n  SSEvec &lt;- rep(NA, epochs) # Empty array to store SSE values after each epoch\n  SSEvec[1] = modelError(x, y, w1, w2, activation)\n\n  for (j in 1:epochs) {\n    for (i in 1:length(x)) {\n      gradient &lt;- backPropagation(x[i], y[i], w1, w2, activation, derivative)\n      # Adjusting model parameters for a given number of epochs\n      w1 &lt;- w1 - learnRate * gradient[[1]]\n      w2 &lt;- w2 - learnRate * gradient[[2]]\n    }\n    SSEvec[j+1] &lt;- modelError(x, y, w1, w2, activation) \n    # Storing SSE values after each iteration\n    }\n    # Beta vector holding model parameters\n    B &lt;- list(w1, w2)\n    result &lt;- list(B, SSEvec)\n    return(result)\n}\n\nModeling\nRunning the SGD function to obtain our optimized model and parameters:\n\nmodel &lt;- SGD(x, y(x), w1, w2, \n             activation = sigmoid, \n             derivative = derivativeActivation,\n             learnRate = 0.01, \n             epochs = 200)\n\nObtaining our adjusted SSE‚Äôs for each epoch:\n\nSSE &lt;- model[[2]]\n\nModel Visualization\nPlotting the SSE from each epoch vs number of epochs\n\nmodel_data &lt;- tibble(x=seq(0, 200, 1), SSE)\n\nggplot(model_data,aes(x, SSE)) +\n  geom_line(linewidth=0.1)+\n  geom_point(shape=21, \n             stroke=0.2, \n             fill=alpha(\"steelblue\", 0.3),\n             color=\"brown\") +\n  labs(title=\"Model SSE by Number of Epochs\",\n       x = \"Epochs\", y = \"Error\")\n\n\n\n\n\n\n\nParameters optimization\nExtracting our new parameters from our model.\n\nnew_w1 &lt;- model[[1]][[1]]\nnew_w2 &lt;- model[[1]][[2]]\n\nComparing our old weight matrices against the new ones.\n\npar(mfrow=c(1,2))\nplot(w1,new_w1)\nabline(0,1)\n\nplot(w2,new_w2)\nabline(0,1)\n\n\n\n\n\n\n\nNew Predictions\nObtaining our new predictions using our optimized parameters.\n\ny_pred &lt;- feedForward(x, new_w1, new_w2, sigmoid)\n\nPlotting training data against our model predictions\n\ndata %&gt;%\n  mutate(y_pred=y_pred) %&gt;%\n  pivot_longer(cols = c(y, y_pred)) %&gt;%\n  ggplot(aes(x, value, group=name, color=name)) +\n  geom_point(shape=21, stroke=0.5) +\n  geom_line() +\n  scale_color_discrete(type = c(\"steelblue\", \"red\")) +\n  labs(title= \"Target Response vs. Predictions\",\n       x=\"Observations\", \n       y=\"Responses\")"
  },
  {
    "objectID": "content/blog/posts/stats/posts/plain_vanilla/index.html#resources",
    "href": "content/blog/posts/stats/posts/plain_vanilla/index.html#resources",
    "title": "Understanding Plain Vanilla - from Scratch",
    "section": "Resources",
    "text": "Resources\n\nThe code used for this example is customized from tristanoprofetto github repository: https://github.com/tristanoprofetto/neural-networks/blob/main/ANN/Regressor/feedforward.R\n\nStatQuest: Neural Networks Pt. 1: Inside the Black Box https://www.youtube.com/watch?v=CqOfi41LfDw\n\nOther-Resources: https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464"
  },
  {
    "objectID": "content/blog/posts/stats/posts/multiple_testing/index.html",
    "href": "content/blog/posts/stats/posts/multiple_testing/index.html",
    "title": "Multiple Hypothesis Testing",
    "section": "",
    "text": "Multiple testing, what does it means?\nSometimes answering a research question means evaluating the different combinations of predictors, and this is the case when multiple hypothesis testing is made.\nThe following are the fundamental steps for understanding how to proceed when multiple hypothesis testing is needed.\n\n\nHypothesis Testing\n\nImage credits: Hypothesis Testing On Linear Regression-medium.com"
  },
  {
    "objectID": "content/blog/posts/stats/posts/multiple_testing/index.html#overview",
    "href": "content/blog/posts/stats/posts/multiple_testing/index.html#overview",
    "title": "Multiple Hypothesis Testing",
    "section": "",
    "text": "Multiple testing, what does it means?\nSometimes answering a research question means evaluating the different combinations of predictors, and this is the case when multiple hypothesis testing is made.\nThe following are the fundamental steps for understanding how to proceed when multiple hypothesis testing is needed.\n\n\nHypothesis Testing\n\nImage credits: Hypothesis Testing On Linear Regression-medium.com"
  },
  {
    "objectID": "content/blog/posts/stats/posts/multiple_testing/index.html#research-questions",
    "href": "content/blog/posts/stats/posts/multiple_testing/index.html#research-questions",
    "title": "Multiple Hypothesis Testing",
    "section": "Research questions",
    "text": "Research questions\nIn general, hypothesis testing is made for comparing the expected values of two predictors which are the key drivers for explaining the trend of certain variables that are depending on the levels of those predictors.\nGoing into more detail, making a real-life example, we might need to evaluate the influence of gender and age on the increase of heart diseases. Under this specification, simple questions to be answered would be:\n\nto be a female can be more or less risky in incurring heart disease?\nor, what is the age on average at which heart diseases start increasing?\n\nTo answer these questions we need to make some assumptions:\nthere is no gender difference in heart diseases increasing age is not a cause of heart diseases"
  },
  {
    "objectID": "content/blog/posts/stats/posts/multiple_testing/index.html#hypothesis-testing",
    "href": "content/blog/posts/stats/posts/multiple_testing/index.html#hypothesis-testing",
    "title": "Multiple Hypothesis Testing",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nThe first hypothesis is namely, the null hypothesis \\(H_{0}\\), and in general, it assumes that there would be no difference in mean (expected value) between the levels of the predictors. In case the null hypothesis fails, an alternative hypothesis is considered, such as there is a difference.\n\nNull hypothesis \\(H_{0}\\): the difference in mean equals zero\nAlternative hypothesis \\(H_{a}\\): the difference in mean is not zero\n\nThis is the starting point of making a hypothesis testing, then more variables can be tested, excluded, or kept whether there is a difference in the mean or not. And, so we are talking of multiple testing of hypothesis.\nSteps to hypothesis testing\n\nDefine the null and alternative hypothesis\nConstruct a test statistic against the null hypothesis\nCompute a p-value to quantify the probability to obtain a value that is the same or more extreme than the test statistic under the null hypothesis\nDecide whether to reject the null hypothesis\nStep back to modeling\nLet‚Äôs hypothesize that we are investigating the root cause of heart diseases. To be simplistic we hypothesize that only gender and age are the key drivers for high blood pressure, the cause of the disease.\nHow would you make a model for answering this question?\nAs an example, we use the heart_disease dataset from the {cheese} package, and to begin our investigation we consider two predictors Age and Sex.\nModel function: \\(y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}\\epsilon\\)\nHypothesis:\n\n\n\\(\\beta_{i}\\) equals to zero\nthere is no difference between the mean blood pressure in female and male groups.\n\n\nlibrary(tidyverse)\nlibrary(cheese)\ncheese::heart_disease %&gt;%head\n\n# A tibble: 6 √ó 9\n    Age Sex    ChestPain           BP Cholesterol BloodSugar MaximumHR\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;          &lt;dbl&gt;\n1    63 Male   Typical angina     145         233 TRUE             150\n2    67 Male   Asymptomatic       160         286 FALSE            108\n3    67 Male   Asymptomatic       120         229 FALSE            129\n4    37 Male   Non-anginal pain   130         250 FALSE            187\n5    41 Female Atypical angina    130         204 FALSE            172\n6    56 Male   Atypical angina    120         236 FALSE            178\n# ‚Ñπ 2 more variables: ExerciseInducedAngina &lt;fct&gt;, HeartDisease &lt;fct&gt;\n\n\n\nheart_disease%&gt;%\n  ggplot(aes(y=HeartDisease,fill=factor(Sex)),color=\"white\") +\n  geom_histogram(stat=\"count\",position = \"dodge\")+\n  labs(fill=\"Sex\")+\n  scale_fill_brewer()+\n  theme_test()\n\n\n\n\n\n\n\n\nResources\n\n\nHeartDisease dataset\nMultiple Testing - Washington"
  },
  {
    "objectID": "content/blog/posts/stats/index.html",
    "href": "content/blog/posts/stats/index.html",
    "title": "Data Analytics and Statistical Techniques",
    "section": "",
    "text": "Understanding Plain Vanilla - from Scratch\n\n\n\nrstats\n\nmodeling\n\nmachine-learning\n\nartificial-neural-networks\n\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies Events Stats\n\n\n\ntext-analysis\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow does geom_smooth() make predictions\n\n\n\nrstats\n\nmodeling\n\nlinear regression\n\ngeom_smooth\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Simpson‚Äôs Paradox: A Simple Explanation\n\n\n\nrstats\n\nmodeling\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics model comparison\n\n\n\nrstats\n\nmodeling\n\nbayesian\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive modeling\n\n\n\nmodeling\n\n\n\n\n\n\n\n\n\nJun 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Hypothesis Testing\n\n\n\n\n\n\n\n\nMay 20, 2022\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/podcasts/posts/the-new-science-of-fighting-infectious-disease/index.html#episode-overview",
    "href": "content/podcasts/posts/the-new-science-of-fighting-infectious-disease/index.html#episode-overview",
    "title": "The New Science of Fighting Infectious Disease",
    "section": "Episode Overview",
    "text": "Episode Overview\n\nThe new science of fighting infectious disease is a multidisciplinary field that integrates health metrics, machine learning, and spatial modeling to predict, track, and mitigate the spread of pathogens. Rather than relying solely on traditional medicine, this approach utilizes advanced data science to understand the \"invisible enemies\" that threaten global health.\n\n\nKey pillars of this modern approach include:\n\n\n\nQuantitative Health Metrics: The foundation of this science involves using metrics like Disability-Adjusted Life Years (DALYs) to quantify the total human toll of a disease, combining years lost to premature death (YLLs) with years lived with disability (YLDs).\n\n\nMachine Learning and Predictive Modeling: Modern researchers use machine learning algorithms‚Äîsuch as Random Forests, Neural Networks, and Gradient Boosting‚Äîto learn patterns from historical data and predict future outbreaks. This allows for parameter auto-calibration, where models automatically adjust to minimize prediction errors based on real-world observations.\n\n\nSpatial and Spatiotemporal Analysis: By using Geographic Information Systems (GIS) and techniques like Kriging, scientists can map infection hotspots and visualize how diseases like COVID-19 or Malaria move across a landscape. This helps identify high-risk areas for targeted interventions, such as vaccination or resource allocation.\n\n\nThe One Health Perspective: This science recognizes that human, animal, and environmental health are interconnected. It focuses on the zoonotic origins of viruses (like those moving from bats or pangolins to humans) to prevent pandemics before they start.\n\n\nTransfer Learning: An emerging technique in this field involves model transfer, where insights and models built from one outbreak (like a past cholera epidemic) are adapted to predict and manage new, similar diseases in low-data settings.\n\n\n\nAnalogy for Understanding the New ScienceThink of this field as a \"High-Tech Biological Radar.\" Traditional medicine is like a lighthouse‚Äîit helps you see the danger when it is already close. This new science, however, is a sophisticated radar system that tracks the weather (pathogen spread), predicts where a storm will hit (outbreak forecasting), and calculates the exact amount of damage expected (health metrics), allowing cities to prepare long before the first raindrop falls.\n\n\n\n\n\nBuy your hardcover here: https://amzn.to/3N7zZjH"
  },
  {
    "objectID": "content/podcasts/posts/the-new-science-of-fighting-infectious-disease/index.html#key-topics-discussed",
    "href": "content/podcasts/posts/the-new-science-of-fighting-infectious-disease/index.html#key-topics-discussed",
    "title": "The New Science of Fighting Infectious Disease",
    "section": "Key Topics Discussed",
    "text": "Key Topics Discussed\n\nData-driven insights\nHealth metrics analysis\nInnovative approaches to public health"
  },
  {
    "objectID": "content/podcasts/posts/valuing-human-life-in-a-spreadsheet/index.html#episode-overview",
    "href": "content/podcasts/posts/valuing-human-life-in-a-spreadsheet/index.html#episode-overview",
    "title": "Valuing Human Life in a Spreadsheet",
    "section": "Episode Overview",
    "text": "Episode Overview\n\nLife expectancies, simulated data and health metrics in a fascinating discussion about how to apply raw data to wel estimated formula."
  },
  {
    "objectID": "content/podcasts/posts/valuing-human-life-in-a-spreadsheet/index.html#key-topics-discussed",
    "href": "content/podcasts/posts/valuing-human-life-in-a-spreadsheet/index.html#key-topics-discussed",
    "title": "Valuing Human Life in a Spreadsheet",
    "section": "Key Topics Discussed",
    "text": "Key Topics Discussed\n\n\nLife expectancies, simulated data and health metrics in a fascinating discussion about how to apply raw data to wel estimated formula"
  },
  {
    "objectID": "content/podcasts/posts/bills-of-mortality-to-ai/index.html#episode-overview",
    "href": "content/podcasts/posts/bills-of-mortality-to-ai/index.html#episode-overview",
    "title": "Bills of Mortality to AI",
    "section": "Episode Overview",
    "text": "Episode Overview\n\n17th Century: The Bills of Mortality. John Graunt, the ‚Äúfather of demography,‚Äù uses London‚Äôs weekly death records to identify the first statistical patterns in human longevity and disease."
  },
  {
    "objectID": "content/podcasts/posts/bills-of-mortality-to-ai/index.html#key-topics-discussed",
    "href": "content/podcasts/posts/bills-of-mortality-to-ai/index.html#key-topics-discussed",
    "title": "Bills of Mortality to AI",
    "section": "Key Topics Discussed",
    "text": "Key Topics Discussed\n\nData-driven insights\nHealth metrics analysis\nInnovative approaches to public health"
  },
  {
    "objectID": "content/rpackages/posts/typeR/index.html#why-i-built-typer",
    "href": "content/rpackages/posts/typeR/index.html#why-i-built-typer",
    "title": "Announcing {typeR}: simulate live typing of R scripts",
    "section": "Why I built {typeR}",
    "text": "Why I built {typeR}\nWhen teaching or recording short demos, I often want code to appear as if it is being typed live ‚Äî without actually typing in real time.\nScreen sharing is not always ideal: text can be small, the pace can be uneven, and the viewer cannot easily copy what they see. On the other hand, showing the full script at once removes the ‚Äústep-by-step‚Äù feel that helps learners follow the logic."
  },
  {
    "objectID": "content/rpackages/posts/typeR/index.html#what-typer-does",
    "href": "content/rpackages/posts/typeR/index.html#what-typer-does",
    "title": "Announcing {typeR}: simulate live typing of R scripts",
    "section": "What {typeR} does",
    "text": "What {typeR} does\nGiven a path to an .R file, typeR():\n\nreads the file line-by-line\nprints each line character-by-character\nwaits a small amount of time between characters (delay)\nmoves to a new line and continues\n\nThis is useful for:\n\nlive coding demos (when you want to control pacing)\nteaching sessions (small scripts, narrated step-by-step)\nrecording short videos or GIFs where you want code to ‚Äúappear‚Äù\nworkshops where you want to reveal code progressively"
  },
  {
    "objectID": "content/rpackages/posts/typeR/index.html#quick-start",
    "href": "content/rpackages/posts/typeR/index.html#quick-start",
    "title": "Announcing {typeR}: simulate live typing of R scripts",
    "section": "Quick start",
    "text": "Quick start\nCreate a small script:\nwriteLines(\n  c(\n    \"# Demo script\",\n    \"x &lt;- 1:10\",\n    \"y &lt;- x * 2\",\n    \"plot(x, y)\"\n  ),\n  \"demo_script.R\"\n)\nThen use typeR() to simulate typing it out:\n\nlibrary(typeR)\n\ntypeR(\"demo_script.R\", delay = 0.03)\n\nThis will print the script to the console, character-by-character, with a 0.03 second delay between characters.\n\n\n\n\n\n\nTips:\n\nUse a slightly larger delay (e.g.¬†0.06‚Äì0.12) for live narration.\nUse a smaller delay (e.g.¬†0.01‚Äì0.03) for short clips / GIFs."
  },
  {
    "objectID": "content/rpackages/posts/typeR/index.html#design-notes",
    "href": "content/rpackages/posts/typeR/index.html#design-notes",
    "title": "Announcing {typeR}: simulate live typing of R scripts",
    "section": "Design notes",
    "text": "Design notes\nInternally, the package separates:\n\na small helper that handles the typing logic\nthe user-facing typeR() function that validates inputs and reads the file\n\nThis keeps the public API simple while keeping the core behaviour easy to maintain."
  },
  {
    "objectID": "content/rpackages/posts/typeR/index.html#installation",
    "href": "content/rpackages/posts/typeR/index.html#installation",
    "title": "Announcing {typeR}: simulate live typing of R scripts",
    "section": "Installation",
    "text": "Installation\nIf you are installing from GitHub:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"fgazzelloni/typeR\")"
  },
  {
    "objectID": "content/rpackages/posts/typeR/index.html#roadmap-small-and-realistic",
    "href": "content/rpackages/posts/typeR/index.html#roadmap-small-and-realistic",
    "title": "Announcing {typeR}: simulate live typing of R scripts",
    "section": "Roadmap (small and realistic)",
    "text": "Roadmap (small and realistic)\nI want to keep {typeR} lightweight, but a few improvements are natural next steps:\n\nbetter handling of tabs / indentation across consoles\nan option to print to a connection (useful for capturing output cleanly)\na minimal vignette with ‚Äúteaching/recording‚Äù workflows"
  },
  {
    "objectID": "content/rpackages/posts/typeR/index.html#feedback-and-contributions",
    "href": "content/rpackages/posts/typeR/index.html#feedback-and-contributions",
    "title": "Announcing {typeR}: simulate live typing of R scripts",
    "section": "Feedback and contributions",
    "text": "Feedback and contributions\nIf you try typeR and it fits your teaching or demo workflow, I‚Äôd love to hear:\n\nwhere it worked well\nwhere the pacing feels off\nwhich platform/IDE you were using (RStudio, Positron, terminal, etc.)\n\nIf you want to contribute, open an issue with a short reproducible example and the expected behaviour."
  },
  {
    "objectID": "content/rpackages/posts/oregonfrogs/index.html#machine-learning",
    "href": "content/rpackages/posts/oregonfrogs/index.html#machine-learning",
    "title": "Oregon spotted a frog: Rana Pretiosa",
    "section": "Machine Learning",
    "text": "Machine Learning\nTuning: Tweaking the hyperparameters of the learner\nIn random forests, the hyperparameters mtry, min.node.size and sample.fraction determine the degree of randomness, and should be tuned. This is when machine learning comes into play.\nHyperparameters:\n\n\nmtry indicates how many predictor variables should be used in each tree\n\nsample.fraction parameter specifies the fraction of observations to be used in each tree\n\nmin.node.size parameter indicates the number of observations a terminal node should at least have\n\nsource: https://geocompr.robinlovelace.net/eco.html\n\n#### tuning\ntune_level = mlr3::rsmp(\"spcv_coords\", folds = 5)\n\nterminator = mlr3tuning::trm(\"evals\", n_evals = 50)\n\ntuner = mlr3tuning::tnr(\"random_search\")\n\nSearch space\nTo specify tuning limits paradox::ps() is used:\n\nsearch_space =\n  paradox::ps(\n    mtry = paradox::p_int(lower = 1,\n                          upper = ncol(task$data()) - 1),\n    sample.fraction = paradox::p_dbl(lower = 0.2,\n                                     upper = 0.9),\n    min.node.size = paradox::p_int(lower = 1,\n                                   upper = 10)\n  )\nsearch_space\n\nAutomation\nAutomated tuning specification via the mlr3tuning::AutoTuner() function:\n\nautotuner_rf =\n  mlr3tuning::AutoTuner$new(\n    learner = learner,\n    store_benchmark_result = TRUE,\n    resampling = mlr3::rsmp(\"spcv_coords\",\n                            folds = 5),\n    # spatial partitioning\n    measure = mlr3::msr(\"classif.acc\"),\n    # performance measure\n    terminator = mlr3tuning::trm(\"evals\",\n                                 n_evals = 50),\n    # specify 50 iterations\n    search_space = search_space,\n    # predefined hyperparameter search space\n    tuner = mlr3tuning::tnr(\"random_search\") # specify random search\n  )\n\n\n# hyperparameter tuning\ntime = Sys.time()\n\nset.seed(0412022)\nautotuner_rf$train(task)\n\nSys.time() - time\n\n\nautotuner_rf$tuning_result\n\n\nautotuner_rf$predict(task)\n# pred = terra::predict(..., model = autotuner_rf, fun = predict)\n\n# save.image(\"data/oregonfrogs_mlr3.RData\")\n\n\nres &lt;- autotuner_rf$predict(task)\nautoplot(res)\n\n\nres %&gt;%\n  fortify() %&gt;%\n  pivot_longer(cols = starts_with(\"prob\"),\n               names_to = \"prob_type\",\n               values_to = \"prob\") %&gt;%\n  mutate(prob_type = gsub(\"prob.\", \"\", prob_type)) %&gt;%\n  ggplot(aes(prob, group = prob_type, fill = prob_type)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(vars(prob_type), scales = \"free\") +\n  labs(fill = \"Water Type\", x = \"Probability\", y = \"Density\") +\n  scale_fill_viridis_d() +\n  theme_bw()"
  },
  {
    "objectID": "about/workshops/index.html",
    "href": "about/workshops/index.html",
    "title": "Federica Gazzelloni",
    "section": "",
    "text": "CDC Centers for Disease Control\nData Analysis and Visualization with R\nDate: 2024-08-19 to 2024-08-20 | Centers for Disease Control\nWebsite: https://rrlove-cdc.github.io/2024-08-19-cdc-online/\nEtherPad: https://pad.carpentries.org/2024-08-19-cdc-online\n\nData Carpentry Genomics\nDate: 2024-06-10 to 2024-06-13 | Centers for Disease Control\nWebsite: https://fgazzelloni.github.io/2024-06-10-cdc-online/\nEtherPad: https://pad.carpentries.org/2024-06-10-cdc-online\n\n\n\nUniversity of Washington\nNetwork of the National Library of Medicine (NNLM) Region 5 Library Carpentry\nDate: March 19-21 2024 | Network of the National Library of Medicine (NNLM)\nWebsite: https://nnlm-ncds.github.io/2024-03-19-nnlm-uw-online/\nEtherPad: https://pad.carpentries.org/2024-03-19-nnlm-uw-online\n\n\n\nHelmholtz Information & Data Science Academy\nSoftware Carpentry (Shell, Git, and programming with R)\nDate: February 12-13 2024 | Helmholtz Online\nWebsite: https://macrobiotus.github.io/2024-02-12-helmholtz-online/\nEtherPad: https://pad.carpentries.org/2024-02-12-helmholtz-online\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/rladies/index.html",
    "href": "about/rladies/index.html",
    "title": "Federica Gazzelloni",
    "section": "",
    "text": "Sharing the mission with the open source community\n\n\n\nR-Ladies DC\nDate: September 2022\nAbstract: This tutorial is meant for people new to spatial analysis and modeling with RStudio but comfortable in making simple data visualization with ggplot2. In this video, you will learn how to make a map with RStudio, and how to use data modeling for making spatial model analysis.\nMaterial: It is helpful to have the following R packages installed beforehand: {tidyverse}, {ggthemes}, {maptools}, {ggmap}, {sf}, {spocc}, {dismo}, {SpatialEpi}, and {oregonfrogs} dataset from\nremotes::install_github(\"fgazzelloni/oregonfrogs\")\n\nGitHub Repo: https://github.com/Fgazzelloni/How-to-Spatial-Modeling-with-R\nBook: https://fgazzelloni.github.io/How-to-Spatial-Modeling-with-R\n\n\n\n\n\nR-Ladies NYC & R-Ladies Rome\nDate: April 2023\nAbstract: In this video, you will learn about Modeling infectious diseases with R using both deterministic and Bayesian SIR model methods. We will explore both Deterministic and Bayesian SIR model methods, and learn how to use the well-known SIR model to understand how epidemics unfold and how to prevent their spread.\nThis video is perfect for students of science, healthcare professionals, or anyone who wants to gain a deeper understanding of how to use R for Modeling infectious diseases.\nAgenda:\n\nIntroduction presentation of the Chapters and R-Ladies Global action\nSIR model with R - quick intro assessment (Speaker Federica Gazzelloni)\nBayesian workflow for disease transmission (Speaker Jacqueline Buros)\nQ&A session\n\nMaterial:\n\nGitHub Repo: https://github.com/Fgazzelloni/sir-model-with-R/\n\n\n\n\n\nR-Ladies Cambridge\nDate: March 2024\nAbstract: In this video, you‚Äôll learn how to use {ggplot2} to replicate one of the ongoing #DuboisChallenge2024 plates. The original plates are part of W.E.B. Du Bois‚Äôs legacy from back in 1900, showcased at the Paris Exposition. I‚Äôll be using modern tools such as R. We‚Äôll explore the colors used in the plates and delve into the intricacies of the challenge by understanding the perspective of hand-made graphs in the ‚ÄòDuboisian‚Äô style.\nMaterial:\n\nGitHub Repo: https://github.com/Fgazzelloni/R-Ladies-Cambridge-Dataviz-lunch-Replicating-Du-Bois-with-R\nWebSite Collection: https://fgazzelloni.quarto.pub/unlocking-the-power-of-data-visualization-with-r/duboischallenge/\n\n\n\n\n\nR-Ladies NYC & R-Ladies Rome\nDate: June 2024\nAbstract: In this video, you‚Äôll learn how to make a website in R with Quarto. We‚Äôll explore the basics of Quarto and how to publish a website. The tutorial is designed to be accessible to beginners, with no prior experience required. By the end of the session, you will have a fully functional website that you can use to showcase your projects and achievements.\nMaterial:\n\nGithub repository: https://github.com/Fgazzelloni/building_a_website_in_r\nFinal version of the Website: https://fgazzelloni.quarto.pub/my-website-in-r‚Äìquarto/\n\n\nResources:\n\nQuarto Docs: https://quarto.org/docs/websites/\nCSS customization for your website: https://www.w3schools.com/css/\nBackground image (change - ‚Äúpink‚Äù with your favorite pick): https://www.pexels.com/search/pink/\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/profile/index.html",
    "href": "about/profile/index.html",
    "title": "Federica Gazzelloni",
    "section": "",
    "text": "STATISTICIAN | ACTUARY | DATA SCIENTIST\n‚ÄúI‚Äôve always loved maths and science, I meant to be a Veterinary, I wanted to do ballet classes, teach gymnastic, be an economist, a psychologist‚Ä¶ Many years passed since I started the journey of my career as an investigator, it turned out to be an everyday challenge with tasks to solve, and most importantly a continuous learning path. Each day brings with it a fresh set of challenges to solve and opportunities for growth. It‚Äôs a path marked by curiosity, resilience, and a relentless pursuit of excellence‚Äîa journey that I‚Äôm grateful to be on.‚Äù\nFederica Gazzelloni is an Actuary, and a Data Scientist with a focus on health metrics, machine learning, and data visualization. With experience spanning corporate, academic, and research roles, Federica has developed a robust skill set that bridges actuarial science, statistical modeling, and public health.\nFederica began her career as an actuary, working in corporate and academic settings where she focused on quantitative analysis, risk modeling, and compliance in the insurance and pensions industry. As a research-oriented actuary, she not only applied advanced actuarial principles but also developed a deep understanding of the statistical methods. Federica taught mathematics to high school students and instructed university students in computer science, helping to cultivate the next generation of data-driven professionals.\nIn recent years, Federica has expanded her focus to health data modeling, particularly in the context of infectious disease research. As the world faced the Covid-19 pandemic, her expertise in statistical methods and health metrics became even more relevant. Collaborating with the Institute for Health Metrics and Evaluation (IHME), Federica contributed to global health studies that aimed to understand the spread and impact of Covid-19, as well as other pressing health issues. This experience inspired her to create a practical manual for health data analysis, which serves as a resource for professionals and students alike who wish to explore health metrics and epidemiological modeling in-depth. Her book, to be published by CRC Press, combines foundational knowledge with practical applications, including R code for real-world case studies in health metrics.\nFederica‚Äôs role in the pre-publication stages of GBD research enables her to directly contribute to some of the most impactful global health research being conducted today. This work not only requires technical acumen in statistical modeling but also a keen eye for detail and a commitment to accuracy, given the far-reaching implications of public health data.\nBeyond her work in health metrics, Federica is an active member of the open-source community, where she contributes to several organizations dedicated to education and software development. She is a Certified Carpentries Instructor, delivering workshops that empower learners with foundational data science skills in R, Python, and other tools. Federica has taught workshops for prestigious organizations, including the Helmholtz Information & Data Science Academy, the University of Washington, and the Centers for Disease Control and Prevention (CDC), where she introduced participants to key concepts in programming, data wrangling, and statistical analysis. Her teaching style is characterized by clarity, engagement, and inclusivity, making technical content accessible to learners of all backgrounds and skill levels.\nFederica also serves as the Lead Organizer for R-Ladies Rome, a chapter of the global R-Ladies organization that promotes gender diversity in the R programming and data science communities. Since its founding in 2023, R-Ladies Rome has grown significantly under her leadership, reaching a large and engaged audience through online and in-person events. Through R-Ladies, Federica has organized a range of activities, from tutorials on data visualization to workshops on advanced R packages, all aimed at fostering a supportive and inclusive community for women and underrepresented groups in data science. Notable events include a session with Hadley Wickham, Chief Scientist at Posit PBC, and an upcoming workshop on building reproducible data pipelines, which reflects the group‚Äôs commitment to high-quality, practical learning opportunities.\nIn addition to her work with R-Ladies Rome, Federica has collaborated with other R user groups globally, including R-Ladies New York, R-Ladies Paris, and the TunisR User Group. These partnerships have expanded the reach of her initiatives, creating a more interconnected and supportive global R community. Federica is also involved with Bioconductor and the R Consortium, furthering her commitment to open-source development and collaborative learning.\nOne of Federica‚Äôs key strengths is her ability to use data visualization as a tool for communication and insight. Her background in data visualization allows her to create compelling visual representations of complex health data, making it easier for diverse audiences to grasp intricate statistical relationships. Whether through static graphs or interactive dashboards, Federica‚Äôs visualizations aim to tell a story, uncover trends, and empower decision-makers with actionable insights.\nThroughout her career, Federica has consistently demonstrated a commitment to knowledge sharing and community building. Her work is informed by a belief in the power of data to drive positive change, especially in fields like public health where informed decision-making can save lives. Her contributions to education, open-source software, and health metrics research underscore her dedication to making data science an inclusive and impactful field.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#news-health-metrics-and-the-spread-of-infectious-diseases-now-published-and-available-worldwide",
    "href": "index.html#news-health-metrics-and-the-spread-of-infectious-diseases-now-published-and-available-worldwide",
    "title": "Federica Gazzelloni",
    "section": "News: Health Metrics and the Spread of Infectious Diseases ‚Äî Now Published and Available Worldwide",
    "text": "News: Health Metrics and the Spread of Infectious Diseases ‚Äî Now Published and Available Worldwide\n\nGazzelloni, Federica. Health Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R. First edition. Boca Raton, FL: CRC Press, 2025. Print.\n\n\nüéâ Explore the Online and Printed Versions\nüìñ Read it online for free:\n\nüîó https://fgazzelloni.github.io/hmsidR/\n\nüõí Order your Hard Printed Copy:\n\nOrder the printed edition:üëâ Routledge Website or üëâ Amazon\n\n\n\n\n\n\n\n\nIt‚Äôs been an incredible journey, and now that the book is officially printed and available on Amazon to order, I feel a mix of emotions‚Äîboth the excitement of sharing it with you and a touch of nostalgia as this creative process comes to a close. üòå\n\nWriting this book has been an immensely rewarding experience. I‚Äôve spent countless hours selecting the right material and striving to present it in a way that‚Äôs accessible and helpful for early-career researchers and students. One of the biggest challenges was deciding what to include and what to leave out, constantly refining the content to ensure it‚Äôs both comprehensive and easy to follow.\nEven though the book is now published, my mind is still buzzing with ideas. There‚Äôs always that lingering thought of what else could be added to make it even better. But at some point, you have to step back and trust that what you‚Äôve created will serve its purpose. And now, it‚Äôs time to see how it will resonate with you, the readers.\n\nAs I reflect on this journey, I‚Äôm filled with gratitude for all the support and feedback I‚Äôve received along the way. Your encouragement has been invaluable in shaping this book into what it is today.\n\nSo, what‚Äôs next? While this project may be wrapping up, my passion for writing and sharing knowledge is far from over. Stay tuned for more updates and future projects! In the meantime, I invite you to dive into the book, explore its content, and share your thoughts. Your insights will be crucial as I continue to learn and grow.\nThank you for being a part of this journey. I hope the book serves as a valuable resource for your research and studies. Let‚Äôs see where this new chapter takes us!\nHappy reading! üìö‚ú®\n\n\n\n\n  SUBSCRIBE\n    Subscribe for the latest tutorials and insights in Data Science and R Programming!\n    \n    \n    \n     \n\n\n\nLatest content"
  }
]